<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.97.3">Hugo</generator><title type="html"><![CDATA[Tom Tech Blog]]></title>
    
        <subtitle type="html"><![CDATA[Share and Learn]]></subtitle>
    
    
    
            <link href="/" rel="alternate" type="text/html" title="HTML" />
            <link href="/feed.xml" rel="self" type="application/atom+xml" title="Atom" />
    <updated>2022-09-17T10:49:41+10:00</updated>
    
    <id>/</id>
        
        <entry>
            <title type="html"><![CDATA[Build Full Stack Webapp Using Cloudflare Worker and Github Pages]]></title>
            <link href="/posts/build-full-stack-webapp-using-worker-and-github-pages/?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/posts/build-full-stack-webapp-using-worker-and-github-pages/</id>
            
            <published>2022-04-20T08:07:31+10:00</published>
            <updated>2022-04-27T22:47:47+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I have been following the COVID situation in Shanghai lately. Intentionally or not, it is surprisingly hard to find a daily chart to reflect the proper case numbers overthere. To solve this problem, I decided to build a webapp just does exactly that: a simple chart the displays Shanghai daily COVID cases that includes both symptomatic and asymptomatic cases.</p>
<p>The overall design includes a backend API that provides daily numbers in JSON format, and a front-end page that presents the data in the form of an area chart.</p>
<p><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-04-2022/shanghaicovid.drawio.png" alt="Overall Design"  /></p>
<p>To start, I created the backend API using Cloudflare Worker. Cloudflare provided a <a class="gblog-markdown__link" href="https://developers.cloudflare.com/pages/tutorials/build-an-api-with-workers/">very detailed document</a> for this exact purpose. I simply follow the first half of this document to build up my API. There are a few small issues you might run into if you try to follow the exact steps described in the document. Let me highlight them for you here.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="install-wrangler">
        Install wrangler
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#install-wrangler" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Install wrangler" href="#install-wrangler">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>The official document indicates to use <code>npm i @cloudflare/wrangler -g</code>, which can cause permission issue in Linux. Instead, use <code>yarn global add @cloudflare/wrangler</code> to install the package.</p>
<div class="gblog-post__anchorwrap">
    <h2 id="setup-github-pages-for-front-end">
        Setup GitHub Pages for Front-end
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#setup-github-pages-for-front-end" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Setup GitHub Pages for Front-end" href="#setup-github-pages-for-front-end">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>The Cloudflare document provides steps to deploy the Front-end with <a class="gblog-markdown__link" href="https://developers.cloudflare.com/pages">Cloudflare Pages</a> service. Which in my case is not possible, as Page in my account has already configured for this very blog. A great alternative is GitHub Pages (Yes, I know. Another &ldquo;Pages&rdquo;, people need to be more creative in naming their products). Here are the steps I took to set the repo up.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="create-the-project-in-github-repo">
        Create the project in GitHub Repo
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#create-the-project-in-github-repo" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Create the project in GitHub Repo" href="#create-the-project-in-github-repo">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>Nothing fancy here, just create a <code>public</code> repo as per usual. Initial the repo with <code>npx create-react-app .</code> and then start coding.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="configure-github-pages">
        Configure GitHub Pages
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#configure-github-pages" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Configure GitHub Pages" href="#configure-github-pages">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>Install <code>gh-pages</code> as part of the dev packages with <code>yarn add -d gh-pages</code>.</p>
<p><code>gh-pages</code> will automatically create the <code>gh-branch</code> which hosts the post-build files that ready to be published through GitHub Pages.</p>
<p>In <code>package.json</code>, add following lines:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-diff" data-lang="diff"><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">  &#34;name&#34;: &#34;shanghai-rona-front&#34;,
</span></span><span class="line"><span class="cl"><span class="gi">+ &#34;homepage&#34;: &#34;https://shanghaicovid.xyz&#34;,
</span></span></span><span class="line"><span class="cl"><span class="gi"></span>  &#34;version&#34;: &#34;0.1.0&#34;,
</span></span><span class="line"><span class="cl">...
</span></span><span class="line"><span class="cl">&#34;scripts&#34;: {
</span></span><span class="line"><span class="cl"><span class="gi">+   &#34;predeploy&#34;: &#34;yarn build&#34;,
</span></span></span><span class="line"><span class="cl"><span class="gi">+   &#34;deploy&#34;: &#34;gh-pages -d build&#34;,
</span></span></span><span class="line"><span class="cl"><span class="gi"></span>    &#34;start&#34;: &#34;react-scripts start&#34;,
</span></span></code></pre></div><div class="gblog-post__anchorwrap">
    <h3 id="setup-custom-domain">
        Setup Custom Domain
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#setup-custom-domain" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Setup Custom Domain" href="#setup-custom-domain">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>By default GitHub Pages use the <code>foo.github.io/project-name</code> as the public domain name. To change this, first create a CNAME record for <code>www.shanghaicovid.xyz</code> in Cloudflare and point it to <code>tomkingchen.github.io</code>. Next, create a txt file named as <code>CNAME</code> and save it in <code>./public</code> folder along with other static files in the folder.</p>
<p>In the file, simply put in the custom domain name.</p>
<pre tabindex="0"><code>www.shanghaicovid.xyz
</code></pre><p>The file will be picked up by <code>gh-pages</code> during deploy process.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="publish-the-site">
        Publish the site
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#publish-the-site" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Publish the site" href="#publish-the-site">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>To publish the React site, we run <code>yarn deploy</code> which will kick off the local build and update <code>gh-pages</code> branch. Once that&rsquo;s done, run <code>git push</code> to push the local branch to GitHub. GitHub will then configure Pages based on contents in the <code>gh-pages</code> branch.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="setup-cicd-using-github-actions">
        Setup CICD using GitHub Actions
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#setup-cicd-using-github-actions" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Setup CICD using GitHub Actions" href="#setup-cicd-using-github-actions">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>One of the advantages to use GitHub Pages is we can automate the CICD build using GitHub Actions. To do so, create <code>.github/workflows/</code> folder. Create a <code>deploy.yml</code> file in it. Below are the steps I use in my pipeline. The only tricky bit is how to push changes to <code>gh-pages</code> branch within the pipeline. Fortuantely I found this <code>ghaction-github-pages</code> action does exactly that.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">on</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="l">push, pull_request]</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">jobs</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">deploy</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy GitHub Page</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l">ubuntu-latest</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">timeout-minutes</span><span class="p">:</span><span class="w"> </span><span class="m">30</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">steps</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Checkout Repo</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">actions/checkout@master</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Setup Node.js 16.14.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">actions/setup-node@master</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">node-version</span><span class="p">:</span><span class="w"> </span><span class="m">16.14.0</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Install Dependencies</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l">yarn --frozen-lockfile</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l">yarn build</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span>- <span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">Deploy</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l">crazy-max/ghaction-github-pages@v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">with</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">target_branch</span><span class="p">:</span><span class="w"> </span><span class="l">gh-pages</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">build_dir</span><span class="p">:</span><span class="w"> </span><span class="l">build</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">        </span><span class="nt">env</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">          </span><span class="nt">GITHUB_TOKEN</span><span class="p">:</span><span class="w"> </span><span class="l">${{ secrets.GITHUB_TOKEN }}</span><span class="w">
</span></span></span></code></pre></div><div class="gblog-post__anchorwrap">
    <h2 id="source-code">
        Source Code
        <a data-clipboard-text="/posts/build-full-stack-webapp-using-worker-and-github-pages/#source-code" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Source Code" href="#source-code">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>You can find the front-end source code from <a class="gblog-markdown__link" href="https://github.com/tomkingchen/shanghai-rona-front">here</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup Kubernetes Cluster from scratch]]></title>
            <link href="/posts/setup-kubernetes-cluster/?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/posts/setup-kubernetes-cluster/</id>
            
            <published>2022-01-28T17:09:43+11:00</published>
            <updated>2022-01-29T16:25:15+11:00</updated>
            
            
            <content type="html"><![CDATA[<p>This post I will try to go through the steps I took to build a Kubernetes cluster from scratch.</p>
<p>The physcial host is an old Dell Latitude laptop with 8GB memory, which runs VMware ESXi 6.7. The plan is to run 3 nodes on it with one of the VM set as master. Each server runs Ubuntu 20.04. I will skip the VM building steps here as our focus is Kubernetes.</p>
<p><strong>Note</strong>
Avoid installing any additional compoenents offered by Ubuntu. E.g. docker. Ubuntu uses Snap to install some of those packages, which could cause problem for us down the line.</p>
<div class="gblog-post__anchorwrap">
    <h2 id="linux-configuration-all-nodes">
        Linux configuration (All Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#linux-configuration-all-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Linux configuration (All Nodes)" href="#linux-configuration-all-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Update OS and install some required packages.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo apt-get update
</span></span><span class="line"><span class="cl">sudo apt install ca-certificates curl gnupg lsb-release
</span></span></code></pre></div><div class="gblog-post__anchorwrap">
    <h2 id="disable-swap-all-nodes">
        Disable Swap (All Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#disable-swap-all-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Disable Swap (All Nodes)" href="#disable-swap-all-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>In order for Kubelet to work properly, we need to disable Swap on each node.
First, check if Swap is on.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo swapon --show
</span></span></code></pre></div><p>By default Swap is on as shown below</p>
<pre tabindex="0"><code>NAME       TYPE SIZE USED PRIO
/swap/file file   2G   0B   -2
</code></pre><p>Turn it of by run</p>
<pre tabindex="0"><code>sudo swapoff -a
sudo rm /swap.img
</code></pre><p>Disable swap on starup by comment it out in <code>etc/fstab</code></p>
<pre tabindex="0"><code>sudo sed -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="configure-iptable-all-nodes">
        Configure Iptable (All Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#configure-iptable-all-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Configure Iptable (All Nodes)" href="#configure-iptable-all-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>One prerequisite we need to set on Kubernetes nodes is to allow iptables to see bridged traffic.
First, we need to check if <code>br_netfilter</code> module is loaded.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">lsmod <span class="p">|</span> grep br_netfilter
</span></span></code></pre></div><p>If answer is no, load it with this command.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">sudo modprobe br_netfilter
</span></span></code></pre></div><p>Next is to set <code>net.bridge.bridge-nf-call-iptables</code> to 1 in <code>sysctl</code> config.</p>
<pre tabindex="0"><code>sudo tee /etc/sysctl.d/kubernetes.conf&lt;&lt;EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
</code></pre><p>Run <code>sudo sysctl --system</code> to reload sysctl.</p>
<div class="gblog-post__anchorwrap">
    <h2 id="install-docker-all-nodes">
        Install Docker (All Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#install-docker-all-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Install Docker (All Nodes)" href="#install-docker-all-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>There are a few options when comes to container runtime for Kubernetes. I am personally most familiar with Docker and it tends to be easier to setup.</p>
<pre tabindex="0"><code>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo   &#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
 $(lsb_release -cs) stable&#34; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null
sudo apt update
sudo apt install docker-ce docker-ce-cli containerd.io
</code></pre><p>Next we need to create <code>/etc/docker/daemon.json</code> with the contents below. This is to set Docker cgroup driver as systemd.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-JSON" data-lang="JSON"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;exec-opts&#34;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&#34;native.cgroupdriver=systemd&#34;</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;log-driver&#34;</span><span class="p">:</span> <span class="s2">&#34;json-file&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;log-opts&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;max-size&#34;</span><span class="p">:</span> <span class="s2">&#34;100m&#34;</span>
</span></span><span class="line"><span class="cl">  <span class="p">},</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;storage-driver&#34;</span><span class="p">:</span> <span class="s2">&#34;overlay2&#34;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>Run these command to reload Docker</p>
<pre tabindex="0"><code>sudo systemctl daemon-reload
sudo systemctl restart docker
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="install-kubeadm-and-other-necessities-all-nodes">
        Install Kubeadm and other necessities (All Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#install-kubeadm-and-other-necessities-all-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Install Kubeadm and other necessities (All Nodes)" href="#install-kubeadm-and-other-necessities-all-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Run the commands below to install kubeadm, kubelet and kubectl.</p>
<pre tabindex="0"><code>sudo apt install -y apt-transport-https
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo &#34;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34; | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update`
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="initialize-the-control-plane-node-master-node">
        Initialize the control-plane node (Master Node)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#initialize-the-control-plane-node-master-node" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Initialize the control-plane node (Master Node)" href="#initialize-the-control-plane-node-master-node">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Run <code>kubeadm init</code> to initialize the cluster control plane node. <code>192.168.0.0/16</code> is the network range I allocated for pod network. <code>10.1.1.10</code> is the IP address of the control plane node. It is in <code>10.1.1.0/24</code> range, which is where other worker nodes sit in.</p>
<pre tabindex="0"><code> sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --apiserver-advertise-address=10.1.1.10
</code></pre><p>Record the output with <code>kubeadm join</code> details</p>
<pre tabindex="0"><code>kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="set-permission-for-kubectl">
        Set permission for kubectl
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#set-permission-for-kubectl" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Set permission for kubectl" href="#set-permission-for-kubectl">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>This is to allow kubectl work for non-root users.</p>
<pre tabindex="0"><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="install-pod-network-addon-cni-master-node">
        Install Pod Network Addon CNI (Master Node)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#install-pod-network-addon-cni-master-node" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Install Pod Network Addon CNI (Master Node)" href="#install-pod-network-addon-cni-master-node">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>If you run <code>kubectl get nodes</code> now, you will notice the master node is in <code>NotReady</code> status. This is because we have not yet provisioned the pod network.</p>
<pre tabindex="0"><code>$ kubectl get nodes
NAME       STATUS     ROLES                  AGE   VERSION
tom-lab1   NotReady   control-plane,master   72m   v1.23.1
</code></pre><p>There are a lot choices when come to pick a Pod Network Addon. In my case, I use <code>Weave Net</code> simply due to its simplicity 😅.
To do that just run</p>
<pre tabindex="0"><code>kubectl apply -f &#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &#39;\n&#39;)&#34;
</code></pre><p>After a few minutes, check the node status again. This time it show as <code>Ready</code>.</p>
<pre tabindex="0"><code>$ kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
tom-lab1   Ready    control-plane,master   73m   v1.23.1
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="join-the-worker-nodes-workers-nodes">
        Join the worker nodes (Workers Nodes)
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#join-the-worker-nodes-workers-nodes" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Join the worker nodes (Workers Nodes)" href="#join-the-worker-nodes-workers-nodes">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Jump onto the Worker nodes and run the command below to join them to the cluster.</p>
<pre tabindex="0"><code>kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="test">
        Test
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#test" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Test" href="#test">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Run <code>kubectl get nodes</code> to check the nodes status. If you see something like below, congratulation👏🏼, you just successfully created you own Kubernetes cluster!!! Sky is the limit for you🚀!</p>
<pre tabindex="0"><code>NAME       STATUS   ROLES                  AGE     VERSION
tom-lab1   Ready    control-plane,master   19h     v1.23.1
tom-lab3   Ready    &lt;none&gt;                 7m35s   v1.23.1
</code></pre><div class="gblog-post__anchorwrap">
    <h2 id="troubleshooting-tips-and-tricks">
        Troubleshooting Tips and Tricks
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#troubleshooting-tips-and-tricks" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Troubleshooting Tips and Tricks" href="#troubleshooting-tips-and-tricks">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>To reverse the kubeadm init or kubeadm join</p>
<pre tabindex="0"><code>sudo kubeadm reset
</code></pre><p>Ultimately, try a reboot of the node.</p>
<p><img src="https://media.giphy.com/media/Oe4V14aLzv7JC/giphy.gif" alt=""  /></p>
<div class="gblog-post__anchorwrap">
    <h2 id="references">
        References
        <a data-clipboard-text="/posts/setup-kubernetes-cluster/#references" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor References" href="#references">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p><a class="gblog-markdown__link" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[I wrote a Cloudflare CLI tool]]></title>
            <link href="/posts/cloudflare-cli-tool/?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/posts/cloudflare-cli-tool/</id>
            
            <published>2021-10-16T11:50:12+10:00</published>
            <updated>2021-10-16T15:07:37+11:00</updated>
            
            
            <content type="html"><![CDATA[<p>I just wrote a Cloudflare CLI tool called <code>flare</code> 🔥! Check it out from my GitHub repo <a class="gblog-markdown__link" href="git@github.com:tomkingchen/cloudflare-cli.git">git@github.com:tomkingchen/cloudflare-cli.git</a>.</p>
<p>The tool does some basic queries to Cloudflare API and retrieves information based on the parameters provided.</p>
<p>The reason for creating the tool is mainly to help myself to quickly identify information hard to find through Cloudflare dashboard like Firewall rule ID.</p>
<p>The current version is to focus on display information only rather than modifing configuration within Cloudflare. At work we use Terraform does most of the provisioning and reconfiguration of Cloudflare. This ensures proper change control and versioning. So there is no point to introduce another tooling for modifying configurations.</p>
<p>The tool is written in Python (3.8) with the use of <a class="gblog-markdown__link" href="https://click.palletsprojects.com">Click</a> package. With <code>Click</code> I was able to easily create a AWS CLI style command line tool.</p>
<p>One of the use case is to get a list of all Firewall rules from Cloudflare and put them into a CSV file. I can then use this CSV as a lookup table in <code>Splunk</code> or other log analytic platforms. This enables us to live match WAF rule IDs obtained from <code>firewall logs</code> with rule description.</p>
<p>Here is the commands to generate the csv file.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-Bash" data-lang="Bash"><span class="line"><span class="cl"><span class="nv">ZONES</span><span class="o">=</span><span class="k">$(</span>flare list-zones <span class="p">|</span>jq -r <span class="s1">&#39;.[].id&#39;</span><span class="k">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> zone in <span class="nv">$ZONES</span><span class="p">;</span> <span class="k">do</span>
</span></span><span class="line"><span class="cl">  flare list-fw-rules --zoneid <span class="nv">$zone</span> <span class="p">|</span>jq -r <span class="s1">&#39;.result[] | {rule_id: .id, rule_name: .description}&#39;</span>
</span></span><span class="line"><span class="cl"><span class="k">done</span>
</span></span></code></pre></div><p>Hope you find this tool somewhat useful. Feel free to drop in a PR if you have any awesome ideas want to add to it!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Bye Bye Google Blogger🖐, Hello Cloudflare Pages😘]]></title>
            <link href="/posts/move-to-cloudflare/?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/posts/move-to-cloudflare/</id>
            
            <published>2021-09-04T11:50:12+10:00</published>
            <updated>2021-09-19T15:38:46+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I had enough of Google Blogger! It has terrible editing UI for and I constantly receiving SPAM comments for my posts😤. Time to move my blog off to somewhere better! I was thinking to run a Wordpress server. Though I think it&rsquo;s way cooler to run my blog simplely without worrying about backend infrastructure.</p>
<p>After look around, I ended up using Cloudflare Pages to publish my blog. This allows me to write my posts with <code>Markdown</code> and then generate static html pages using <a class="gblog-markdown__link" href="gohugo.io">Hugo</a>. The pages are then published using GitHub flow which in turn triggers a new build in Cloudflare Page. It&rsquo;s totally version controlled and I don&rsquo;t need to worry about maintaining any infrastructure.</p>
<p>So here&rsquo;s how I set everything up.</p>
<ol>
<li>Install <code>Hugo</code>.</li>
<li>Creat a new GitHub repo.</li>
<li>Clone the repo to local.</li>
<li>Create a new site using Hugo</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hugo new site sitename
</span></span></code></pre></div><ol start="5">
<li>Download a theme to <code>myblog/sitename/themes</code> folder. I choose <a class="gblog-markdown__link" href="https://themes.gohugo.io/themes/hugo-geekblog/">hugo-geekblog</a>. You can choose whichever you like. Though be aware they all work slightly different.</li>
<li>Create a Cloudflare Page</li>
<li>Configure the Cloudflare Page build settings as below.</li>
</ol>
<pre tabindex="0"><code>Production branch: main
Preview branches: All non-production branches
Build command: hugo
Build output directory: /public
Root directory: /
</code></pre><ol start="8">
<li>I have to specify Hugo Version as <code>hugo-geekblog</code> theme only supports hugo version 0.65+.</li>
</ol>
<pre tabindex="0"><code>HUGO_VERSION 0.87.0
</code></pre><ol start="9">
<li>Create a post by running:</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">hugo new post/postname.md
</span></span></code></pre></div><ol start="10">
<li>This creates a md file under <code>content/posts/postname.md</code>.</li>
<li>Once complete the post commit and push the changes to GitHub repo.</li>
<li>To redirect traffic from <code>tomking.xyz</code> to my new site, simply setup a custom domain in <code>Pages</code>.</li>
</ol>
<p>To migrate all my existing contents from Blogger, I use <a class="gblog-markdown__link" href="https://github.com/palaniraja/blog2md">blog2md</a>. From what I can see most pictures were retained!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup Cloudflare for S3 Bucket]]></title>
            <link href="/2021/08/setup-cloudflare-for-s3-bucket.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/08/setup-cloudflare-for-s3-bucket.html</id>
            
            <published>2021-08-14T22:38:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>One way to improve website performance is to use CDN to distribute the static contents of your site. S3 is a common place to host such type contents. In this post, I will show you how to publish a S3 bucket using Cloudflare. In fact, the screen shots used in this blogpost is published exactly through this manner.</p>
<div class="gblog-post__anchorwrap">
    <h2 id="setup-s3-bucket-permissions">
        Setup S3 Bucket Permissions
        <a data-clipboard-text="/2021/08/setup-cloudflare-for-s3-bucket.html#setup-s3-bucket-permissions" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Setup S3 Bucket Permissions" href="#setup-s3-bucket-permissions">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>This is an optional step which adds a S3 bucket policy to your bucket. This restricts S3 access to  Cloudflare IPs only. It&rsquo;s still up to debate whether this is worth doing or not. But I believe it&rsquo;s always a best practice to reduce the public access surface whenever you can.</p>
<p>{ </p>
<p>    &ldquo;Version&rdquo;: &ldquo;2012-10-17&rdquo;, </p>
<p>    &ldquo;Statement&rdquo;: [ </p>
<p>        { </p>
<p>            &ldquo;Sid&rdquo;: &ldquo;PublicReadGetObject&rdquo;, </p>
<p>            &ldquo;Effect&rdquo;: &ldquo;Allow&rdquo;, </p>
<p>            &ldquo;Principal&rdquo;: &ldquo;*&quot;, </p>
<p>            &ldquo;Action&rdquo;: &ldquo;s3:GetObject&rdquo;, </p>
<p>            &ldquo;Resource&rdquo;: &ldquo;arn:aws:s3:::YOUR_BUCKET_NAME/*&quot;, </p>
<p>            &ldquo;Condition&rdquo;: { </p>
<p>                &ldquo;IpAddress&rdquo;: { </p>
<p>                    &ldquo;aws:SourceIp&rdquo;: [ </p>
<p>                        &ldquo;2400:cb00::/32&rdquo;, </p>
<p>                        &ldquo;2606:4700::/32&rdquo;, </p>
<p>                        &ldquo;2803:f800::/32&rdquo;, </p>
<p>                        &ldquo;2405:b500::/32&rdquo;, </p>
<p>                        &ldquo;2405:8100::/32&rdquo;, </p>
<p>                        &ldquo;2a06:98c0::/29&rdquo;, </p>
<p>                        &ldquo;2c0f:f248::/32&rdquo;, </p>
<p>                        &ldquo;173.245.48.0/20&rdquo;, </p>
<p>                        &ldquo;103.21.244.0/22&rdquo;, </p>
<p>                        &ldquo;103.22.200.0/22&rdquo;, </p>
<p>                        &ldquo;103.31.4.0/22&rdquo;, </p>
<p>                        &ldquo;141.101.64.0/18&rdquo;, </p>
<p>                        &ldquo;108.162.192.0/18&rdquo;, </p>
<p>                        &ldquo;190.93.240.0/20&rdquo;, </p>
<p>                        &ldquo;188.114.96.0/20&rdquo;, </p>
<p>                        &ldquo;197.234.240.0/22&rdquo;, </p>
<p>                        &ldquo;198.41.128.0/17&rdquo;, </p>
<p>                        &ldquo;162.158.0.0/15&rdquo;, </p>
<p>                        &ldquo;172.64.0.0/13&rdquo;, </p>
<p>                        &ldquo;131.0.72.0/22&rdquo;, </p>
<p>                        &ldquo;104.16.0.0/13&rdquo;, </p>
<p>                        &ldquo;104.24.0.0/14&rdquo; </p>
<p>                    ] </p>
<p>                } </p>
<p>            } </p>
<p>        } </p>
<p>    ] </p>
<p>}</p>
<p>To allow Internet visitors to access the contents, you do need to turn off Block all public access restriction. This of course is not recommended, if you host sensitive contents in the bucket other than just pictures and CSS. You should consider separate these contents into different buckets if this is the case.</p>
<p><a class="gblog-markdown__link--raw" href="https://blogfiles.tomking.xyz/15-08-2021/s3publicblock.png"><img src="https://blogfiles.tomking.xyz/15-08-2021/s3publicblock.png" alt=""  /></a></p>
<div class="gblog-post__anchorwrap">
    <h2 id="change-request-header-in-cloudflare">
        Change Request Header in Cloudflare
        <a data-clipboard-text="/2021/08/setup-cloudflare-for-s3-bucket.html#change-request-header-in-cloudflare" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Change Request Header in Cloudflare" href="#change-request-header-in-cloudflare">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>As indicated in this AWS document (<a class="gblog-markdown__link" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html</a>), S3 only allows request headers with hostname set to its bucket name. In order to access the bucket using our own domain name (blogfiles.tomking.xyz), we have to either</p>
<ul>
<li>Rename the bucket to blogfiles.tomking.xyz</li>
<li>Or, change the request header, so its hostname is changed to the S3 bucket name.</li>
</ul>
<p>The drawback of option 1 is you will have to use HTTP as S3 static site does not support HTTPS. Plus you will have to rename URLs if the bucket name is changed. So to me, the preferred option is to change the header.</p>
<p>If you have a business account, this can be easily done by creating a <strong>Page Rule</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="https://blogfiles.tomking.xyz/15-08-2021/pagerule.png"><img src="https://blogfiles.tomking.xyz/15-08-2021/pagerule.png" alt=""  /></a></p>
<p>But if you are poor like me using a free account, then we will have to create a Worker to do this. Below is the code to achieve the same as the Page Rule.</p>
<p>async function handleRequest(request) {</p>
<p>  // Override host header with S3 URL </p>
<p>  let newurl = new URL(request.url);</p>
<p>  newurl.hostname = &ldquo;YOUR_BUCKET_NAME.s3.ap-southeast-1.amazonaws.com&rdquo;;</p>
<p>  const newRequest = new Request(</p>
<p>    newurl,</p>
<p>    request,</p>
<p>   )</p>
<p>   try {</p>
<p>     return await fetch(newRequest)</p>
<p>   } catch (e) {</p>
<p>     return new Response(JSON.stringify({ error: e.message }), { status: 500 })</p>
<p>   }</p>
<p> }</p>
<p> addEventListener(&ldquo;fetch&rdquo;, event =&gt; {</p>
<p>  event.respondWith(handleRequest(event.request))</p>
<p>})</p>
<div class="gblog-post__anchorwrap">
    <h2 id="create-a-cname-record-for-the-bucket">
        Create a CNAME Record for the Bucket
        <a data-clipboard-text="/2021/08/setup-cloudflare-for-s3-bucket.html#create-a-cname-record-for-the-bucket" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Create a CNAME Record for the Bucket" href="#create-a-cname-record-for-the-bucket">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h2>
</div><p>Depends on where you host the DNS record for the bucket, these are the steps to cutover the traffic to Cloudflare.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="dns-hosted-in-cloudflare">
        DNS hosted in Cloudflare
        <a data-clipboard-text="/2021/08/setup-cloudflare-for-s3-bucket.html#dns-hosted-in-cloudflare" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor DNS hosted in Cloudflare" href="#dns-hosted-in-cloudflare">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><ol>
<li>Create a CNAME record as below, which points the subdomain to the S3 bucket.</li>
<li></li>
</ol>
<pre><code>[![](https://blogfiles.tomking.xyz/15-08-2021/cname.png)](https://blogfiles.tomking.xyz/15-08-2021/cname.png)
</code></pre>
<div class="gblog-post__anchorwrap">
    <h3 id="dns-not-hosted-in-cloudflare">
        DNS not hosted in Cloudflare
        <a data-clipboard-text="/2021/08/setup-cloudflare-for-s3-bucket.html#dns-not-hosted-in-cloudflare" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor DNS not hosted in Cloudflare" href="#dns-not-hosted-in-cloudflare">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><ol>
<li>Create a CNAME record in <strong>Cloudflare</strong> as shown above.</li>
<li>Create a CNAME record in <strong>your own DNS provider</strong> as below, which points the subdomain to Cloudflare DNS server.</li>
</ol>
<p>blogfiles 300 IN CNAME blogfiles.tomking.xyz.cdn.cloudflare.net.</p>
<p>That&rsquo;s it, now you should be able to browse the S3 contents using the custom domain name, which is going through Cloudflare CDN.</p>
<p>A useful test is using cURL command below. As you can see it got a 200 response back and confirmed it&rsquo;s a MISS, and server is cloudflare .</p>
<p>➜ ~ curl <a class="gblog-markdown__link" href="https://blogfiles.tomking.xyz/29-07-2021/cloudflare.png">https://blogfiles.tomking.xyz/29-07-2021/cloudflare.png</a> -v</p>
<p>* Trying 104.21.12.143&hellip;</p>
<p>* TCP_NODELAY set</p>
<p>* Connected to blogfiles.tomking.xyz port 443 (#0)</p>
<p>* ALPN, offering h2</p>
<p>* ALPN, offering http/1.1</p>
<p>* successfully set certificate verify locations:</p>
<p>&hellip;</p>
<p>* Using Stream ID: 1 (easy handle 0x55cb8d261580)</p>
<p>* TLSv1.3 (OUT), TLS Unknown, Unknown (23):</p>
<p>&gt; GET /29-07-2021/cloudflare.png HTTP/2</p>
<p>&gt; Host: blogfiles.tomking.xyz</p>
<p>&gt; User-Agent: curl/7.58.0</p>
<p>&gt; Accept: */*</p>
<p>&gt;</p>
<p>&hellip;</p>
<p>&lt; HTTP/2 200</p>
<p>&lt; date: Sun, 15 Aug 2021 05:27:06 GMT</p>
<p>&hellip;</p>
<p>&lt; cf-cache-status: MISS</p>
<p>&hellip;</p>
<p>&lt; server: cloudflare</p>
<p>&hellip;</p>
<p>* Connection #0 to host blogfiles.tomking.xyz left intact</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup Cloudflare for AWS API Gateway]]></title>
            <link href="/2021/06/setup-cloudflare-for-aws-api-gateway.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/06/setup-cloudflare-for-aws-api-gateway.html</id>
            
            <published>2021-06-27T00:25:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p> In this post I will show how to setup Cloudflare for a Serverless app built with AWS API Gateway and Lambda. For demonstration, I use a simple web app I built (rona.tomking.xyz). The app is hosted in  AWS Sydney region. It displays daily Victoria COVID case and that&rsquo;s it.</p>
<p>To use Cloudflare, I have signed up a free Cloudflare account. The <strong>first site</strong> can be added for free with following features.</p>
<ul>
<li>DNS hosting</li>
<li>Free SSL certificates</li>
<li>CDN</li>
<li>DDoS attacks mitigation up to 67 Tbps capacity</li>
<li>Up to 100k workers requests and 30 scripts</li>
<li>3 Page rules</li>
</ul>
<p>To allow Cloudflare become the first ingress point for our web app, the first step is to change our DNS Nameserver to Cloudflare&rsquo;s DNS NS records. In Cloudflare portal, click + Add site</p>
<p>, and type the site TLD, in our case it&rsquo;s tomking.xyz. Cloudflare will display the NS records set for the site. As I got my tomking.xyz from GoDaddy, I then need to log into GoDaddy DNS Zone management portal and change the nameservers from GoDaddy&rsquo;s NS records to Cloudflare&rsquo;s NS. By default Cloudflare will pickup all existing records, so you shouldn&rsquo;t need to recreate the records already there.</p>
<p><a class="gblog-markdown__link--raw" href="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/nameservers.png"><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/nameservers.png" alt=""  /></a></p>
<p>Once confirm the NS change worked successfully, the next step is to create a certificate using AWS ACM. The certificate will be used by API Gateway Custom Domain to verify the redirection.</p>
<ul>
<li>In ACM, click <strong>Request a certificate</strong> and choose to <strong>Request a public certificate</strong>.</li>
<li>Add your site&rsquo;s domain name to the certificate. In my case it is rona.tomking.xyz.</li>
<li>Choose <strong>DNS validation</strong> and add a Name tag to the certificate.</li>
<li>Submit the certificate request and you should be given a TXT record to be added to your DNS zone.</li>
<li>Add the TXT record in Cloudflare and wait for ACM to validate the record. This should take no longer than an hour.</li>
</ul>
<p>After the certificate status changed to Issued you can now setup <strong>Custom domain name</strong> for the API Gateway.</p>
<ul>
<li>In API Gateway, under <strong>Custom domain name</strong> click <strong>Create</strong> to create a new custom domain.</li>
<li>Type in the domain name, which is rona.tomking.xyz for the demo.</li>
<li>Use <strong>TLC 1.2</strong> as recommended TLS version.</li>
<li>Choose the ACM certificate we have just created and click <strong>Create domain name</strong>.</li>
<li>Under API mapping select the API you want to associate with the custom domain name.</li>
<li>Record the API Gateway domain name for the mapped custom domain.</li>
</ul>
<p><a class="gblog-markdown__link--raw" href="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/endpointconfig.png"><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/endpointconfig.png" alt=""  /></a></p>
<p>Come back to Cloudflare DNS portal, add a CNAME record for the mapped API gateway.</p>
<p><a class="gblog-markdown__link--raw" href="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/cname.png"><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/cname.png" alt=""  /></a></p>
<p>That&rsquo;s it! Try hit the URL a few times and you should see traffic coming through Cloudflare and show up under <strong>Analytics</strong> tab.</p>
<p><a class="gblog-markdown__link--raw" href="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/analytic.png"><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/analytic.png" alt=""  /></a></p>
<p>Look at the certificate of rona.tomking.xzy, it&rsquo;s using Cloudflare SNI certificate.</p>
<p><a class="gblog-markdown__link--raw" href="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/snicert.png"><img src="https://tomking-blog-files.s3.ap-southeast-2.amazonaws.com/27-06-2021/snicert.png" alt=""  /></a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Automate EC2 Instance Security Group Rules Update]]></title>
            <link href="/2021/06/automate-ec2-instance-security-group.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/06/automate-ec2-instance-security-group.html</id>
            
            <published>2021-06-04T22:47:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ever come into the situation where you need to whitelist a long list of IPs for a EC2 instance? It can be painful to add them manually one by one. On top of that, what if these IPs change on a regular basis? You are in luck! I will show you how to update Security Group rules automatically using Python.</p>
<p>Here&rsquo;s my use case. I got an EC2 instance takes syslog feeds from VMWare&rsquo;s WorkspaceOne. For security sake, it should only allow IPs owned by VMware SaaS solution, which are all listed in this <a class="gblog-markdown__link" href="https://kb.vmware.com/s/article/2960995">KB</a>. There are around 990 IPs and subnets need to be added. Plus, they are constantly get updated.</p>
<p>The solution I put in place is to scrape the IPs off the KB webpage, and then add them into security groups attached to the EC2 instance. Sounds like a piece of cake, right? Well, there are a few huddles we need to address first.</p>
<p>First, the VMware KB page is not a static HTML page. The IP list is rendered from Javascript. If we just use Requests library, it will return None as the page is not rendered. To overcome that, I use Selenium&rsquo;s ChromeDriver to mimic a browser request and get the rendered HTML content. The code below shows how this is achieved.</p>
<p>options = Options()</p>
<p>options.headless = True</p>
<p>driver = webdriver.Chrome(&rsquo;/usr/bin/chromedriver&rsquo;, options=options)</p>
<p>driver.get(&lsquo;<a class="gblog-markdown__link" href="https://kb.vmware.com/s/article/2960995%27">https://kb.vmware.com/s/article/2960995'</a>)</p>
<p>time.sleep(5)</p>
<p>page = driver.page_source </p>
<p>The next step is to retrieve the IP list from the HTML page using BeautifulSoup.</p>
<p>soup = BeautifulSoup(page, &lsquo;html.parser&rsquo;)</p>
<p>page_content = soup.find(id=&lsquo;article_content&rsquo;)</p>
<p>page_containers = page_content.find_all(&lsquo;div&rsquo;, class_=&lsquo;container&rsquo;)</p>
<p>ip_list_content = page_containers[1].find(&lsquo;div&rsquo;, class_ = &lsquo;content&rsquo;)</p>
<p>There is a soft limit of 5 Security Groups per network interface in AWS. Each Security Group by default can contain no more than 60 ingress rules. With over 900 IPs in the VMware KB page, we need to reach out to AWS Support uplift those software limits first. I end up raise the limit to 10 x SGs per interface and each SG can contain up to 100 ingress rules.</p>
<p>Once this is in place, we can then create the SGs based on the IP list and attach the SGs to the instance. Existing SGs will be detacched and removed prior the new SG attachment.</p>
<p>Here is the completed code <a class="gblog-markdown__link" href="https://github.com/tomkingchen/auto-sg-update">https://github.com/tomkingchen/auto-sg-update</a>.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup Splunk Universal Forwarder with TLS]]></title>
            <link href="/2021/05/setup-splunk-universal-forwarder-with.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/05/setup-splunk-universal-forwarder-with.html</id>
            
            <published>2021-05-21T18:23:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>One of the best practice to setup Splunk Universal Forwarder (UF) is to encrypt incoming log traffic with TLS. This is especially important if your intake is from an external source on Internet, e.g from a SaaS solution. In this blog I will demostrate the steps to get this setup. </p>
<p>First, we will create a public A DNS record for the UF. This is because our UF will be receiving logs from Internet. </p>
<p>Next, we need to purchase a new TLS certificate for the A record we just registered. Assume the domain name we set for the certificate is syslog.contoso.com. </p>
<p>On the UF, run the command below to generate a CSR to submit to the public CA (DigiCert, GoDaddy&hellip;).</p>
<p>openssl req -new -key syslog.contoso.com.key -out syslog.contoso.com.csr  </p>
<p>Copy the private key to Syslog-ng cert.d folder. The private key is automatically generated along with the CSR.</p>
<p>cp syslog.contoso.com.key /usr/local/etc/syslog-ng/cert.d/syslog_contoso_com_key.pem  </p>
<p>Submit the CSR file to CA and wait for the certificate to be issued. Once you receive the certificate, upload it to the UF server and move them to Syslog-ng cert.d folder. </p>
<p>mv syslog_contoso_com.crt /usr/local/etc/syslog-ng/cert.d/syslog_contoso_com.pem</p>
<p>Once you have the certificate and its private key in place, add following code to Syslog-ng&rsquo;s conf file. You can normally find the config file at /etc/syslog-ng/conf.d/syslog-ng.conf.</p>
<p>source contoso_saasapp_syslog {</p>
<p>network(</p>
<p>ip(0.0.0.0)</p>
<p>port(1999)</p>
<p>transport(&ldquo;tls&rdquo;)</p>
<p>tls(</p>
<p>key-file(&quot;/usr/local/etc/syslog-ng/cert.d/syslog_contoso_com_key.pem&quot;)</p>
<p>cert-file(&quot;/usr/local/etc/syslog-ng/cert.d/syslog_contoso_com.pem&quot;)</p>
<p>peer-verify(optional-untrusted)</p>
<p>)</p>
<p>);</p>
<p>};</p>
<p>Restart Splunk service and the UF should now accept logs with TLS.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use PowerShell to delete SPAM Blogger comments]]></title>
            <link href="/2021/02/work-with-google-oauth2-flow-and.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/02/work-with-google-oauth2-flow-and.html</id>
            
            <published>2021-02-27T21:29:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I haven&rsquo;t been very diligent on maintaining this blog. There has been quite a few SPAM comments accumulated on my posts. I am going to turn on moderation to block those. But I need a way to clean all those existing SPAM comments. So over the weekend, I wrote this PowerShell script to do just that. </p>
<p>In the end, it will probably take less time if I just manually all the cleanup manually. But I believe this is a good opportunity to learn about Google API and OAuth2 flow. Plus, now I have something to write about :)</p>
<p>Google API can be accessed with both API key and OAuth2 token. However, the API key is only allowed for public accessible resources and actions like read a post or comment. Actions like deleting a post or comment require OAuth2 to be setup. Now let&rsquo;s look at how this is achieved using PowerShell.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="setup-the-app-in-google-developer-console">
        <strong>Setup the app in Google Developer Console</strong>
        <a data-clipboard-text="/2021/02/work-with-google-oauth2-flow-and.html#setup-the-app-in-google-developer-console" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Setup the app in Google Developer Console" href="#setup-the-app-in-google-developer-console">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>Go to <a class="gblog-markdown__link" href="https://console.developers.google.com/">https://console.developers.google.com/</a> and setup a new project.</p>
<p>Under <strong>Library</strong>, find <strong>Blogger v3 API</strong> and enable it.</p>
<p>Under <strong>OAuth consent screen</strong>, create new consent screen for your project. </p>
<p>Fill in only the compulsary options, leave the rest and continue.</p>
<p>For scopes, choose the Blogger API v3 blogger scope.</p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-yJf9SHvViB4/YDseOaH0PTI/AAAAAAAAKzA/_bz5Zs1MCk8uxWQLKkS-UqZ13A8M92csgCLcBGAsYHQ/s480/image.png"><img src="https://1.bp.blogspot.com/-yJf9SHvViB4/YDseOaH0PTI/AAAAAAAAKzA/_bz5Zs1MCk8uxWQLKkS-UqZ13A8M92csgCLcBGAsYHQ/s320/image.png" alt=""  /></a></p>
<p>In next screen, don&rsquo;t add any test users and just click <strong>Save and Continue</strong>.</p>
<p>Review the summary and save.</p>
<p>Once the consent screen is created, we can now create OAuth2 Client ID.</p>
<p>For Client ID, all you need to know is that for PowerShell scripts, we will use <strong>Desktop app</strong> as the app type.</p>
<p>Save the Client ID and Client Secret in a secure vault like LastPass or BitWarden.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="get-oauth2-token-using-powershell">
        Get OAuth2 Token using PowerShell
        <a data-clipboard-text="/2021/02/work-with-google-oauth2-flow-and.html#get-oauth2-token-using-powershell" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Get OAuth2 Token using PowerShell" href="#get-oauth2-token-using-powershell">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>I will skip explaining about OAuth2, as it&rsquo;s quite a large topic and I have already done so in my previous post <a class="gblog-markdown__link" href="https://www.tomking.xyz/2018/11/understand-oauth-and-open-id-connect.html">here</a>. Instead, I will walk you through the PowerShell code in details.</p>
<p>To obtain the access token, we need following parameters.</p>
<ul>
<li>Client ID and Client Secret of your app/script, which we created in the console.</li>
<li>Scope of your app (<a class="gblog-markdown__link" href="https://www.googleapis.com/auth/blogger">https://www.googleapis.com/auth/blogger</a>)</li>
<li>Google Authorization Endpoint (<a class="gblog-markdown__link" href="https://accounts.google.com/o/oauth2/v2/auth">https://accounts.google.com/o/oauth2/v2/auth</a>)</li>
<li>Google Token Endpoint (<a class="gblog-markdown__link" href="https://www.googleapis.com/oauth2/v4/token">https://www.googleapis.com/oauth2/v4/token</a>)</li>
</ul>
<p>With these information, we can then form a proper request URI as below:</p>
<p>Start-Process &ldquo;<a class="gblog-markdown__link" href="https://accounts.google.com/o/oauth2/v2/auth?client">https://accounts.google.com/o/oauth2/v2/auth?client</a>_id=$client_id&amp;scope=$scopes&amp;access_type=offline&amp;response_type=code&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&rdquo;</p>
<p>This will initialize an interactive browser session using the supplied URI. Authorize the request with your Google account and you will get the <strong>Authorization Code</strong> in the end.</p>
<p>The code can then be stored as a variable using <strong>Read-Host</strong>.</p>
<p>We then use <strong>Invoke-WebRequest</strong> to send a POST request to the URL below. </p>
<p><a class="gblog-markdown__link" href="https://www.googleapis.com/oauth2/v4/token?client">https://www.googleapis.com/oauth2/v4/token?client</a>_id=$client_id&amp;client_secret=$client_secret&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;code=$code&amp;grant_type=authorization_code</p>
<p>This request will return both the <strong>Access Token</strong> and <strong>Refresh Token</strong>. Normally the access token is only valid for an hour. For our script, this is way more than enough. We will not worry abou the refresh token.</p>
<div class="gblog-post__anchorwrap">
    <h3 id="access-blogger-api-v3-using-oauth2-token">
        Access Blogger API v3 using OAuth2 Token
        <a data-clipboard-text="/2021/02/work-with-google-oauth2-flow-and.html#access-blogger-api-v3-using-oauth2-token" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Access Blogger API v3 using OAuth2 Token" href="#access-blogger-api-v3-using-oauth2-token">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>The access token should be used as a parameter in the API request in the format below.</p>
<p>access_token=your_access_token</p>
<p>In addition to this, we will need following information.</p>
<p>Blog ID, this can be easily identified via your Blogger URL.</p>
<p>Blogger API URI, which is <a class="gblog-markdown__link" href="https://blogger.googleapis.com/v3/blogs">https://blogger.googleapis.com/v3/blogs</a>.</p>
<p>A typical request URI to Blogger API looks like this:</p>
<p><a class="gblog-markdown__link" href="https://blogger.googleapis.com/v3/blogs/$blogid/posts/$postid/comments/$commentid?access">https://blogger.googleapis.com/v3/blogs/$blogid/posts/$postid/comments/$commentid?access</a>_token=your_access_token</p>
<div class="gblog-post__anchorwrap">
    <h3 id="script-logic">
        Script Logic
        <a data-clipboard-text="/2021/02/work-with-google-oauth2-flow-and.html#script-logic" class="gblog-post__anchor gblog-post__anchor--right clip" aria-label="Anchor Script Logic" href="#script-logic">
            <svg class="icon gblog_link"><use xlink:href="#gblog_link"></use></svg>
        </a>
    </h3>
</div><p>With the access to API sorted, the rest are pretty straight forward. I simply check posts with reply field set to more than 0. Then list all the comments of those posts. If the comment contains hyper-link syntax &ldquo;href=&rdquo;, it will be categorized as a SPAM. The script will show all these type of comments and prompt me to confirm whether they should be deleted or not. Upon receiving confirmation, the script then sends a DELETE request to each comments API URI to remove them.</p>
<p>You can find more details about how to form the query from my code:</p>
<p><a class="gblog-markdown__link" href="https://github.com/tomkingchen/blogger-cleaner">https://github.com/tomkingchen/blogger-cleaner</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Ansible to update Splunk Universal Forwarder Configuration]]></title>
            <link href="/2021/02/use-ansible-to-update-splunk-universal.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2021/02/use-ansible-to-update-splunk-universal.html</id>
            
            <published>2021-02-20T20:55:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Today we will look at how to use Ansible to update Splunk UF (Universal Forwarder) configuration. The benefits of using Ansible to achive this are:</p>
<p>- Save the hassel to manually modify conf files of syslog-ng and splunk uf.</p>
<p>- Codify Splunk UF configuratoin, so they can be version controlled via GitHub.</p>
<p>- Automate multiple UFs update without the need to ssh to each single server. </p>
<p>- The playbook can also be used to configure newly provisioned Spunk UF.</p>
<p>Update log inputs on Splunk Universal Forwarder, normally invovles following tasks</p>
<p>1. Modify two configuratoin files:</p>
<p>- Syslog-ng conf file in conf.d/syslog-ng.conf. </p>
<p>- Syslog app inputs config file under Splunk UF installation folder.</p>
<p>2. After the modification, Splunk service needs to be restarted.</p>
<p>For configuration files, we use Jinja2 template to simpilify its format. The Jinja2 templates use parameters supplied from CSV file to populiate the final conf file.</p>
<p>The Ansible playbook converts the template to conf file use template task. The conf file is saved in specified path on the remote server. Existing config files are overwritten if there are changes. </p>
<p>Upon complete the config files update, Ansible use service task to issue a service restart of Splunk.</p>
<p>Code example can be found at my GitHub repo <a class="gblog-markdown__link" href="https://github.com/tomkingchen/ansible-splunkuf-update">https://github.com/tomkingchen/ansible-splunkuf-update</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[OWA and ECP failure after Install Exchange 2016 CU17 ]]></title>
            <link href="/2020/11/owa-and-ecp-failure-after-install.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2020/11/owa-and-ecp-failure-after-install.html</id>
            
            <published>2020-11-02T21:07:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I recently ran into an issue after update Exchange 2016 from CU15 to CU17. The upgrade installation took around an hour, but was eventually completed successfully according to the Installation Wizard at least. When I tried to access ECP, I got the error below even before the login page shows up. At the meantime, Exchange Management Shell is inaccessible due to the error.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-gd0qSlqsU_8/X6DdzzhtAhI/AAAAAAAAKto/4N8H88608cAzJ56dDwd0oNtALMzAi6xtQCLcBGAsYHQ/image.png"><img src="https://lh3.googleusercontent.com/-gd0qSlqsU_8/X6DdzzhtAhI/AAAAAAAAKto/4N8H88608cAzJ56dDwd0oNtALMzAi6xtQCLcBGAsYHQ/image.png" alt=""  /></a></p>
<p>In the eventlog, there are lots of 1003 errors relate to MSExchange Front End HTTP Proxy. </p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-9LuMl4xt0u8/X6DeCvAr6_I/AAAAAAAAKts/p-KDCvE_sKgrkvqF1aVI2EIaVYYmingvACLcBGAsYHQ/image.png"><img src="https://lh3.googleusercontent.com/-9LuMl4xt0u8/X6DeCvAr6_I/AAAAAAAAKts/p-KDCvE_sKgrkvqF1aVI2EIaVYYmingvACLcBGAsYHQ/image.png" alt=""  /></a></p>
<p>After some research, it appears the issue is caused by corrupted SharedWebConfig.config files in C:\Program Files\Microsoft\Exchange Server\V15\FrontEnd\HttpProxy and C:\Program Files\Microsoft\Exchange Server\V15\ClientAccess. But regenerating the files base on <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/exchange/troubleshoot/client-connectivity/event-1309-code-3005-cannot-access-owa-ecp">this MS document</a> didn&rsquo;t fix the issue.</p>
<p>I end up had to setup a test Exchange 2016 CU17 in my own lab environment. Once the new Exchange is up, I copied those 2 SharedWebConfig.config files to the production Exchange server and then did a IISRESET. To my dismay, ECP this time came up with the login page, but after authentication it returned back with a blank page.</p>
<p>Upon checking eventlog, I notice there are bunch of HttpEvent errors. These errors seem to indicate issue is to do with the certificate used for ECP and OWA.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-M3V3TkH3wf8/X6DjTAPxgqI/AAAAAAAAKuI/RBT2fskckWoQJytJdXv4C1E4EAvGUG6eACLcBGAsYHQ/image.png"><img src="https://lh3.googleusercontent.com/-M3V3TkH3wf8/X6DjTAPxgqI/AAAAAAAAKuI/RBT2fskckWoQJytJdXv4C1E4EAvGUG6eACLcBGAsYHQ/image.png" alt=""  /></a></p>
<p>It turns out the ECP backend was somehow binded to a deleted certificate. So the fix is just to set the backend binding to a valid SSL certificate and restart IIS!</p>
<p>I am still not sure how those SharedWebConfig files are corrupted, as the CU installation process completed successfully. But if you need, here is <a class="gblog-markdown__link" href="https://github.com/tomkingchen/exchange2016cu17sharedwebconfig">the link</a> for the good CU17 config files. Save you the hassle to build another Exchange server (It took me hours).</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Package and deploy a PowerShell Lambda function with custom modules]]></title>
            <link href="/2020/06/package-and-deploy-powershell-lambda.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2020/06/package-and-deploy-powershell-lambda.html</id>
            
            <published>2020-06-06T05:20:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I had the need to create a Lambda function with PowerShell 7. The function is to synchronize data between two REST APIs. It&rsquo;s fairly simple, but does need to use a custom made module. I spent quite bit time to find out how to deploy PowerShell Lambdas with custom modules. Thought might write a guide to help people want to do the same.   </p>
<p>My script is fairly simple, it gets a list of users from one API and then convert it to a XML format object and export into the target API. To reuse some of the code, The script requires AWS.Tools module as well as a custom made module, let&rsquo;s call it CAT. The CAT is a module contains some functions that are invoked by the main script. The inital lines of my main script look like below:</p>
<pre tabindex="0"><code>#Requires -Modules @{ModuleName=&#39;AWS.Tools.Common&#39;;ModuleVersion=&#39;4.0.5.0&#39;}  
#Requires -Modules @{ModuleName=&#39;AWS.Tools.SecretsManager&#39;;ModuleVersion=&#39;4.0.5.0&#39;}  
#Requires -Modules CAT 
</code></pre><p>In addition to install the <strong>AWS.Tools</strong> modules, I manually copied my CAT module folder to the <strong>$Env:PSModulePath</strong>.</p>
<p>The nex thing is to package the code into a zip file along with all required modules. To do so, install <strong>AWSLambdaPSCore</strong> module and run this PowerShell command:</p>
<pre tabindex="0"><code>New-AWSPowerShellLambdaPackage -ScriptPath &#34;../functions/FunctionName.ps1&#34; -outputPackage &#34;FunctionName.zip&#34;
</code></pre><p>You should see the script and modules are all packaged as shown below.</p>
<pre tabindex="0"><code>Staging deployment at C:\\Users\\tom\\AppData\\Local\\Temp\\myfunction  
Configuring PowerShell to version 7.0.0  
Generating C# project C:\\Users\\tom\\AppData\\Local\\Temp\\myfunction\\myfunction.csproj used to create Lambda function bundle.  
Generating C:\\Users\\tom\\AppData\\Local\\Temp\\myfunction\\Bootstrap.cs to load PowerShell script and required modules in Lambda environment.  
Generating aws-lambda-tools-defaults.json config file with default values used when publishing project.  
Copying PowerShell script to staging directory  
Copying local module AWS.Tools.Common(4.0.5.0) from C:\\Users\\tom\\OneDrive\\Documents\\PowerShell\\Modules\\AWS.Tools.Common\\4.0.5.0  
Copying local module AWS.Tools.SecretsManager(4.0.5.0) from C:\\Users\\tom\\OneDrive\\Documents\\PowerShell\\Modules\\AWS.Tools.SecretsManager\\4.0.5.0  
Copying local module CAT(0.0.2) from C:\\program files\\powershell\\7\\Modules\\CAT  
Resolved full output package path as C:\\Users\\tom\\Dropbox\\github\\myproject\\deploy\\myfunction.zip  
Creating deployment package at C:\\Users\\tom\\Dropbox\\github\\myproject\\deploy\\myfunction.zip  
Restoring .NET Lambda deployment tool  
Initiate packaging  
When deploying this package to AWS Lambda you will need to specify the function handler. The handler for this package is: myfunction::myfunction.Bootstrap::ExecuteFunction. To request Lambda to invoke a specific PowerShell function in your script specify the name of the PowerShell function in the environment variable AWS\_POWERSHELL\_FUNCTION\_HANDLER when publishing the package.  
LambdaHandler                                         PathToPackage                                                         PowerShellFunctionHandlerEnvVar  
\-------------                                         -------------                                                         -------------------------------  
myfunction::myfunction.Bootstrap::ExecuteFunction C:\\Users\\tom\\Dropbox\\github\\myproject\\deploy\\myfunction.zip AWS\_POWERSHELL\_FUNCTION\_HANDLER
</code></pre><p>The steps below are not required if you have a CICD pipeline built up like me. As soon as I push my updated code and package to GitHub, the pipeline will automatically upload the package and deploy it. But if don&rsquo;t, read on! </p>
<p>The next step is to create a S3 bucket to host your package. You can skip this step if you already have a designated bucket for such purpose.</p>
<p>After created the bucket, we will then create a <strong>SAM template</strong>. SAM template is basically a transformed CloudFormation template. It allows us to add all the neccessary bits like Events and DeadLetterQueue under AWS::Serverless::Function resource. Here&rsquo;s a simple example. Pay close attention to <strong>CodeUri</strong>. Notice it&rsquo;s pointing to the local zip file instead of a S3 URL? This is by purpose. I will explain in next step.</p>
<pre tabindex="0"><code>AWSTemplateFormatVersion: &#39;2010-09-09&#39;  
Transform: &#39;AWS::Serverless-2016-10-31&#39;  
Resources:  
  PopCycler:  
    Type: &#39;AWS::Serverless::Function&#39;  
    Properties:  
      Handler: myfunction::myfunction.Bootstrap::ExecuteFunction  
      Runtime: dotnetcore3.1  
      **CodeUri: &#39;./myfunction.zip&#39;**  
      Role:  
        Fn::GetAtt:  
        - LambdaRole  
        - Arn  
      Environment:  
        Variables:  
          ENV: dev  
          CATURL: https://dev.CAT.com  
          CATAPPID: Abc12345  
          CATSECRETID: CAT-api-key  
          VENDORSECRETID: myproject  
      Events:  
        ScanInterval:  
          Type: Schedule  
          Properties:  
            Schedule: &#39;rate(1 day)&#39;  
      Timeout: 300  
      MemorySize: 256  
      Tags:  
        owner: tom  
  LambdaRole:  
    Type: &#39;AWS::IAM::Role&#39;  
    Properties:  
      RoleName: myproject-lambdarole  
      Tags:  
        -   
          Key: &#39;owner&#39;  
          Value: &#39;myteam&#39;  
      ManagedPolicyArns:  
        - &#39;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole&#39;  
      AssumeRolePolicyDocument:  
        Version: 2012-10-17  
        Statement:  
          - Effect: Allow  
            Principal:  
              Service:  
                - lambda.amazonaws.com  
            Action:  
              - &#39;sts:AssumeRole&#39;  
      Policies:  
        - PolicyName: MyProjectPolicy  
          PolicyDocument:  
            Version: 2012-10-17  
            Statement:  
              - Effect: Allow  
                Action: secretsmanager:GetSecretValue  
                Resource:  
                  - Fn::Sub: &#39;arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:CAT-api-key&#39;  
              - Effect: Allow  
                Action: kms:Decrypt  
                Resource:  
                  - Fn::Sub: &#39;arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:CAT-api-key&#39;
</code></pre><p>After created the template, run the AWS CLI command below. What it does is simply upload the zip file to the specified S3 bucket (myBucket in my example) and update <strong>CodeUri</strong> in your SAM template with the actual S3 URI path to the zip file. Then output a new template for you to use in the next step. You can open up the SAM template <strong>updated.yml</strong> to confirm. </p>
<pre tabindex="0"><code>aws cloudformation package --template-file myfunction.yml --s3-bucket mybucket --output-template-file updated.yml
</code></pre><p>The last step is to deploy the Lambda function from <strong>updated.yml</strong></p>
<pre tabindex="0"><code>aws cloudformation deploy --stack-name myfunctionstack --template-file updated.yml --capabilities CAPABILITY\_AUTO\_EXPAND CAPABILITY\_NAMED\_IAM
</code></pre><p>That&rsquo;s it folks! I know this isn&rsquo;t as straight forward in comparison to Python and Node.Js runtimes. But as I mentioned before, with a CICD system in place, you should be able to automate the whole deploy process fairly easily. Hope you find it useful!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="/tags/PowerShell" term="PowerShell" label="PowerShell" />
                             
                                <category scheme="/tags/AWS" term="AWS" label="AWS" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Test out PowerShell 7 new features in WSL1]]></title>
            <link href="/2020/03/test-out-powershell-7-new-features-in.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2020/03/test-out-powershell-7-new-features-in.html</id>
            
            <published>2020-03-21T17:59:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Finally, PowerShell 7 is now GA! As a heavy WSL user, I was keen to see how some of its new features will work in WSL1 (Ubuntu 4.4.0-18362-Microsoft). Below are the tests I have done.</p>
<p><strong>Installation in WSL</strong><br>
Download the binary from Github repo to a local folder /usr/share/powershell</p>
<pre tabindex="0"><code>sudo wget https://github.com/PowerShell/PowerShell/releases/download/v7.0.0/powershell-7.0.0-linux-x64.tar.gz
</code></pre><p>Untar the file</p>
<p>sudo tar xzvf powershell-7.0.0-linux-x64.tar.gz</p>
<p>Add path for your shell</p>
<p>export PATH=/usr/share/PowerShell:$PATH</p>
<p>Reload .bashrc</p>
<p>source .bashrc</p>
<p>Remove the tar ball</p>
<p>sudo rm /usr/share/PowerShell/powershell-7.0.0-linux-x64.tar.gz</p>
<p>Run PowerShell 7 by run <strong>pwsh</strong></p>
<p><strong>Import Windows Modules in WSL</strong><br>
Install commonly Vendor released modules like VMware PowerCli</p>
<p>Install-Module -Name VMware.PowerCli</p>
<p>Install .Net based modules also works fine.</p>
<p>Install-Package PrtgApi</p>
<p>What about those module require Windows GUI, like Out-GridView? PowerShell 7 on Windows fully support this kind module. But for WSL some tweaks are required.</p>
<p>First Install GraphicalTools</p>
<p>Install-Module Microsoft.PowerShell.GraphicalTools</p>
<p>Tried to run Out-GridView but encountered Exception error.</p>
<p>PS /home/tom&gt;</p>
<p>Unhandled Exception: System.TypeInitializationException: The type initializer for &lsquo;OutGridView.Application.AvaloniaAppRunner&rsquo; threw an exception. &mdash;&gt; System.Exception: XOpenDisplay failed</p>
<p>   at Avalonia.X11.AvaloniaX11Platform.Initialize(X11PlatformOptions options)</p>
<p>   at Avalonia.Controls.AppBuilderBase`1.Setup()</p>
<p>   at OutGridView.Application.AvaloniaAppRunner.BuildAvaloniaApp() in D:\a\1\s\src\OutGridView.Gui\AvaloniaAppRunner.cs:line 34</p>
<p>   at OutGridView.Application.AvaloniaAppRunner..cctor() in D:\a\1\s\src\OutGridView.Gui\AvaloniaAppRunner.cs:line 31</p>
<p>   &mdash; End of inner exception stack trace &mdash;</p>
<p>   at OutGridView.Application.Program.Main(String[] args) in D:\a\1\s\src\OutGridView.Gui\Program.cs:line 26</p>
<p>To walk around that, we need to install XServer in Windows. I use VCSXRV downloaded from SourceForge.</p>
<p>The installation of VCSXRV is just click the next buttons.</p>
<p>Keep VCSXRV running and back in WSL, exit pwsh type the command below</p>
<p>export DISPLAY=localhost:0</p>
<p>That&rsquo;s it!. Let&rsquo;s test it out</p>
<p>Get-Process | Out-GridView</p>
<p>Walah!!!</p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-rOkibpvbprM/Xmy37Gc92DI/AAAAAAAAKmo/xP_5EQPQvOsYMq5GqRkkJqPgl3V9ze0FwCLcBGAsYHQ/s1600/image.png"><img src="https://1.bp.blogspot.com/-rOkibpvbprM/Xmy37Gc92DI/AAAAAAAAKmo/xP_5EQPQvOsYMq5GqRkkJqPgl3V9ze0FwCLcBGAsYHQ/s400/image.png" alt=""  /></a></p>
<p><strong>ForEach-Object Parallel option</strong><br>
This is another long wanted feature. I remember once I had to create thousands of large files on a hard disc and it was such a pain to wait for the loop to go through each creation process sequentially. With Parallel option, it can potentially save hours of waiting in this case.</p>
<p>Interestingly though, it is not necessarily a silver bullet for shortening your loop&rsquo;s excution time. Let me show you some test results.</p>
<p><strong>Parallel option slows the script</strong></p>
<p>PS /home/tom/powershell/foo&gt; measure-command {get-vm|foreach {$filename = $_.Name; New-Item $filename;Set-Content .\$filename &ldquo;this is bunch of blah blah blah&hellip;..&rdquo; }}</p>
<p>Days              : 0</p>
<p>Hours             : 0</p>
<p>Minutes           : 0</p>
<p>Seconds           : 1</p>
<p>Milliseconds      : 16</p>
<p>Ticks             : 10161391</p>
<p>TotalDays         : 1.1760869212963E-05</p>
<p>TotalHours        : 0.000282260861111111</p>
<p>TotalMinutes      : 0.0169356516666667</p>
<p>TotalSeconds      : 1.0161391</p>
<p>TotalMilliseconds : 1016.1391</p>
<p>PS /home/tom/powershell/foo&gt; measure-command {get-vm|foreach <strong>-parallel</strong> {$filename = $_.Name; New-Item $filename;Set-Content .\$filename &ldquo;this is bunch of blah blah blah&hellip;..&rdquo; }}</p>
<p>Days              : 0</p>
<p>Hours             : 0</p>
<p>Minutes           : 0</p>
<p>Seconds           : 12</p>
<p>Milliseconds      : 234</p>
<p>Ticks             : 122341726</p>
<p>TotalDays         : 0.000141599219907407</p>
<p>TotalHours        : 0.00339838127777778</p>
<p>TotalMinutes      : 0.203902876666667</p>
<p>TotalSeconds      : 12.2341726</p>
<p>TotalMilliseconds : 12234.1726</p>
<p>As you can see, in this case the Parallel option actually makes the whole script slower, a lot slower. Why is that? The new ForEach-Object -Parallel parameter set uses existing PowerShell APIs for running script blocks in parallel. These APIs have been around since PowerShell v2, but are cumbersome and difficult to use correctly. This new feature makes it much easier to run script blocks in parallel. But there is a fair amount of overhead involved.</p>
<p>In our case, as the code calls to create a file is simple, putting the loop into Parallel adds a lot overhead and prolongs the running time of the script.</p>
<p><strong>Parallel option improves performance</strong><br>
Now let&rsquo;s add a wait condition into the code.</p>
<p>PS /home/tom/powershell/foo&gt; measure-command {get-vm|ForEach-Object <strong>-parallel</strong> {$filename = $_.Name; New-Item $filename;Set-Content .\$filename &ldquo;this is bunch of blah blah blah&hellip;..&quot;;sleep 1}}</p>
<p>Days              : 0</p>
<p>Hours             : 0</p>
<p>Minutes           : 0</p>
<p>Seconds           : 35</p>
<p>Milliseconds      : 741</p>
<p>Ticks             : 357414908</p>
<p>TotalDays         : 0.000413674662037037</p>
<p>TotalHours        : 0.00992819188888889</p>
<p>TotalMinutes      : 0.595691513333333</p>
<p>TotalSeconds      : 35.7414908</p>
<p>TotalMilliseconds : 35741.4908</p>
<p>PS /home/tom/powershell/foo&gt; measure-command {get-vm|ForEach-Object {$filename = $_.Name; New-Item $filename;Set-Content .\$filename &ldquo;this is bunch of blah blah blah&hellip;..&quot;;sleep 1}}</p>
<p>Days              : 0</p>
<p>Hours             : 0</p>
<p>Minutes           : 2</p>
<p>Seconds           : 9</p>
<p>Milliseconds      : 848</p>
<p>Ticks             : 1298486786</p>
<p>TotalDays         : 0.00150287822453704</p>
<p>TotalHours        : 0.0360690773888889</p>
<p>TotalMinutes      : 2.16414464333333</p>
<p>TotalSeconds      : 129.8486786</p>
<p>TotalMilliseconds : 129848.6786</p>
<p>By adding sleep 1, each loop excution takes at least 1 second. This significantly increased the duration of the whole script. In this case the even with the overhead of Parallel, it is still beneficial to use the option.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Monitor AWS VPC Connectivity with Python]]></title>
            <link href="/2019/12/how-to-monitor-aws-direct-connect.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/12/how-to-monitor-aws-direct-connect.html</id>
            
            <published>2019-12-12T13:14:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>We recently have the need to cutover our AWS Direct Connects to a different vendor. In order to carry out the change, I was tasked to find a way to monitor Direct Connect connectivities to our on premise network from our hundreds of VPCs in AWS.</p>
<p>After some discussion with our network engineers and security team, the solution I end up using is to deploy a single EC2 instance into each those VPCs that has a connection to VGW. We then add those instance IPs into PRTG to monitor with Ping and Http sensors.</p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-abWMpg5NOS8/XfHLgZIlDMI/AAAAAAAAKkI/qHvZ2xtXPYUp_UsJv-kEi3Q77vVa1PuZgCLcBGAsYHQ/s1600/DXMonitor.png"><img src="https://1.bp.blogspot.com/-abWMpg5NOS8/XfHLgZIlDMI/AAAAAAAAKkI/qHvZ2xtXPYUp_UsJv-kEi3Q77vVa1PuZgCLcBGAsYHQ/s320/DXMonitor.png" alt=""  /></a></p>
<p>To allow the instance to be deployed into the targeted AWS accounts, we use CloudFormation StackSet to push out a role into each of those accounts first. The role then allows the &ldquo;Master Account&rdquo; to have permission to create and update necessary resources within the target accounts.</p>
<p>The instance uses a t3.nano tier alogn with Amazon hvm Linux2 AMI (Use our own hardened AMI in my case). A simple Nginx test page is installed on the instance to allow us to monitor TCP traffics. The total running cost of the solution depends on the number of instances/VPCs you have. For example, 100 instances cost around AU$500 a month.</p>
<p>Yes, Lambda potentially can be cheaper, but it does not allow us to have real time monitoring as it does not support ICMP traffic. The underlaying container images for Lambda does not support that. <a class="gblog-markdown__link" href="https://aws.amazon.com/blogs/networking-and-content-delivery/debugging-tool-for-network-connectivity-from-amazon-vpc/">AWS recommended way</a> to monitor VPC connectivities is to use Ping.</p>
<p>The script can be easily transformed into a Lambda function and deployed with Serverless Framwork.</p>
<p>The script can be found in my repo.<br>
<a class="gblog-markdown__link" href="https://github.com/tomkingchen/DirectConnectMonitor">https://github.com/tomkingchen/DirectConnectMonitor</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[RDP to EC2 with SSM Port Forwarding]]></title>
            <link href="/2019/10/rdp-to-ec2-with-ssm-port-forwarding.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/10/rdp-to-ec2-with-ssm-port-forwarding.html</id>
            
            <published>2019-10-19T14:05:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Say you have a bunch of Windows servers hosted in AWS. The VPC they are in does not have VPN or Direct Connect connect back to your on premse network. Expose RDP port through public IP for these Windows servers is a very good way to get hacked. So how can we securely connect to the servers in this kind setup?</p>
<p>Fortunately we have SSM for the rescue. In August, AWS announced a new feature for SSM Session Manager, which allows us to securely create tunnels between your EC2 instances deployed in private subnets and your local machine. You can read about the announcement <a class="gblog-markdown__link" href="https://aws.amazon.com/blogs/aws/new-port-forwarding-using-aws-system-manager-sessions-manager/">here</a>.</p>
<p>Here are the steps you can setup for Windows Instances.<br>
1. Configure the Windows EC2 as Managed Instances in SSM.<br>
This mainly involves assign a IAM EC2 Role to the instance with SSM policies. Since the focus of this post is about Session Manager Port Forwarding, I won&rsquo;t expand this too much. you can find more details about initial setup of SSM <a class="gblog-markdown__link" href="https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-instance-profile.html">here</a>.</p>
<p>2. For your existing Windows EC2s, you will need to update the SSM Agent on them to the latest version so it has the new feature. You can easily carry out SSM agent update via SSM State Manager.</p>
<p>3. In addition to AWS CLI or AWS PowerShell Toolkit, you will need to install Session Manager Plugin on your local machine. The plugin can be downloaded from <a class="gblog-markdown__link" href="https://s3.amazonaws.com/session-manager-downloads/plugin/latest/windows/SessionManagerPluginSetup.exe">here</a>.</p>
<p>4. Type the command below to initiate the Port Fowarding session. Make sure you have the correct region setup.</p>
<pre tabindex="0"><code>Start-SSMSession -Target i-0x8x888xxxx8888xx -DocumentName AWS-StartPortForwardingSession -Parameter @{portNumber=&#39;3389&#39;;localPortNumber=&#39;9989&#39;}  
</code></pre><p>The line above is pretty self-explanatory. You need to provide the Instance ID and the &ldquo;Document&rdquo; name, which is AWS term for SSM runbook. And also which port you want the fowarding.</p>
<p>5. Next, simply fire up Remote Desktop Connection (mstsc) and type <strong>localhost:9989</strong>.</p>
<p>6. That&rsquo;s it! You should now be able to RDP into the Windows EC2 instance <strong>WITHOUT</strong>:</p>
<ul>
<li>VPN/Direct Connection to the VPC</li>
<li>Assign public IP to the EC2 instnace</li>
<li>Open port 3389 in Security Group (That&rsquo;s right, you don&rsquo;t even need to do that!)</li>
</ul>
<p>PS. Session Manager does allow Remote PowerShell access without any extra setup. However, the shell session is authenticated with a generic <strong>ssm-user</strong> account, which is not ideal for security audit.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[VMware Site Recovery Manager Multi-Site Pair Deployment]]></title>
            <link href="/2019/09/vmware-site-recovery-manager-multi-site.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/09/vmware-site-recovery-manager-multi-site.html</id>
            
            <published>2019-09-06T23:22:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I was recently involved in a data center migration project, which used VMware SRM (Site Recovery Manager) as the migration tool to move virtual machines between 3 DCs. The diagram below shows how the setup looks like. The version of SRM is 8.1.</p>
<p>[SiteA] &lt;&mdash;-&gt; [SiteB] &lt;&mdash;-&gt; [SiteC]</p>
<p>VMware documentation refer the above scenario as Shared Recovery Site. For each site-pair, you will need to deploy individual SRM server to ensure the <strong>SRM Plug-in ID</strong> is unique to that pair. So in our case, SiteB-C pair requires a new SRM server in Site B and Site C.</p>
<p><strong>Install SRM</strong></p>
<p>The steps below shows the configuration details of the installation process on each SRM server.<br>
1. Provide the PSC server details of the site<br>
2. Provide the vCenter server details of the site<br>
3. Provide the local SRM extension details<br>
4. This is the most important part of the configuration. Make sure you choose <strong>Custom Site Recovery Manager Plug-in Identifier</strong>. The Plug-in ID needs to be <strong>the same</strong> on both Site B and Site C SRM instance.<br>
5. Choose certificate<br>
6. Configure Embedded Database configuration. Make sure your Windows Firewall allows the database port, as by default it is blocked even for localhost!<br>
7. Choose service account. My recommendation is to use Local System account, so you don&rsquo;t have to manage another privileged AD account!</p>
<p><strong>Configure SRM Site Pair</strong><br>
1. Log into SRM portal. Normally its https://srmservername/dr<br>
2. Create New Site Pair<br>
3. Select local vCenter server<br>
4. Select the remote site vCenter server you want to pair<br>
5. For replication method, we will be using Storage Replication. This require us to install correct Storage Replication Adapter on the SRM server (In our case it&rsquo;s HPE Nimble). Usually the SRA can be downloaded from VMware site.<br>
6. For the site pair, check SRA is in OK status</p>
<p><strong>Configure Nimble Array for Storage Replication</strong><br>
Create a new Replication Partner in Nimble<br>
Choose on-premise Replicatin Partner<br>
Put in the remote array details. Make sure you type the correct Array Group Name with the right case. It is case sensitive for Nimble (Given the controller basically is a Linux server).<br>
Repeat the same step on remote Replicatin Partner.<br>
Create Volume Collection with the configured Replication Partner.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How Secure is RDP?]]></title>
            <link href="/2019/06/hands-up-if-you-have-following.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/06/hands-up-if-you-have-following.html</id>
            
            <published>2019-06-23T03:06:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Hands up if you have following setup/practices in your organization:</p>
<ul>
<li>
<p>A RDP server (Terminal server) that everyone can jump onto. Apart from the IT admins, some users have local admin rights on the box, just so they can run or configure a particular application.</p>
</li>
<li>
<p>To help troubleshooting an issue, your IT admins often RDP to servers directly from user&rsquo;s laptop, which the user is a local admin.</p>
</li>
<li>
<p>A group of users have local admin rights on a particular Windows box, and your Domain Admins also need to RDP to from time to time.</p>
</li>
</ul>
<p>If any of above scenario applies to your organization, you might want to consider introducing some changes. This is why:</p>
<p>It is well known that Windows Remote Desktop Service has this feature that allows you to connect to another user&rsquo;s session. But obviously you will need to know the other user&rsquo;s credential to do that, right? No, not necessarily. With NT AUTHORITY/SYSTEM account, you can hijack the user&rsquo;s RDP session without the need for type in the credential. As mentioned in this blog post <a class="gblog-markdown__link" href="https://doublepulsar.com/rdp-hijacking-how-to-hijack-rds-and-remoteapp-sessions-transparently-to-move-through-an-da2a1e73a5f6">https://doublepulsar.com/rdp-hijacking-how-to-hijack-rds-and-remoteapp-sessions-transparently-to-move-through-an-da2a1e73a5f6</a>  If you are a local admin on the server/desktop, you can easily hijack another user&rsquo;s RDP session (whether it&rsquo;s connected or disconnected) by run the following commands:</p>
<ol>
<li>Query User to get the SEESION Name and ID</li>
</ol>
<p>C:\Users\ben&gt;query user</p>
<p>USERNAME              SESSIONNAME        ID  STATE   IDLE TIME  LOGON TIME</p>
<p>tom-admin             rdp-tcp#19          4  Active         15  6/22/2019 5:14 AM</p>
<p>&gt;ben                   rdp-tcp#20          5  Active          .  6/22/2019 6:53 AM</p>
<ol start="2">
<li>Use SC to create a service which will connect to the privileged account (tom-admin) RDP session from a non-privileged users (Ben) session. The service by default will use SYSTEM account to launch.</li>
</ol>
<p>sc create rdphijack binpath=&ldquo;cmd.exe /k tscon 4 /dest:rdp-tcp#20&rdquo;</p>
<ol start="3">
<li>Start the service, and THAT&rsquo;S IT!!!</li>
</ol>
<p>net start rdphijack</p>
<p>This hack is so easy to achieve, a security novice like me was able to get it going in no time.</p>
<p>Image the follow scenario: Your Domain Admin RDP to a jump/bastion server. He/She left the session there without properly logoff. A hacker compromised a normal domain user account. The account somehow has local admin rights on this server. The hacker will then be able to easily connect to the Domain Admin&rsquo;s session, without the need for any 3rd party tools! The worst thing is, this kind of hijack will not trigger any alarms in solutions like ATA.</p>
<p>I know there are also lots people still arguing whether this is an issue. I think the point is with this kind of flaw in place, we really need to avoid those scenario I mentioned in the beginning. As they can easily lead to a compromised DA account. RDP from the beginning isn&rsquo;t a very secure way of accessing servers. As a IT Admin, we should try minimize the use of it. Luckily, we now have System Admin Center. With that we can accomplish a lot admin tasks without the need of RDP to the remote servers.</p>
<p>On the other hand, I would love to hear your opinion or experience with System Admin Center. I am not aware of any security issues with the tool. But be sure do let me know if you know otherwise.</p>
<p><strong>Update:</strong> I have tested this issue on Windows 2019 and was not able to replicate it. So here you go, just upgrade all your servers to 2019!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Terraform to build server in VMware]]></title>
            <link href="/2019/05/use-terraform-to-build-server-in-vmware.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/05/use-terraform-to-build-server-in-vmware.html</id>
            
            <published>2019-05-18T00:25:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Like Cloud Formation and ARM Templates, Terraform enables the way of Infrastructure as Code to provision resources in Clouds, but it also works with on premise infrastructures like VMware vSphere and NSX. I recently have been working on the automation of on premise server provision process. The goal is to provision a Ubuntu server on our vSphere 6.5 environment with iPerf3 installed and configured.</p>
<p>It surprises me that there aren’t many useful resources/examples out there when comes to using Terraform with VMware. Yes, there are tons of blog posts about how to build a VMware VM with Terraform, but almost all of those are just touched on very very basic stuff. I can’t find any good reference for how to install and configuration applications within the VM. Without those, I seriously doubt the value of using IaaC. A simple VMware Template with ClickOps will be way more efficient.</p>
<p>I did end up finding a very detailed repo about using Terraform to build a Kubernate cluster in VMware in GitHub. But this only make me feel more frustrated. This picture reflects exactly how I feel after went through that repo…</p>
<p><a class="gblog-markdown__link--raw" href="https://i.kym-cdn.com/photos/images/original/000/572/078/d6d.jpg"><img src="https://i.kym-cdn.com/photos/images/original/000/572/078/d6d.jpg" alt=""  /></a></p>
<p>Another surprise finding is the lacking of native means to securely protect sensitive data like secret keys and passwords alike. To connect to the vCenter, I have the choice of either save my admin password inside the variables file or type the password unmasked in command prompt. Credentials for VMs can easily be leaked through state file. After consulted with one of our seasoned Terraform users and some Googling, it seems the best way to securely use Terraform is take advantage of some of the 3rd party services, e.g. store passwords in Secret Manager, share state file through S3 buckets.</p>
<p>In my case, all resources are on premise. I don’t want to call AWS APIs just so I can enter my password for vCenter. I decided to simply use PowerShell script to get the credential then save it into variables file during the runtime of the Terraform template. Once the template is applied, the PowerShell script will reverse the variables file to its original version, which cleans out the plaintext password. One thing I should but haven’t done is to add to the script is to clean out memory.</p>
<p>To avoid store plain password for the VM in the template, I configure the VM template to use SSH key pair for authentication. This is not difficult, but as a ClickOps Windows guy, it does take me sometime to figure out. WSL again proves its value in this case. The template itself is fairly simple. I got most references from Terraform’s official document.</p>
<p>The whole script + template example can be found in my GitHub repo <a class="gblog-markdown__link" href="https://github.com/tomkingchen/terraform-linux-vmware.git">here</a>. As usual, hope you find it helpful.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Sydney AWS Summit 2019 - My Experience]]></title>
            <link href="/2019/05/sydney-aws-summit-2019-my-experience.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/05/sydney-aws-summit-2019-my-experience.html</id>
            
            <published>2019-05-03T21:41:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>The past week I attended AWS Summit Sydney for three days. It&rsquo;s such an action packed show. Full of brilliant speakers and tons of interesting workshops. I feel so hard to decide on my agenda. In the end, based on the technologies I am interested in and the relevance to my job these are sessions I went for.</p>
<p><strong>AWS Innovation Day</strong></p>
<p>Keynote - I was late for it. But still It was good to hear the story of Qantas and learn about how they improve performance and efficiency with AWS. I do have doubts of their ambitious goal about flying customers directly from Australia to US and Europa without a stop. Not sure if I want to stuck in a plane for 20 hours&hellip; Maybe it&rsquo;s time for a new Concord!  </p>
<p>After Keynote, I went around the &ldquo;Cloud Zone&rdquo; to talk to different vendors. Among all the vendors, the one left the best impression with me is CloudHealth from VMware. The ability to provide deep analysis of the current spending and indicate detailed remediation plans are lacking from some of the competitors solutions. </p>
<p>With so many IT pros came to the summit, it&rsquo;s no surprise I ran into quite a few familiar faces that afternoon. The day end on a high note with a nice dinner with an ex-colleague who has recently joined AWS. </p>
<p><strong>Summit Day 1</strong></p>
<p>The highlight of the morning Keynote is able to see SEEK&rsquo;s logo high up on the screen while Paul Bassat was talking about the future of Australia business especially in technology sector. </p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-HZ9zN-XVHFQ/XM0X3WFZibI/AAAAAAAAKhI/A9nbSK3qbQ8a5fjoMMQ0Yzelop_eB4lEQCLcBGAs/s1600/IMG_5237.JPG"><img src="https://3.bp.blogspot.com/-HZ9zN-XVHFQ/XM0X3WFZibI/AAAAAAAAKhI/A9nbSK3qbQ8a5fjoMMQ0Yzelop_eB4lEQCLcBGAs/s320/IMG_5237.JPG" alt=""  /></a></p>
<p>I then went to the &ldquo;Fast-Track Your Application Modernisation Journey with Containers&rdquo; workshop. The workshop focuses on containerization with ECS Fargate service. It&rsquo;s a 3 hour long &ldquo;follow the lab notes&rdquo; type of session. The user case in the example is very well written and very practical in my opinion. If you are interested in Fargate, the lab notes can be found at <a class="gblog-markdown__link" href="http://bit.ly/ecs19syd">http://bit.ly/ecs19syd</a>. Just be aware, it does occur charges as the lab will require creation of some ECS clusters and related VPC components. But the cost should be something negligible if you do a proper cleanup straight after the lab. Ah, another thing, it was suggest to create the lab in us-west-2 region.</p>
<p>After the lab I went to a short session talk about how Qantas use AWS System Manager to scale its cloud operations. The session does provide some insight to how we can take advantage of SSM for some SOE related automation tasks, like JIT user access provision; updating AMI images. At the meantime I found the session is way too short (30 mins) and lack of technical details is rather disappointing. </p>
<p>I was late for the &ldquo;Building Serverless Application That Align with 12Factor Methods&rdquo; workshop. While I didn&rsquo;t manage to grab a sit in position, I got the link for all the lab notes, which allows me to do the workshop anyway. You can find the downloadable ZIP file from <a class="gblog-markdown__link" href="http://bit.ly/12FactorWorkshop">http://bit.ly/12FactorWorkshop</a></p>
<p><strong>Summit Day 2</strong></p>
<p>The most interesting bit of that day&rsquo;s keynote is the drone they placed on the stage. I bet everyone in the audience thought they will fly it. In the end they just played a video how it can rescue people from ocean by dropping a self-inflatable baton, which is quite impressive anyway.</p>
<p>Learnt from previous day&rsquo;s mistake, I invested all the rest of my time into workshops that day. The &ldquo;Security Best Practice Workshop&rdquo; is a bit basic, as most labs in it are just use clickOps to deploy CloudFormation templates. However, the instructors in the workshop are very helpful, and I was able to learn quite a lot about Security Hub, which is a new security view in a glance service from AWS. The lab notes for this workshop is under AWS Well-Architect GitHub repo.</p>
<p>I squeezed another workshop session into my schedule before depart for the airport. It&rsquo;s definitely an interesting one: Creating Voice Experiences with Alexa Skills. It&rsquo;s a very good session with comprehensive guidance provided by the instructor. Though I wasn&rsquo;t able to complete the workshop due to time. I will definitely do some more experiments with the development tool kit in my own time. </p>
<p>Overall it&rsquo;s a really busy but worth while event. It&rsquo;s great to see Australia has such a huge and skillful AWS community.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup Cross Account S3 Access for Cloudberry Drive]]></title>
            <link href="/2019/04/setup-cross-account-s3-access-for.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/04/setup-cross-account-s3-access-for.html</id>
            
            <published>2019-04-13T16:05:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I recently run into a scenario, which one of EC2 instances in our production AWS account (IT) need to access a S3 bucket hosted in a separate account (Marketing). The EC2 instance is a Windows 2008 R2 server. It runs Cloudberry Drive to map the S3 bucket as a local volume for a local application to retrieve the data off it.</p>
<p>The easiest way to make this work is to create an IAM user in the and assign it with Access keys. But this is against <a class="gblog-markdown__link" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#delegate-using-roles">AWS IAM best practice</a>. Cloudberry Drive does provide the option to use Role for S3 bucket access. Though their documentation is a bit lacking on how to setup this in a cross account scenario. After some Googling, it turns out to be fairly straight forward. Here&rsquo;s how I did it.</p>
<p>Let&rsquo;s start with a picture as it helps to clarify where things are in this two account setup.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-KDI-f-Y1UlQ/XLJyRHj_VgI/AAAAAAAAKgM/6iN6kIxAD3MvS8_GntWo5VxJpRlTgGzaQCLcBGAs/s1600/diagram.png"><img src="https://2.bp.blogspot.com/-KDI-f-Y1UlQ/XLJyRHj_VgI/AAAAAAAAKgM/6iN6kIxAD3MvS8_GntWo5VxJpRlTgGzaQCLcBGAs/s640/diagram.png" alt=""  /></a></p>
<p>First, in the <strong>IT account (111111111111)</strong>:<br>
1. Create an IAM role <strong>market-s3-role</strong> with the following policy: <strong>fullaccess-marketing-bucket</strong>. The policy allows access to the S3 bucket <strong>market-s3-bucket</strong> and its objects in the Marketing account (88888888888).<br>
Here’s the policy JSON file.</p>
<pre tabindex="0"><code>{  
    &#34;Version&#34;: &#34;2012-10-17&#34;,  
    &#34;Statement&#34;: \[{  
            &#34;Effect&#34;: &#34;Allow&#34;,  
            &#34;Principal&#34;: {  
                &#34;AWS&#34;: &#34;arn:aws:iam::111111111111:role/market-s3-role&#34;  
            },  
            &#34;Action&#34;: \[  
                &#34;s3:ListBucket&#34;,  
                &#34;s3:GetBucketLocation&#34;  
            \],  
            &#34;Resource&#34;: \[  
                &#34;arn:aws:s3:::market-s3-bucket&#34;  
            \]},  
        {  
            &#34;Effect&#34;: &#34;Allow&#34;,  
            &#34;Principal&#34;: {  
                &#34;AWS&#34;: &#34;arn:aws:iam::1111111111:role/market-s3-role&#34;  
            },  
            &#34;Action&#34;: \[  
                &#34;s3:GetObject&#34;,  
                &#34;s3:PutObject&#34;,  
                &#34;s3:DeleteObject&#34;  
            \],  
            &#34;Resource&#34;: \[  
                &#34;arn:aws:s3:::market-s3-bucket/\*&#34;  
            \]  
        }  
    \]  
}  
</code></pre><p>2. Attach the role to the EC2 instance (the Windows server). This can be done when the instance is online.</p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-NuzVO1I9tew/XLJy9-c9-VI/AAAAAAAAKgU/WAmtv-cFHqcZ1FXxf35k7hRb36WjU_b6gCLcBGAs/s1600/attachec2role.png"><img src="https://1.bp.blogspot.com/-NuzVO1I9tew/XLJy9-c9-VI/AAAAAAAAKgU/WAmtv-cFHqcZ1FXxf35k7hRb36WjU_b6gCLcBGAs/s640/attachec2role.png" alt=""  /></a></p>
<p>3. In CloudBerry Drive options, set the Storage Account to <strong>Use AWS IAM role policy</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-kobupxQ026M/XLJzaN1372I/AAAAAAAAKgg/zKNZGd5-UqgDkiLgmk5GS2VqE-un5RtrACLcBGAs/s1600/driveAccountsetup.png"><img src="https://2.bp.blogspot.com/-kobupxQ026M/XLJzaN1372I/AAAAAAAAKgg/zKNZGd5-UqgDkiLgmk5GS2VqE-un5RtrACLcBGAs/s400/driveAccountsetup.png" alt=""  /></a></p>
<p>Now, switch to the <strong>Marketing account (888888888888)</strong>:<br>
1. Create a bucket policy to the S3 bucket <strong>market-s3-bucket</strong> as below. As you can see, it’s basically the same as the Role Policy in the IT account.</p>
<pre tabindex="0"><code>{  
    &#34;Version&#34;: &#34;2012-10-17&#34;,  
    &#34;Statement&#34;: \[{  
            &#34;Effect&#34;: &#34;Allow&#34;,  
            &#34;Principal&#34;: {  
                &#34;AWS&#34;: &#34;arn:aws:iam::111111111111:role/market-s3-role&#34;  
            },  
            &#34;Action&#34;: \[  
                &#34;s3:ListBucket&#34;,  
                &#34;s3:GetBucketLocation&#34;  
            \],  
            &#34;Resource&#34;: \[  
                &#34;arn:aws:s3:::market-s3-bucket&#34;  
            \]},  
        {  
            &#34;Effect&#34;: &#34;Allow&#34;,  
            &#34;Principal&#34;: {  
                &#34;AWS&#34;: &#34;arn:aws:iam::1111111111:role/market-s3-role&#34;  
            },  
            &#34;Action&#34;: \[  
                &#34;s3:GetObject&#34;,  
                &#34;s3:PutObject&#34;,  
                &#34;s3:DeleteObject&#34;  
            \],  
            &#34;Resource&#34;: \[  
                &#34;arn:aws:s3:::market-s3-bucket/\*&#34;  
            \]  
        }  
    \]  
}  
</code></pre><p>Once the bucket policy is in place, we can now test the access from the EC2 instance in IT account. Open CloudBerry Drive and mount the S3 bucket as G: drive. You should be able to see, create and delete files/folders in the drive.</p>
<p>Note: In this setup we used IAM Resource Based Policy to gain access cross account AWS resources. This is the easiest way in our scenario. But not all AWS resource support Resource Based Policy. For resources do not support, we will need to use Cross-account IAM roles. You can read more from <a class="gblog-markdown__link" href="https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/">here</a>.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[The Un-documented Way to Setup AWS SSO with Okta]]></title>
            <link href="/2019/02/the-un-documented-way-to-setup-aws-sso.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/02/the-un-documented-way-to-setup-aws-sso.html</id>
            
            <published>2019-02-22T21:22:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>In this article I would like to share an un-documented way of setting up AWS SSO by using Okta.In case you don’t know what Okta is. It is one of the popular identity management solutions out in the market. It provides Identity as a service through its Web portal and APIs.</p>
<p>There is a detailed <a class="gblog-markdown__link" href="https://saml-doc.okta.com/SAML_Docs/How-to-Configure-SAML-2.0-for-Amazon-Web-Service">document</a> provided by Okta walks through steps of how to setting up SAML SSO between your AWS accounts and Okta. So why don’t we follow that? Well, the solution suggested by the official document requires a privileged <strong>USER Account</strong> to be setup in your <strong>Master AWS Account</strong>. The account will then be configured to be able to assume roles in all your other accounts. In other words, if this account is compromised, your whole organization’s AWS Accounts are exposed to potential security breach.</p>
<p>Ok, what’s the better way then? I actually talked about this in one of my old post <a class="gblog-markdown__link" href="https://www.tomking.xyz/2018/06/setup-sso-access-to-aws-console-with.html">Setup SSO Access to AWS Console with Azure AD</a>. Like the Azure AD solution, we will map user groups to different AWS Roles. With that in place, we can then assign different levels of AWS accesses to users by simply adding them into correspondent groups. This is actually a lot easier to setup in OKTA than Azure AD. Here’s how it can be done.</p>
<p><strong>In OKTA</strong><br>
1. Log into your OKTA tenant. If you don’t have one, you can sign up a free Developer account from <a class="gblog-markdown__link" href="https://developer.okta.com/">developer.okta.com</a>.<br>
2. Click the Admin button on the up right corner</p>
<p>3. If you like me are using a Dev account, click Developer Console and switch to <strong>Classic UI</strong>.</p>
<p>4. Go to <strong>Applications</strong> and click <strong>Add Application.</strong><br>
5. Search for “Amazon Web Service”, NOT “AWS”, otherwise you will get a different app.</p>
<p>6. Add the application and leave everything as default and click Done. Don’t worry about the settings at this stage.</p>
<p>7. Now is the important step. After added your AWS application, raise a <strong>Support Call</strong> with OKTA! Yes, they provide support even for free dev accounts. And now ask for below specific attributes to be enabled for your AWS Application. They are hidden by default and can only be enabled by OKTA.</p>
<ul>
<li>APP FILTER</li>
<li>GROUP FILTER</li>
<li>ROLE VALUE PATTERN</li>
</ul>
<p>The support call usually takes 48 hours to come around for a free dev account. </p>
<p>8. Go back to your AWS Application in OKTA, and click Edit under <strong>Sign On</strong> tab.<br>
9. Change the <strong>SIGN ON METHOD</strong> to SAML 2.0.<br>
10. Once the attributes are added, you should see something like below.</p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-JimaFQpljCM/XHDYRGyto1I/AAAAAAAAKd8/24qMY2ppvUE8Atuufi1oun-rGAChWMS3wCLcBGAs/s1600/signonOptions.png"><img src="https://4.bp.blogspot.com/-JimaFQpljCM/XHDYRGyto1I/AAAAAAAAKd8/24qMY2ppvUE8Atuufi1oun-rGAChWMS3wCLcBGAs/s320/signonOptions.png" alt=""  /></a></p>
<p>The Group Filter applies to your group names in OKTA. The default Regex aws_(?{{accountid}}\d+)_(?{{role}}[a-zA-Z0-9+=,.@\-_]+) means the group name needs to match something like “<strong>aws_12345678910_adminrole</strong>”. 12345..910 is the Account Number of the AWS Account.</p>
<p>The Role Value Pattern applies to your role ARN in AWS. The Regex arn:aws:iam::${accountid}:saml-provider/OKTA,arn:aws:iam::${accountid}:role/${role} means the role ARN should be something like “<strong>arn:aws:iam::12345678910:saml-provider/OKTA,arn:aws:iam::12345678910:role/adminrole</strong>”. In this case, again 12345678910 refers your AWS Account number. OKTA is the name you gave to the Identity Provider in AWS.<br>
Both regex strings can be tweaked to fit your own needs.</p>
<p>11. Click the <strong>Identity Provider metadata</strong> link to download the metadata file.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-yZ7920U2TkY/XHDX2K0jgPI/AAAAAAAAKd0/P73raPcOep0JurTBMM8EKFi5eNKqIlNDACLcBGAs/s1600/METAlink.png"><img src="https://3.bp.blogspot.com/-yZ7920U2TkY/XHDX2K0jgPI/AAAAAAAAKd0/P73raPcOep0JurTBMM8EKFi5eNKqIlNDACLcBGAs/s320/METAlink.png" alt=""  /></a></p>
<p>12. Create OKTA groups to match the “Group Filter” regex string. Or, if your groups are synced from other identity source like AD, create the groups there. Assign AWS Application to the group(s).</p>
<p><strong>In AWS</strong><br>
1. Login to the AWS Console and click on IAM → Identity Providers → Create Providers<br>
2. Select SAML as Provider Type<br>
3. Enter &ldquo;OKTA&rdquo; as Provider Name<br>
4. Upload the Metadata XML file downloaded previously</p>
<blockquote>
<p>Click on Next → Create</p>
</blockquote>
<blockquote>
<p>Copy the ARN of Identity Provider to Notepad for later use</p>
</blockquote>
<p>Now we need to setup IAM Roles to define the access.<br>
5. Click on Roles → Create new role → Select SAML 2.0 federation as the trusted entity type<br>
6. Select &ldquo;OKTA&rdquo; as SAML Provider<br>
7. Click on Next Step<br>
8. Select AdministratorAccess as policy<br>
9. Enter &ldquo;awsadmin&rdquo; as Role name.<br>
10. Save all your changes.</p>
<p><strong>Test</strong><br>
Back in OKTA, add a user to the “aws_12345678910_adminrole” group.</p>
<p>Login as the user into OKTA. After user click the app, he/she should receive a login screen like below. In this case, the user has two roles associated with this particular account.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-lQNTtNIHytc/XHDXR_nrwqI/AAAAAAAAKds/GqFuk0CuASw-ukWL51bYQFoU7MPsrOZ5gCLcBGAs/s1600/awsSSO.png"><img src="https://3.bp.blogspot.com/-lQNTtNIHytc/XHDXR_nrwqI/AAAAAAAAKds/GqFuk0CuASw-ukWL51bYQFoU7MPsrOZ5gCLcBGAs/s320/awsSSO.png" alt=""  /></a></p>
<p>The solution works for all scale of AWS environment. No matter how many AWS Accounts you have, all you need is user groups with matching name of its AWS Roles.</p>
<p>The AWS Idp and role creation part can be further automated with CloudFormation or CLI scripts.<br>
In  compare to the Azure AD solution, there is no need to change any existing configuration (like the manifest JSON file in AAD) every time you add a role or whole new AWS account. Which is really convenient when you have like over 100 AWS accounts to manage!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Infrastructure as Code with CloudFormation]]></title>
            <link href="/2019/01/infrastructure-as-code-with.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2019/01/infrastructure-as-code-with.html</id>
            
            <published>2019-01-04T22:37:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I was working on a server migration task, which is to move a Windows IIS web server to AWS. The server’s sole purpose is to redirect bunch of the short URLs to some of the most frequently used long URLs. E.g. if user type in “o365/“ in browser, it will be redirected to <a class="gblog-markdown__link" href="https://portal.office365.com">https://portal.office365.com</a>.</p>
<p>Instead of uplifting the whole Windows server to AWS, I have decided to use a Linux server with Apache to replace this box. Other options like ALB, S3 HTTP Redirection and Route 53 were considered. But none of them can redirect short hostname like “o365”. I have also thought about the idea of using container and serverless options, but given all we need is a single redirect service and the a fixed IP is needed for alias lookup, they are not suitable in this case.</p>
<p>The Linux server is provisioned through a CloudFormation template. This allows automation of the whole deploy process. Furthermore, any future updates to the Apache service can also be carried out with an update of the CloudFormation stack.</p>
<p>Let’s start with the parameters of the template. In the parameters, we provide VPCId, serverName, SubnetId, KeyPairName, S3URL and an UpdateString. A lot of these are self explanatory. I have provide reasons for why we need UpdateString in the later part of this article.</p>
<pre tabindex="0"><code>Parameters:   
  VPCId:  
    Type: String  
    Description: VPC ID  
  ServerName:  
    Type: String  
    Description: Server name  
  VPCSubnetId:  
    Type: String  
    Description: Subnet ID in the format of subnet-c1xxxxxx  
  KeyPairName:  
    Type: String  
    Description: Key Pair name for the instance  
  S3URL:  
    Type: String  
    Description: Pre-Signed S3 URL for httpd.conf  
  UpdateString:  
    Type: String  
    Description: Update String to trigger cfn-hup  
Mappings:   
  RegionMap:  
    ap-southeast-2:  
      ALinux : &#39;ami-00c3d41691e25e54c&#39;  
</code></pre><p>The CloudFormation template contains only two resources: a Security Group and EC2 Instance.</p>
<p>There is nothing special about the security group. It simply allows HTTP and SSH access to the box.</p>
<pre tabindex="0"><code>  redirSG:  
    Type: AWS::EC2::SecurityGroup  
    Properties:  
      GroupName: sec-httpredir  
      GroupDescription: Security Group for URL redirect  
      VpcId:   
        Ref: VPCId  
      SecurityGroupIngress:  
      \- IpProtocol: tcp  
        FromPort: 80  
        ToPort: 80  
        CidrIp: 10.0.0.0/8  
      \- IpProtocol: tcp  
        FromPort: 22  
        ToPort: 22  
        CidrIp: 10.0.0.0/8  
      Tags:  
        \- Key: Name  
          Value: sec-httpredir    
</code></pre><p>The EC2 resource though, needs quite bit work to for Apache configuration. It uses cfn-init to install and configure Apache. And here is how it is done.</p>
<p>First, we define the AWS::CloudFormation::Init metadata. Note, all this key words are case sensitive. So be mindful when you type them in. Once I spend hours to workout it is a lower case caused by template to fail.</p>
<pre tabindex="0"><code>  Metadata:  
      AWS::CloudFormation::Init:  
        config:  
          packages:  
            yum:  
              httpd: \[\]  
          files:  
            &#34;/etc/cfn/cfn-hup.conf&#34;:  
              content: !Sub |  
                \[main\]  
                stack=${AWS::StackId}  
                region=${AWS::Region}  
                verbose=true  
                interval=5  
              mode: &#34;000400&#34;  
              owner: root  
              group: root  
            &#34;/etc/cfn/hooks.d/cfn-auto-reloader.conf&#34;:  
              content: !Sub |  
                \[cfn-auto-reloader-hook\]  
                triggers=post.update  
                path=Resources.WebServer.Metadata.AWS::CloudFormation::Init  
                action=/opt/aws/bin/cfn-init --verbose --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region}  
                runas=root  
              mode: &#34;000400&#34;  
              owner: root  
              group: root  
            &#34;/etc/httpd/conf/httpd.conf&#34;:  
              source: !Ref S3URL  
              mode: &#34;000400&#34;  
              owner: &#34;root&#34;  
              group: &#34;root&#34;  
            &#34;/var/www/html/index.html&#34;:  
              content: !Sub |  
                &lt;html&gt;  
                &lt;body&gt;  
                &lt;h1&gt;${UpdateString}&lt;/h1&gt;  
                &lt;/body&gt;  
                &lt;/html&gt;  
              mode: &#34;000644&#34;  
              owner: &#34;apache&#34;  
              group: &#34;apache&#34;  
          services:  
            sysvinit:  
              cfn-hup:  
                enabled: &#34;true&#34;  
                ensureRunning: &#34;true&#34;  
                files:  
                  \- &#34;/etc/cfn/cfn-hup.conf&#34;  
                  \- &#34;/etc/cfn/hooks.d/cfn-auto-reloader.conf&#34;  
              httpd:  
                enabled: &#34;true&#34;  
                ensureRunning: &#34;true&#34;  
          commands:  
            replacePrivateIP:  
              cwd: &#34;/etc/httpd/conf&#34;  
              command: sed -i s@127.0.0.1@$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)@g httpd.conf  
            restartApache:  
              cwd: &#34;/etc/httpd&#34;  
              command: service httpd restart  
</code></pre><p>With cfn-init, we tell the instance to configure cfn-hup service, which allows the instance to be updated when the CloudFormation Stack is updated.</p>
<p>Then we tell the instance to load Apache configure file httpd.conf from a S3 bucket. The S3URL parameter is a pre-signed S3 URL, which is only validate for 1 hours by default. This is to ensure the security of the configuration file, although it contains anything sensitive. Still, it’s better to be secure than just leave the gate open.</p>
<p>Next, cfn-init will load a default homepage indiex.html with the given code, which displays the UpdateString. This parameter can be anything, it simply tricks CloudFormation service to update the stack because something has changed. Otherwise, even if you made a change in the httpd.conf file. CloudFormation won’t be able to detect the change and will not initiate cfn-hup to rerun the configuration procedure.</p>
<p>In the services section, the template tells cfn-init to keep cfn-hup and Apache service to keep running.</p>
<p>In the command section, the template initiates two commands.</p>
<p>- one use powerful sed command to replace 127.0.0.1 local IP with the EC2 instance’s private IP address in the Apache config file. This is to ensure Apache service is always configured with the correct private IP of the instance.</p>
<p>- The next command restarts Apache service after each configuration update.</p>
<p>That summarizes the cfn-init configuration. Now let’s look at the EC2 properties section.</p>
<p>The Properties contains the usual elements: KeyName, ImageId, InstanceType, NetworkInstances and UserData.</p>
<pre tabindex="0"><code>Properties:  
      KeyName: !Ref KeyPairName  
      ImageId:   
        Fn::FindInMap:  
          \- RegionMap  
          \- Ref: &#39;AWS::Region&#39;  
          \- ALinux  
      InstanceType: t2.micro  
      NetworkInterfaces:  
        \- AssociatePublicIpAddress: &#34;false&#34;  
          DeviceIndex: &#34;0&#34;  
          GroupSet:  
            \- !Ref redirSG  
          SubnetId: !Ref VPCSubnetId  
      UserData:   
        &#34;Fn::Base64&#34;:  
          !Sub |  
            #!/bin/bash -xe  
            yum update -y aws-cfn-bootstrap  
            /opt/aws/bin/cfn-init -v -s ${AWS::StackName} -r WebServer --region ${AWS::Region} || error\_exit &#39;Failed to run cfn-init&#39;  
            yum -y update  
            /opt/aws/bin/cfn-signal -e $? --stack ${AWS::StackName} --resource WebServer --region ${AWS::Region}  
      Tags:  
        \- Key: Name  
          Value: !Ref ServerName  
        \- Key: owner  
          Value: &#34;SysEng&#34;  
        \- Key: ChangeTag  
          Value: !Ref UpdateString  
    CreationPolicy:  
      ResourceSignal:  
        Count: 1  
        Timeout: &#34;PT10M&#34;  
</code></pre><p>UserData is where we specify cfn-init to configure which stack and which resource.</p>
<p>cfn-signal provides the result of cfn-init. If anything go wrong with cfn-init, cfn-signal will provide the result to “ResourceSignal” defined in “CreationPolicy”. The CreationPolicy defines if within 10 mins the stack didn’t return any result, it will consider the resource creation failed and cause the stack creation/update to roll back.</p>
<p>The CloudFormation template code example can be downloaded from my GitHub repo here.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Use Microsoft Graph API to extract Excel file contents]]></title>
            <link href="/2018/12/use-microsoft-graph-api-to-extract.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/12/use-microsoft-graph-api-to-extract.html</id>
            
            <published>2018-12-09T02:40:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I was working on automating a process that extracts contents from a Excel spreadsheet stored in Office 365 SharePoint Online. It took me quite sometime to figure out how to get this done. I thought there must be people out there looking for the same thing. Hence here is the post.</p>
<p>Based on the requirements, the Excel file needs to be shared among few specified staff. They should be able to modify the spreadsheet with their Office 365 accounts. Then a automated process will export the contents of the sheet to an external destination.</p>
<p>The first half of the request can be easily achieved with Office 365 Group. While creating Office 365, it generates a SharePoint site for the team (Group Drive), which allows members of the team to share documents within the group. For the second half, we will be using MS Graph along with PowerShell. Microsoft Graph API provides the ability to read and modify Excel workbooks stored in OneDrive for Business, SharePoint site or Group drive, which is exactly the place we will store the Excel file.</p>
<p>The first thing we need to do is to create a Office 365 Group. In this example I created one named as &ldquo;ShareTest&rdquo;. This will in turn creates a SharePoint site dedicated for the group members to share files. The site URL is <a class="gblog-markdown__link" href="https://contoso.sharepoint.com/sites/sharetest">https://contoso.sharepoint.com/sites/sharetest</a>.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-rfZ8HSBAVKw/XAzwy2D-QHI/AAAAAAAAKaY/biuMf-DdnHc7KZI9LK9o-JN_xhfTuYT7gCHMYCw/s1600-h/Image%25289%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-6pUnRvAp8RI/XAzwzwpbq1I/AAAAAAAAKac/qc3_pExUGEAlgQlLWCDp1xc5BexfjynFwCHMYCw/Image%25289%2529_thumb?imgmax=800" alt="Image(9)"  title="Image(9)" /></a></p>
<p>As you can see an Excel file is uploaded to the site. Any members of the &ldquo;ShareTest&rdquo; group can now read this document by default. To allow the members to edit the file, click the permission drop down buttion and select Edit. This will give all group members right to edit any files upload to the site.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-MkDoadrj8uM/XAzw1c-pWcI/AAAAAAAAKag/U3u9vv0qknUT2KsY6iXPaH2O9z9CaoAFACHMYCw/s1600-h/Image%252810%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-7kpAF_GKs9A/XAzw2GTQjjI/AAAAAAAAKak/RoVaRiyAuswp_lTPEHNDZno-y0HF5KsrQCHMYCw/Image%252810%2529_thumb?imgmax=800" alt="Image(10)"  title="Image(10)" /></a></p>
<p>Now we have solved our first requirement, next we need to identify the MS Graph URI to retrieve the Excel contents. There is a <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/graph/api/resources/excel?view=graph-rest-beta">Microsoft doucment</a> explains how to use MS Graph to work with Excel spreadsheets. But there is no mentioning about how to get the contents if the Excel file is stored in SharePoint. After some researching, I finally find the way to identify URI. Here are the steps I took.</p>
<ol>
<li>
<p>Log into MS Graph Explorer with a ShareTest member account.</p>
</li>
<li>
<p>We created the site with 365 group. So the URI we use is for <strong>groups</strong> instead of <strong>sites</strong>. The URI uses query parameters to search for the group started with &ldquo;sharetest&rdquo;. It will return the 365 Group details, which contains the group id of the 365 group.</p>
</li>
</ol>
<p><strong>Note:</strong> More about query parameters can be found <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/graph/query-parameters">here</a>.</p>
<p><a class="gblog-markdown__link" href="https://graph.microsoft.com/v1.0/groups/?$filter=startswith%28displayname,%27sharetest%27%29">https://graph.microsoft.com/v1.0/groups/?$filter=startswith(displayname,'sharetest')</a></p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-YASqQgxkBOY/XAzw3OXkJUI/AAAAAAAAKao/xOEmRb_LgPsa3QQXDEDvtFPeV0GM77bhgCHMYCw/s1600-h/Image%252811%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-zaxuRRGLUNc/XAzw4c0yjyI/AAAAAAAAKas/gdRUPfr1hw4Xhc3s0Y6wou4WW1fG26MLQCHMYCw/Image%252811%2529_thumb?imgmax=800" alt="Image(11)"  title="Image(11)" /></a></p>
<ol start="3">
<li>With the group id, we can now form the URI to query the SharePoint site. The URI will list all files in the library.</li>
</ol>
<p><a class="gblog-markdown__link" href="https://graph.microsoft.com/v1.0/groups/%7bgroup-id%7d/drive/root/children">https://graph.microsoft.com/v1.0/groups/{group-id}/drive/root/children</a></p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-dNbYBxno1Fs/XAzw5Caj1hI/AAAAAAAAKaw/v-pcUD83EcciYd2JrosbpYvkMRLO1bUJwCHMYCw/s1600-h/Image%252812%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-qz1LhhNpBSA/XAzw58yr1PI/AAAAAAAAKa0/H8j2Xz5OeRQ2bcwA-8-FbBF31V7KT8wkgCHMYCw/Image%252812%2529_thumb?imgmax=800" alt="Image(12)"  title="Image(12)" /></a></p>
<ol start="4">
<li>Next, we need to identify the worksheet ID in order to export its contents.</li>
</ol>
<p><a class="gblog-markdown__link" href="https://graph.microsoft.com/v1.0/groups/%7bgroup-id%7d/drive/">https://graph.microsoft.com/v1.0/groups/{group-id}/drive/</a><strong>root:/example.xlsx:</strong>/workbook/worksheets</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-19NuimhfSB8/XAzw6qU6cHI/AAAAAAAAKa4/tvWjcOgRFlUCh5DOaJR8dHYT_MLEgLOWwCHMYCw/s1600-h/Image%252813%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-YqOBueF4qzk/XAzw7qEUFNI/AAAAAAAAKa8/nuB27it4AdI794vVHy0-S3M1A7IPkzQ8wCHMYCw/Image%252813%2529_thumb?imgmax=800" alt="Image(13)"  title="Image(13)" /></a></p>
<ol start="5">
<li>This is the final URI we need to get the contents out. We use the range function to include all the data in the sheet.</li>
</ol>
<p><a class="gblog-markdown__link" href="https://graph.microsoft.com/v1.0/groups/%7bgroup-id%7d/drive/root:/example.xlsx:/workbook/worksheets%28%27%7bworksheet-id%7d%27%29/Range%28address=%27Sheet1!A1:G10%27%29">https://graph.microsoft.com/v1.0/groups/{group-id}/drive/root:/example.xlsx:/workbook/worksheets('{worksheet-id}')/Range(address='Sheet1!A1:G10')</a></p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-3uY6393a1_4/XAzw8ux-5xI/AAAAAAAAKbA/BO7_m4Uxv9I6BTQd5gGvfrw9c8XJu-FzwCHMYCw/s1600-h/Image%252814%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-aPySzgTX_0g/XAzw9g_x2tI/AAAAAAAAKbE/IbUs9Gf9IVE2ta8SBGhiCvO2u3TlBWuOQCHMYCw/Image%252814%2529_thumb?imgmax=800" alt="Image(14)"  title="Image(14)" /></a></p>
<ol start="6">
<li>The output is a JSON output of the worksheet contents. It still needs a lot work to strip away those unnecessary bits. But if you copy paste the whole contents into <a class="gblog-markdown__link" href="http://json2table.com">http://json2table.com</a>, you will get something like below.</li>
</ol>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-J_ItylqYH1s/XAzw-ij23DI/AAAAAAAAKbI/H1tMiGdObQAagKa10syD_L2O2qpceSbmwCHMYCw/s1600-h/Image%252815%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-Al7hYp3VYnU/XAzxABHecoI/AAAAAAAAKbM/gLTuU9u71O44CcE-8jpPFaLxdXpUyDD-ACHMYCw/Image%252815%2529_thumb?imgmax=800" alt="Image(15)"  title="Image(15)" /></a></p>
<p>The above process can be automated with a PowerShell script. But to do that, first we need to register an Azure AD App for the script.</p>
<p>Log into your Azure portal, to create the Azure AD App, you do not need a subscription. So just go to <strong>Azure Active Directory</strong> and under <strong>App registration</strong>, click <strong>+ New application registration</strong>.</p>
<p>The process to create the app is pretty straight forward. Give the app a proper name, like &ldquo;PowerShell App&rdquo;. And make sure <strong>Application Type</strong> is set to: <strong>Native</strong>. <strong>Redirect URI</strong> does not need to be a real URL. So just leave it as <a class="gblog-markdown__link" href="https://redirectURI.com">https://redirectURI.com</a>.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-mkWTNbDYsmA/XAzxBthRjSI/AAAAAAAAKbQ/yg7ZODi6ffsJXeYJTslVIMoLcnKo77gHACHMYCw/s1600-h/Image%252816%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-ZdtwICoAjec/XAzxCnncRhI/AAAAAAAAKbU/rJUHB2kmZNcbFLkA0ODkuaPxo_lVdA4egCHMYCw/Image%252816%2529_thumb?imgmax=800" alt="Image(16)"  title="Image(16)" /></a></p>
<p>Once the app is created, go to <strong>Settings</strong> and click <strong>Required permissions</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-CHKAZEuK39c/XAzxDnklfVI/AAAAAAAAKbY/BYe9VYkJUBsuiM6sOxdjJ-dYRViDScq_QCHMYCw/s1600-h/Image%252817%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-wi4Rg2snhF4/XAzxEpzFr4I/AAAAAAAAKbc/Erd2zbrH0PcHyk-nlDmBFBwKu1boI05rQCHMYCw/Image%252817%2529_thumb?imgmax=800" alt="Image(17)"  title="Image(17)" /></a></p>
<p>Add following Microsoft Graph Delegated Permissions to the app. This will allow the PowerShell script to query the API with proper delegated rights.</p>
<ul>
<li>
<p>Read files that the user selects (preview)</p>
</li>
<li>
<p>Read user files</p>
</li>
<li>
<p>Read all files that user can access</p>
</li>
</ul>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-7IxeNwfoVgQ/XAzxFnzBi_I/AAAAAAAAKbg/AXo-QIWcTXYUUm9r-4ZnFRglkbGJyeTagCHMYCw/s1600-h/Image%252818%2529%255B3%255D"><img src="https://lh3.googleusercontent.com/-jw7mkT_Uojs/XAzxGf3l6fI/AAAAAAAAKbk/OfAc_BoehbwvVZRvRE-_Sg9VKPwPouXmACHMYCw/Image%252818%2529_thumb?imgmax=800" alt="Image(18)"  title="Image(18)" /></a></p>
<p>Once you save the changes, write down the <strong>Application ID.</strong></p>
<p>Here is the script to extract contents from the Excel spreadsheet.</p>
<pre tabindex="0"><code>Function Get-AccessToken ($TenantName, $ClientID, $redirectUri,  $resourceAppIdURI, $CredPrompt){  
  
    Write-Host &#34;Checking for AzureAD module...&#34;  
  
    if (!$CredPrompt){$CredPrompt \= &#39;Auto&#39;}  
  
    $AadModule \= Get-Module \-Name &#34;AzureAD&#34; \-ListAvailable  
  
    if ($AadModule \-eq $null) {$AadModule \= Get-Module \-Name &#34;AzureADPreview&#34;  \-ListAvailable}  
  
    if ($AadModule \-eq $null) {write-host &#34;AzureAD Powershell module is not  installed. The module can be installed by running &#39;Install-Module AzureAD&#39; or  &#39;Install-Module AzureADPreview&#39; from an elevated PowerShell prompt. Stopping.&#34; \-f  Yellow;exit}  
  
    if ($AadModule.count \-gt 1) {  
  
        $Latest\_Version \= ($AadModule | select version | Sort-Object)\[-1\]  
  
        $aadModule      \= $AadModule | ? { $\_.version \-eq $Latest\_Version.version  }  
  
        $adal           \= Join-Path $AadModule.ModuleBase  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.dll&#34;  
  
        $adalforms      \= Join-Path $AadModule.ModuleBase  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.Platform.dll&#34;  
  
        }  
  
    else {  
  
        $adal           \= Join-Path $AadModule.ModuleBase  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.dll&#34;  
  
        $adalforms      \= Join-Path $AadModule.ModuleBase  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.Platform.dll&#34;  
  
        }  
  
    \[System.Reflection.Assembly\]::LoadFrom($adal) | Out-Null  
  
    \[System.Reflection.Assembly\]::LoadFrom($adalforms) | Out-Null  
  
    $authority          \= &#34;https://login.microsoftonline.com/$TenantName&#34;  
  
    $authContext        \= New-Object  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.AuthenticationContext&#34;  \-ArgumentList $authority  
  
    $platformParameters \= New-Object  &#34;Microsoft.IdentityModel.Clients.ActiveDirectory.PlatformParameters&#34;     \-ArgumentList $CredPrompt  
  
    $authResult         \= $authContext.AcquireTokenAsync($resourceAppIdURI,  $clientId, $redirectUri, $platformParameters).Result  
  
    return $authResult  
  
    }  
  
Function Invoke-MSGraphQuery($AccessToken, $Uri, $Method, $Body){  
  
    Write-Progress \-Id 1 \-Activity &#34;Executing query: $Uri&#34; \-CurrentOperation  &#34;Invoking MS Graph API&#34;  
  
    $Header \= @{  
  
        &#39;Content-Type&#39;  \= &#39;application\\json&#39;  
  
        &#39;Authorization&#39; \= $AccessToken.CreateAuthorizationHeader()  
  
        }  
  
    $QueryResults \= @()  
  
    if($Method \-eq &#34;Get&#34;){  
  
        do{  
  
            \# $Results =  Invoke-RestMethod -Headers $Header -Uri $Uri  -UseBasicParsing -Method $Method -ContentType &#34;application/json&#34;  
  
            $Results \=  Invoke-RestMethod \-Headers $Header \-Uri $Uri \-Method  $Method  
  
            if ($Results.value \-ne $null){$QueryResults += $Results.value}  
  
            else{$QueryResults += $Results}  
  
            write-host &#34;Method: $Method | URI $Uri | Found:&#34; ($QueryResults).Count  
  
            $uri \= $Results.&#39;@odata.nextlink&#39;  
  
            }until ($uri \-eq $null)  
  
        }  
  
    Write-Progress \-Id 1 \-Activity &#34;Executing query: $Uri&#34; \-Completed  
  
    \# Return $QueryResults  
  
    $results |ConvertTo-Json \-Depth 5  
  
    }  
  
$resourceAppIdURI \= &#34;https://graph.microsoft.com&#34;  
  
$ClientID         \= &#34;5ccaxxxx-xxxx-4xxx-8dxx-75xxxxx08833&#34;   #AKA Application ID  
  
$TenantName       \= &#34;contoso.onmicrosoft.com&#34;             #Your Tenant Name  
  
$CredPrompt       \= &#34;Auto&#34;                                   #Auto, Always, Never,  RefreshSession  
  
$redirectUri      \= &#34;https://RedirectURI.com&#34;                #Your Application&#39;s  Redirect URI  
  
$Uri              \=  &#34;https://graph.microsoft.com/v1.0/groups/d727xxxx-8x0x-47xx-88xx-3xxxxxxxxb95/drive/root:/example.xlsx:/workbook/worksheets(&#39;{5118xxxx-7xxE-43xx-Axxx-14xxxxxxFC13}&#39;)/Range(address=&#39;Sheet1!A1:G10&#39;)&#34; #Query retrieve XLSX contents  
  
$Method           \= &#34;Get&#34;                                    #GET or PATCH  
  
$AccessToken      \= Get-AccessToken \-TenantName $TenantName \-ClientID $ClientID  \-redirectUri $redirectUri \-resourceAppIdURI $resourceAppIdURI \-CredPrompt  $CredPrompt  
  
$JSON \= @&#34;  
  
 {  
  
 &#34;userPrincipalName&#34;: &#34;tom@contoso.com&#34;  
  
 }  
  
&#34;@ #JSON Syntax if you are performing a PATCH  
  
Invoke-MSGraphQuery \-AccessToken $AccessToken \-Uri $Uri \-Method $Method  
</code></pre>]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Understand OAuth and Open ID Connect]]></title>
            <link href="/2018/11/understand-oauth-and-open-id-connect.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/11/understand-oauth-and-open-id-connect.html</id>
            
            <published>2018-11-17T19:39:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>IT world is always full of buzz words. “Digital Transformation”, “Automation”, “Blockchain”, “AI”, “Machine Learning”, etc, etc… We like to talk about them all the time, to show that we are not out of touch, we are up to date. Although I have to admit some of those words are used so often yet so few people have the really proper understanding of the actual technologies themselves. I believe OAuth is one of them. It’s a technology we use everyday, yet if you asked a lot of people why we need it and when to use, I bet not many will be able to explain (well). I hope this article can help you understand the technology better. After read it, hopefully you will at least have a basic idea of what OAuth is.</p>
<p>First, instead of providing a Wikipedia type definition for the term, let’s look at what particular technical issue OAuth helps us solve.</p>
<p>Like many other IT technologies, OAuth become widely adopted largely thanks for the prosperity of Smart Phones. It solves a key security issue came up while we use mobile apps on those phones.</p>
<p><strong>The Challenge 1</strong></p>
<p>Imaging this, a start up company created a mobile app which hosts cat videos (bad example, I know). Let’s call it CatTube… To help the app take off, you added a feature to send invitation to all Email contacts of a new subscriber.</p>
<p>To make the case a bit easier to understand, let’s assume the new user: Jack has a Gmail account. Now in order to access Jack’s Email contact list, CatTube need to somehow access Jack’s Gmail account. How can we do that?</p>
<p>Well, the easiest (dumbest) way is obviously to get Jack’s username and password for Gmail, and then CatTube will use this credential to log into Jack’s mailbox and retrieve his contact list. I know, this sounds ridiculous nowadays, right? But this is exactly what people were doing back then. Not only there is no technical guarantee that the credential will not be abused by the app provider, but what if their database got compromised, which never happens right? (check out <a class="gblog-markdown__link" href="https://haveibeenpwned.com/" title="https://haveibeenpwned.com/">https://haveibeenpwned.com/</a>)</p>
<p>So how can we delegate rights for CatTube, so it only retrieves the contact lists from Jack’s Gmail account and do nothing more or less?</p>
<p><strong>The Solution</strong></p>
<p>To solve this problem, we need a secure way to authorize CatTube’s delegation rights. This is where OAuth comes in. Let’s look at the below <strong>OAuth Authorization Flow</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-v5SDMnnUO14/W_D1fEe_LeI/AAAAAAAAKYM/T8_wG-QDpiQmq2Bs0GawzTw3CfXOI4odwCHMYCw/s1600-h/image%255B2%255D"><img src="https://lh3.googleusercontent.com/-MqDx1yevhOQ/W_D1gEeHWcI/AAAAAAAAKYQ/bmhVu0edv0kiq8oTRdefDFXWfLwl4RfXQCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a></p>
<p>A user click a button in CatTube app to allow it to access his/her Gmail contact list. After the button is clicked, the browser/mobile app send a request to Google servers holds the user accounts and asking for an authorization code for read contacts.</p>
<p>The initial request from the user contains following information</p>
<p>- What is the type this request for: Response type: Code –&gt; request an authorization code</p>
<p>- What are the rights this request want: Scope</p>
<p>- Where should the authorization code to send back to: Redirect URI</p>
<p>Google servers then respond by asking user to put his/her Google credential. Once the credential is verified, user will receive a confirmation prompt to confirm the delegation. After user clicked yes, Google generates the authorization code based on the request details and sends the <strong>Authorization Code</strong> to CatTube (through Redirect URI <em>CatTube.com/callback</em>).</p>
<p>After obtain the <strong>Authorization Code</strong>, CatTube <strong>backend servers</strong> send the <strong>Authorization Code</strong> back to Google to exchange for an <strong>Access Code</strong>. This is the actual code allows CatTube to do things. Google issues CatTube with the Access Code upon receiving the correct authorization code. CatTube <strong>backend servers</strong> then call upon Gmail API with this Access Code to retrieve the user Gmail contacts.</p>
<p>You may ask, why don’t we just issue the Access Code directly to the user, rather than going through these extra steps with 2 codes method (Authorization code and Access code).</p>
<p>Well, this is to prevent unauthorized access even when non-legit user got the authorization code.</p>
<p>If you look the diagram closely, you may notice I use two types of lines. The dot lines in the diagram represents communication between Google servers and CatTube servers. The solid line represents sessions between the user browsers/mobile app and servers.</p>
<p>To ensure Access Code will never be captured by the browser, it is always transferred in the backend. In case the authorization code is compromised, the unauthorized party will not be able to get the Access Code, as Google will only issue the code with CatTube servers.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-LxtDkq5eDa8/W_EvouDuhZI/AAAAAAAAKZM/ntTRUyFW_KcNfycxGdu5zqPBMDVXtUBmQCHMYCw/s1600-h/image%255B8%255D"><img src="https://lh3.googleusercontent.com/-1OGlG34TGkM/W_EvqMOg0XI/AAAAAAAAKZQ/L0dEH5bIhUEXd-sFjotxJEK-ZlLaDYKgACHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>What about those apps do not have backend, like Angular, React? You can use a different type of OAuth Flow, called <strong>Implicit</strong>, which is for pure front channel authorization. With Implicit flow, the app set <strong>Response type</strong> as <strong>token</strong> when send request to the authorization server. The authorization server will then provide the Access Code (the token) directly through user browser session.</p>
<p><strong>The Challenge 2</strong></p>
<p>From the above user case, we can clearly see OAuth is a very good way to handle delegated authorization for applications. But what if we want to log into an application with another identity provider (like Facebook or Google)? In the above example, we got access to Jack’s Gmail contacts, but CatTube does not know or care who the user is. What if Jack wants to log back into CatTube with his Google account?</p>
<p><strong>The Solution</strong></p>
<p>OAuth originally is an authorization protocol, while it thrives in space of Web app delegated authorization, it is not designed for authentication purpose. To solve the second challenge, we need an authentication protocol designed for web apps. OpenID Connect is here to help us with that. It is an extension of OAuth 2.0, which means it is built on top of OAuth protocol.</p>
<p>To authenticate users, OpenID Connect use the 2 attributes below to capture user information.</p>
<p>- ID Token: user information represent user id</p>
<p>- UserInfo: provide more user information</p>
<p>Because OpenID Connect is built on top of OAuth, it uses the OAuth flow. The only difference is, the initial request will have <strong>Scope</strong> set as OpenID. Then in return, upon the Authorization server receives the authorization code, it will return with a ID Token as well.</p>
<p>Below is the OpenID Connect flow</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-WkBybb47Mj8/W_EyfCgnD0I/AAAAAAAAKZs/FRkTTbnyTOMXkT28O6xXDp1yzS9rYN9-QCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-kGVaaRf_Ct4/W_EygJ08viI/AAAAAAAAKZw/Z-hW4dqc5Tk-4JZ0FE6w-Tl0BEOyGxkiwCHMYCw/image_thumb%255B3%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>So here you go, that’s a quick run down of the basic concept of OAuth and OpenID Connect.</p>
<p>This article is mostly based on Nate Barbettini’s explanation of OAuth 2.0 in his <a class="gblog-markdown__link" href="https://www.youtube.com/watch?v=996OiexHze0&amp;t=1098s">Youtube clip</a>. Hopefully, you find it is useful.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[How did I pass AWS Solution Architect Professional Exam in just 3 weeks]]></title>
            <link href="/2018/10/how-did-i-pass-aws-solution-architect.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/10/how-did-i-pass-aws-solution-architect.html</id>
            
            <published>2018-10-26T12:08:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>This post will be a bit different from my usual technical walk through. I recently passed AWS Solution Architect Professional exam with just three weeks of preparation. In this post, I want to share my experience with the exam itself as well as how I prepared for it. Hopefully you will find it somewhat useful.</p>
<p>I booked the exam on 1st Oct 2018. Honestly I didn’t expect myself to pass at that time. This was the first time I tried for the Professional exam. I did my SA Associate back in April 2017. I prepared that one for over 2 months. But for this professional one, I only decided to take the exam on 1st Oct after found out the Advanced Architect course I took offered a free exam voucher. I thought to myself, this will just be a good learning experience. But somehow I managed to pass the exam in the end!</p>
<p>Now I won’t deny luck plays a big part, given all the answers are in multi-choice format. But there I believe are also other key approaches I took contributed to my pass. After re-track my steps before the exam, I summarized these approaches into the points below.</p>
<p><strong>Start Preparation Early</strong></p>
<p>Yes. I know I indicated I only prepared for 3 weeks, that is not completely true. While I only decided to take the exam 3 weeks before the actual exam date, I have been (casually) preparing for SA Professional since early this year. At work I have done a few AWS related projects. At my own time, I watches lots of AWS related videos, either from YouTube, LinuxAcademy or aCloud Guru. I didn&rsquo;t strictly follow the exam blueprint while doing all these learning. They definitely filled my brain with lots of practical AWS Knowledge. Though I didn’t expect I would be ready in such a short notice. My original plan is to take it after at least another few months.</p>
<p><strong>Subscribe to an Online Course</strong></p>
<p>Early this year I bought one year Linux Academy subscription. They offer a wide range of AWS courses, include some deep dive courses for particular AWS services like Lambda and CloudFormation (Though some of the deep dive courses are rubbish, really just “scratch of the surface” kind). I know this is not a cheap option, cost around AU$250 even after MSDN discount. But for an IT professional I think the investment is worth a while. I am not sure about you, but to me if I am paying for the course, I will make sure I take value from it. I found one effective way is to listen to the lessons while driving. Most of time, my mind doesn&rsquo;t catch half of the lesson. But I am good with the other half&hellip;</p>
<p>In addition to LinuxAcademy, I subscribed to aCloud Guru for a 7 days trial just 2 weeks before the exam. aCloud Guru has good reputation for AWS training. I passed my SA Associate mainly by doing the aCloud Guru course. I won&rsquo;t say their SA Pro course is the top notch. But their CloudFormation course is worth the money. It&rsquo;s built on real world scenarios and had plenty useful template examples. One interesting tip, I cancelled the trial before it incurs charge. But somehow I was still able to watch course videos on my iPhone app afterwards, as long as you have the course in your recent play list.</p>
<p><strong>Took Instructor Led Course</strong></p>
<p>Again this is a costly item, the training course I did is Advanced Architecting on AWS, itself cost around AU$ 2500. Mine is covered by work. It comes with a voucher for SA Professional Exam. Which I was only made aware after finished the training. The voucher expires 1 month after the training!!!</p>
<p>The good thing about the course is it covers services Elastic Beanstalk, EMR, which came up a lot in my exam! There are usual hands on labs plus instructor led group discussions. I especially like the group discussion session, where students form into different teams and each team work on their own version of solution to a real world design scenario. Interestingly although I am the only student doesn’t have Solution Architect as job title, I found myself became the one leading the team conversation. It appears to me most classmates don’t have a lot of real exposure to AWS.</p>
<p><strong>Hands On Experience</strong></p>
<p>Like many other IT exams, it is key to have real world hands on experience with AWS SA Pro exam. Although most questions are about high level design, with practical knowledge, you will straight away see the correct answer that makes the most sense, because you have done that in the real world! A good example is question around AWS Identity solution setup. I have setup Azure AD as AWS SSO source in a recent project. So while I saw the answers, I just picked &ldquo;Setup Idp in IAM&rdquo;, &ldquo;Setup STS&rdquo;, and &ldquo;configure IAM role&rdquo; without hesitation.</p>
<p>What if your work doesn&rsquo;t involve AWS? Well, you can create your own projects. I usually have two or three dummy projects set for myself every month. And those projects does not necessarily match the exam blueprint. I spent a lot of time writing CloudFormation Templates, which is not an essential requirement for the exam. I played with containers in ECS that has never been part of the exam blueprint. But all these hands on experience enriches my overall AWS knowledge.</p>
<p><strong>The Exam</strong></p>
<p>Above are some essential elements I think are key to my success. I would also like to share the exam experience itself, as to me it is quite unique on its own (not necessarily a pleasant one).</p>
<p>I have done many certification exams in my IT career for different vendors and technologies, like Cisco, Juniper, Microsoft, VMware and AWS SA Asscoiate. But the SA Pro one still came as a surprise to me. My exam is booked at HWT building in South Bank Melbourne. Upon arrival, the receptionist did not ask for any IDs, only my kiosk number. I was then led into a small room with only two computers. Without any briefing or guidance the receptionist simply showed me to the computer I supposed to work on and left the room. The &ldquo;PSI exam kiosk&rdquo; has two webcam built with it along with a scanner for ID scanning. Follow the screen prompts, I was then asked to scan my IDs by using the built in scanner. However, after tried number of times, the scanner wasn&rsquo;t able to capture my ID and I had to put the IDs up to the cams for the exam monitor to eyeball them&hellip; Yes there is an exam monitor on the other side to live monitor and chat with you during the whole exam.</p>
<p>After verification, I finally got to start the exam. By this stage, no instructions have been provided how you should handle the usual stuff like your phone, wallet or making notes. But anyway, I was in the exam doing it like a boss&hellip; 5 mins in, got a message from the monitor asking me to put both my hands on the keyboard. Obviously this is the rule, you can&rsquo;t put your hands anywhere else, but keyboard&hellip; 20 mins into the exam, guess what&hellip; the exam app crashed! I had to use hand signal to inform the monitor and he/she restarted the computer for me. Fortunately, I didn&rsquo;t lose my progress, although my train of thoughts are derailed number of times by this stage. 1 and half hours into the exam, another message came up on the screen, tell me to keep my eyes on the screen!!! Wonderful, I can&rsquo;t even take a mental break! There are number of times I got interrupted during the exam in addition to the ones mentioned above. So overall if not I passed the exam, it is a terrible experience! I don&rsquo;t know if it is just for this exam site or all SA Professional are carried out like this. But be prepared, in addition to the tough questions, you may have to handle some extra rubbish.</p>
<p>That&rsquo;s all the information I can provide regards the exam. Again, hopefully you can pick some useful stuff from it and good luck with the exam! And last, if not sure, just choose &ldquo;C&rdquo; ;)</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Deploy Docker Image with AWS ECS (Part 2)]]></title>
            <link href="/2018/10/deploy-docker-image-with-aws-ecs-part-2.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/10/deploy-docker-image-with-aws-ecs-part-2.html</id>
            
            <published>2018-10-16T20:34:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>In Part 1 we uploaded a Docker image to AWS ECR. In this post, we will complete building the ECS Cluster and deploy the container image onto the cluster.</p>
<p>Note: The lab I worked on was recreated. The container image was renamed from webfront to testweb.</p>
<p>Before we start, you need to understand some ECS basic concepts.</p>
<p><strong>Task Definition</strong></p>
<p>A task definition describes one or more containers, their relationships, how they should be launched etc. It’s basically a JSON file contains the configuration details of the container(s).</p>
<p><strong>Task</strong></p>
<p>A task is the instantiation of a task definition. They are created based on the task definitions you provided.</p>
<p><strong>Service (Scheduler)</strong></p>
<p>In short, the service or service scheduler controls the tasks running across the ECS cluster.</p>
<p><strong>Cluster</strong></p>
<p>Cluster is the mothership of those tasks. It hosts the containers launched from the tasks.</p>
<p><strong>Container Instance</strong></p>
<p>By now, AWS ECS offers two types of ECS Cluster: Fargate and EC2. With Fargate, AWS manages the cluster resources for you. With EC2, ECS provisions EC2 instances based on your specification and you are in charge of maintaining those EC2 instances. Those EC2 instances are “Container Instances” as they are the ones host containers.</p>
<p><strong>Container Agent</strong></p>
<p>For a EC2 ECS cluster, container agent runs on each Container Instance. It is the key component that controls ECS tasks and resource utilization.</p>
<p>To learn more about ECS basic concepts, you can refer to <a class="gblog-markdown__link" href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/Welcome.html">this AWS document</a>.</p>
<p><strong>1. Create Task Definition</strong></p>
<p>The first step we take is to create a new Task Definition.</p>
<p>Choose <strong>EC2</strong> as the launch type.</p>
<p>Name the Task Definition as <strong>testweb-task</strong>.</p>
<p>Leave Network Mode as <default>, which will be Bridged for Linux instance.</p>
<p>Set Task size as below.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-rQH3Hs9Icrg/W8atqJFEMNI/AAAAAAAAKVc/MG8tdbCLL-swdTOcbDtxS9HCES4RZN4sACHMYCw/s1600-h/image%255B2%255D"><img src="https://lh3.googleusercontent.com/-ri9BQXFJzCc/W8atr_Gx4II/AAAAAAAAKVg/Dl-794wWZ18mFamYfMktaSDwLu7NhmuEQCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a></p>
<p>Click <strong>Add container</strong> to add the container image from ECR.</p>
<p>Name the container as <strong>testweb.</strong></p>
<p>Under <strong>Image</strong>, put in the ECR repo URL with tag, below is an example.</p>
<p><em>1234567891011.dkr.ecr.ap-southeast-1.amazonaws.com/testweb:latest</em></p>
<p>For memory limits, set hard limit as “512” and soft limit as “256”.</p>
<p>Set Port mappings as 80 to 80 tcp.</p>
<p>Click <strong>Add</strong> to add the container configuration.</p>
<p>Click Create <strong>to create the Task Definition.</strong></p>
<p>The Task Definition can also be created with JSON. Below is the code.</p>
<pre tabindex="0"><code>{  
  &#34;executionRoleArn&#34;: null,  
  &#34;containerDefinitions&#34;: \[  
    {  
      &#34;dnsSearchDomains&#34;: null,  
      &#34;logConfiguration&#34;: null,  
      &#34;entryPoint&#34;: null,  
      &#34;portMappings&#34;: \[  
        {  
          &#34;hostPort&#34;: 80,  
          &#34;protocol&#34;: &#34;tcp&#34;,  
          &#34;containerPort&#34;: 80  
        }  
      \],  
      &#34;command&#34;: null,  
      &#34;linuxParameters&#34;: null,  
      &#34;cpu&#34;: 512,  
      &#34;environment&#34;: \[\],  
      &#34;ulimits&#34;: null,  
      &#34;dnsServers&#34;: null,  
      &#34;mountPoints&#34;: \[\],  
      &#34;workingDirectory&#34;: null,  
      &#34;dockerSecurityOptions&#34;: null,  
      &#34;memory&#34;: 512,  
      &#34;memoryReservation&#34;: 256,  
      &#34;volumesFrom&#34;: \[\],  
      &#34;image&#34;: &#34;12345678910.dkr.ecr.ap-southeast-1.amazonaws.com/testweb:latest&#34;,  
      &#34;disableNetworking&#34;: null,  
      &#34;interactive&#34;: null,  
      &#34;healthCheck&#34;: null,  
      &#34;essential&#34;: true,  
      &#34;links&#34;: null,  
      &#34;hostname&#34;: null,  
      &#34;extraHosts&#34;: null,  
      &#34;pseudoTerminal&#34;: null,  
      &#34;user&#34;: null,  
      &#34;readonlyRootFilesystem&#34;: null,  
      &#34;dockerLabels&#34;: null,  
      &#34;systemControls&#34;: null,  
      &#34;privileged&#34;: null,  
      &#34;name&#34;: &#34;testweb&#34;  
    }  
  \],  
  &#34;placementConstraints&#34;: \[\],  
  &#34;memory&#34;: &#34;512&#34;,  
  &#34;taskRoleArn&#34;: null,  
  &#34;compatibilities&#34;: \[  
    &#34;EC2&#34;  
  \],  
  &#34;taskDefinitionArn&#34;: &#34;arn:aws:ecs:ap-southeast-1:297012811963:task-definition/testweb-task:1&#34;,  
  &#34;family&#34;: &#34;testweb-task&#34;,  
  &#34;requiresAttributes&#34;: \[  
    {  
      &#34;targetId&#34;: null,  
      &#34;targetType&#34;: null,  
      &#34;value&#34;: null,  
      &#34;name&#34;: &#34;com.amazonaws.ecs.capability.ecr-auth&#34;  
    },  
    {  
      &#34;targetId&#34;: null,  
      &#34;targetType&#34;: null,  
      &#34;value&#34;: null,  
      &#34;name&#34;: &#34;com.amazonaws.ecs.capability.docker-remote-api.1.21&#34;  
    }  
  \],  
  &#34;requiresCompatibilities&#34;: \[  
    &#34;EC2&#34;  
  \],  
  &#34;networkMode&#34;: null,  
  &#34;cpu&#34;: &#34;1024&#34;,  
  &#34;revision&#34;: 1,  
  &#34;status&#34;: &#34;ACTIVE&#34;,  
  &#34;volumes&#34;: \[\]  
}  
</code></pre><p><strong>2. Create ECS Cluster</strong></p>
<p>Under Amazon ECS click <strong>Clusters</strong> and click <strong>Create Cluster</strong>.</p>
<p>Select <strong>EC2 Linux + Networking</strong> and click <strong>Next step</strong>.</p>
<p>Name the cluster. I named it as testweb-clu, which is a bad name, you never want to name your cluster with a single image name, as the container suppose to present a micro service of your application.</p>
<p>Choose the EC2 instance type. For testing we will use t2.micro and 1 for Number of instances. Select a keypair so we can log into the Container Instances for some troubleshooting if needed.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-6bzT30sFlAg/W8a_LuWhMjI/AAAAAAAAKV8/mz67QJGxFL86XZrSwnIOstbnZdm6rIWbACHMYCw/s1600-h/image%255B5%255D"><img src="https://lh3.googleusercontent.com/-vvvO-6bwwKo/W8a_Mr1JTpI/AAAAAAAAKWA/evWbE17Hh7sWyAl8BdnItwPN-868KHUVQCHMYCw/image_thumb%255B1%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Under Networking, Create a new VPC, along with two new subnets.</p>
<p>Create a new security group and add rule to allow public access to TCP port 80.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-yeAnpBHmek4/W8a_N9_UN_I/AAAAAAAAKWE/rRiiuX4IhRUs-NBbs08oCykAwPBV_HfZQCHMYCw/s1600-h/image%255B8%255D"><img src="https://lh3.googleusercontent.com/-1sxWGaowNGQ/W8a_PKLKEdI/AAAAAAAAKWI/9M6zR9mTmM8uMQZfyZtkiC8-bUOjpr6zACHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>To allow the EC2 Container Instance to access ECS, we ill need to assign an IAM role to it. The IAM role contains 2 Amazon policies.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-GBamYe8OY5I/W8a_QeMR3pI/AAAAAAAAKWM/TUPjevgfa041G2wVCDzhjCnw1CGZ0Fu2gCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-IthuLqwSjH8/W8a_RVyc3FI/AAAAAAAAKWQ/jb_2WyHF4vglqlDmtTb6w-FWpeQit7OWwCHMYCw/image_thumb%255B3%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Click <strong>Create</strong> to create the Cluster. Interestingly, you can see the cluster provision is actually done through a CloudFormation template.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-vtmnPz7P8oM/W8a_StkvyAI/AAAAAAAAKWU/BjxeEnO4tpgrMrWTKUzWTWt15NCd44ZRACHMYCw/s1600-h/image%255B14%255D"><img src="https://lh3.googleusercontent.com/-iJ3IW2oxYJE/W8a_TkldYbI/AAAAAAAAKWY/WkJPPAwDhnwmICrj_km9-VYCiw-Zg-M9gCHMYCw/image_thumb%255B4%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>From the CloudFormation stack details, we can see all the script did was to provision a Launch Configuration and a AutoScaling Group.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-9iyQWPRL-O4/W8a_UlsAPfI/AAAAAAAAKWc/x65lG0CBex0DP5n09GytWr80gINP-07hwCHMYCw/s1600-h/image%255B17%255D"><img src="https://lh3.googleusercontent.com/-psk9l27yWCU/W8a_VgK2cUI/AAAAAAAAKWg/lFKfkUimUSEPLyz6KmTyLxYwx_bOlNbdgCHMYCw/image_thumb%255B5%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Here is the CloudFormation Template code.</p>
<pre tabindex="0"><code>AWSTemplateFormatVersion: &#39;2010-09-09&#39;  
Description: \&gt;  
  AWS CloudFormation template to create a new VPC  
  or use an existing VPC for ECS deployment  
  in Create Cluster Wizard. Requires exactly 1  
  Instance Types for a Spot Request.  
Parameters:  
  EcsClusterName:  
    Type: String  
    Description: \&gt;  
      Specifies the ECS Cluster Name with which the resources would be  
      associated  
    Default: default  
  EcsAmiId:  
    Type: String  
    Description: Specifies the AMI ID for your container instances.  
  EcsInstanceType:  
    Type: CommaDelimitedList  
    Description: \&gt;  
      Specifies the EC2 instance type for your container instances.  
      Defaults to m4.large  
    Default: m4.large  
    ConstraintDescription: must be a valid EC2 instance type.  
  KeyName:  
    Type: String  
    Description: \&gt;  
      Optional - Specifies the name of an existing Amazon EC2 key pair  
      to enable SSH access to the EC2 instances in your cluster.  
    Default: &#39;&#39;  
  VpcId:  
    Type: String  
    Description: \&gt;  
      Optional - Specifies the ID of an existing VPC in which to launch  
      your container instances. If you specify a VPC ID, you must specify a list of  
      existing subnets in that VPC. If you do not specify a VPC ID, a new VPC is created  
      with atleast 1 subnet.  
    Default: &#39;&#39;  
    ConstraintDescription: \&gt;  
      VPC Id must begin with &#39;vpc-&#39; or leave blank to have a  
      new VPC created  
  SubnetIds:  
    Type: CommaDelimitedList  
    Description: \&gt;  
      Optional - Specifies the Comma separated list of existing VPC Subnet  
      Ids where ECS instances will run  
    Default: &#39;&#39;  
  SecurityGroupId:  
    Type: String  
    Description: \&gt;  
      Optional - Specifies the Security Group Id of an existing Security  
      Group. Leave blank to have a new Security Group created  
    Default: &#39;&#39;  
  VpcCidr:  
    Type: String  
    Description: Optional - Specifies the CIDR Block of VPC  
    Default: &#39;&#39;  
  SubnetCidr1:  
    Type: String  
    Description: Specifies the CIDR Block of Subnet 1  
    Default: &#39;&#39;  
  SubnetCidr2:  
    Type: String  
    Description: Specifies the CIDR Block of Subnet 2  
    Default: &#39;&#39;  
  SubnetCidr3:  
    Type: String  
    Description: Specifies the CIDR Block of Subnet 3  
    Default: &#39;&#39;  
  AsgMaxSize:  
    Type: Number  
    Description: \&gt;  
      Specifies the number of instances to launch and register to the cluster.  
      Defaults to 1.  
    Default: &#39;1&#39;  
  IamRoleInstanceProfile:  
    Type: String  
    Description: \&gt;  
      Specifies the Name or the Amazon Resource Name (ARN) of the instance  
      profile associated with the IAM role for the instance  
  SecurityIngressFromPort:  
    Type: Number  
    Description: \&gt;  
      Optional - Specifies the Start of Security Group port to open on  
      ECS instances - defaults to port 0  
    Default: &#39;0&#39;  
  SecurityIngressToPort:  
    Type: Number  
    Description: \&gt;  
      Optional - Specifies the End of Security Group port to open on ECS  
      instances - defaults to port 65535  
    Default: &#39;65535&#39;  
  SecurityIngressCidrIp:  
    Type: String  
    Description: \&gt;  
      Optional - Specifies the CIDR/IP range for Security Ports - defaults  
      to 0.0.0.0/0  
    Default: 0.0.0.0/0  
  EcsEndpoint:  
    Type: String  
    Description: \&gt;  
      Optional - Specifies the ECS Endpoint for the ECS Agent to connect to  
    Default: &#39;&#39;  
  VpcAvailabilityZones:  
    Type: CommaDelimitedList  
    Description: \&gt;  
      Specifies a comma-separated list of 3 VPC Availability Zones for  
      the creation of new subnets. These zones must have the available status.  
    Default: &#39;&#39;  
  EbsVolumeSize:  
    Type: Number  
    Description: \&gt;  
      Optional - Specifies the Size in GBs, of the newly created Amazon  
      Elastic Block Store (Amazon EBS) volume  
    Default: &#39;0&#39;  
  EbsVolumeType:  
    Type: String  
    Description: Optional - Specifies the Type of (Amazon EBS) volume  
    Default: &#39;&#39;  
    AllowedValues:  
      \- &#39;&#39;  
      \- standard  
      \- io1  
      \- gp2  
      \- sc1  
      \- st1  
    ConstraintDescription: Must be a valid EC2 volume type.  
  DeviceName:  
    Type: String  
    Description: Optional - Specifies the device mapping for the Volume  
  UseSpot:  
    Type: String  
    Default: &#39;false&#39;  
  IamSpotFleetRoleArn:  
    Type: String  
    Default: &#39;&#39;  
  SpotPrice:  
    Type: String  
    Default: &#39;&#39;  
  SpotAllocationStrategy:  
    Type: String  
    Default: &#39;diversified&#39;  
    AllowedValues:  
      \- &#39;lowestPrice&#39;  
      \- &#39;diversified&#39;  
  UserData:  
    Type: String  
  IsWindows:  
    Type: String  
    Default: &#39;false&#39;  
Conditions:  
  CreateEC2LCWithKeyPair:  
    !Not \[!Equals \[!Ref KeyName, &#39;&#39;\]\]  
  SetEndpointToECSAgent:  
    !Not \[!Equals \[!Ref EcsEndpoint, &#39;&#39;\]\]  
  CreateNewSecurityGroup:  
    !Equals \[!Ref SecurityGroupId, &#39;&#39;\]  
  CreateNewVpc:  
    !Equals \[!Ref VpcId, &#39;&#39;\]  
  CreateSubnet1: !And  
    \- !Not \[!Equals \[!Ref SubnetCidr1, &#39;&#39;\]\]  
    \- !Condition CreateNewVpc  
  CreateSubnet2: !And  
    \- !Not \[!Equals \[!Ref SubnetCidr2, &#39;&#39;\]\]  
    \- !Condition CreateSubnet1  
  CreateSubnet3: !And  
    \- !Not \[!Equals \[!Ref SubnetCidr3, &#39;&#39;\]\]  
    \- !Condition CreateSubnet2  
  CreateWithSpot: !Equals \[!Ref UseSpot, &#39;true&#39;\]  
  CreateWithASG: !Not \[!Condition CreateWithSpot\]  
  CreateWithSpotPrice: !Not \[!Equals \[!Ref SpotPrice, &#39;&#39;\]\]  
Resources:  
  Vpc:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::VPC  
    Properties:  
      CidrBlock: !Ref VpcCidr  
      EnableDnsSupport: &#39;true&#39;  
      EnableDnsHostnames: &#39;true&#39;  
  PubSubnetAz1:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::Subnet  
    Properties:  
      VpcId: !Ref Vpc  
      CidrBlock: !Ref SubnetCidr1  
      AvailabilityZone: !Select \[ 0, !Ref VpcAvailabilityZones \]  
      MapPublicIpOnLaunch: true  
  PubSubnetAz2:  
    Condition: CreateSubnet2  
    Type: AWS::EC2::Subnet  
    Properties:  
      VpcId: !Ref Vpc  
      CidrBlock: !Ref SubnetCidr2  
      AvailabilityZone: !Select \[ 1, !Ref VpcAvailabilityZones \]  
      MapPublicIpOnLaunch: true  
  PubSubnetAz3:  
    Condition: CreateSubnet3  
    Type: AWS::EC2::Subnet  
    Properties:  
      VpcId: !Ref Vpc  
      CidrBlock: !Ref SubnetCidr3  
      AvailabilityZone: !Select \[ 2, !Ref VpcAvailabilityZones \]  
      MapPublicIpOnLaunch: true  
  InternetGateway:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::InternetGateway  
  AttachGateway:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::VPCGatewayAttachment  
    Properties:  
      VpcId: !Ref Vpc  
      InternetGatewayId: !Ref InternetGateway  
  RouteViaIgw:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::RouteTable  
    Properties:  
      VpcId: !Ref Vpc  
  PublicRouteViaIgw:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::Route  
    DependsOn: AttachGateway  
    Properties:  
      RouteTableId: !Ref RouteViaIgw  
      DestinationCidrBlock: 0.0.0.0/0  
      GatewayId: !Ref InternetGateway  
  PubSubnet1RouteTableAssociation:  
    Condition: CreateSubnet1  
    Type: AWS::EC2::SubnetRouteTableAssociation  
    Properties:  
      SubnetId: !Ref PubSubnetAz1  
      RouteTableId: !Ref RouteViaIgw  
  PubSubnet2RouteTableAssociation:  
    Condition: CreateSubnet2  
    Type: AWS::EC2::SubnetRouteTableAssociation  
    Properties:  
      SubnetId: !Ref PubSubnetAz2  
      RouteTableId: !Ref RouteViaIgw  
  PubSubnet3RouteTableAssociation:  
    Condition: CreateSubnet3  
    Type: AWS::EC2::SubnetRouteTableAssociation  
    Properties:  
      SubnetId: !Ref PubSubnetAz3  
      RouteTableId: !Ref RouteViaIgw  
  EcsSecurityGroup:  
    Condition: CreateNewSecurityGroup  
    Type: AWS::EC2::SecurityGroup  
    Properties:  
      GroupDescription: ECS Allowed Ports  
      VpcId: !If \[ CreateSubnet1, !Ref Vpc, !Ref VpcId \]  
      SecurityGroupIngress:  
        IpProtocol: tcp  
        FromPort: !Ref SecurityIngressFromPort  
        ToPort: !Ref SecurityIngressToPort  
        CidrIp: !Ref SecurityIngressCidrIp  
  EcsInstanceLc:  
    Type: AWS::AutoScaling::LaunchConfiguration  
    Condition: CreateWithASG  
    Properties:  
      ImageId: !Ref EcsAmiId  
      InstanceType: !Select \[ 0, !Ref EcsInstanceType \]  
      AssociatePublicIpAddress: true  
      IamInstanceProfile: !Ref IamRoleInstanceProfile  
      KeyName: !If \[ CreateEC2LCWithKeyPair, !Ref KeyName, !Ref &#34;AWS::NoValue&#34; \]  
      SecurityGroups: \[ !If \[ CreateNewSecurityGroup, !Ref EcsSecurityGroup, !Ref SecurityGroupId \] \]  
      BlockDeviceMappings:  
      \- DeviceName: !Ref DeviceName  
        Ebs:  
         VolumeSize: !Ref EbsVolumeSize  
         VolumeType: !Ref EbsVolumeType  
      UserData:  
        Fn::Base64: !Ref UserData  
  EcsInstanceAsg:  
    Type: AWS::AutoScaling::AutoScalingGroup  
    Condition: CreateWithASG  
    Properties:  
      VPCZoneIdentifier: !If  
        \- CreateSubnet1  
        \- !If  
          \- CreateSubnet2  
          \- !If  
            \- CreateSubnet3  
            \- \[ !Sub &#34;${PubSubnetAz1},  ${PubSubnetAz2},  ${PubSubnetAz3}&#34; \]  
            \- \[ !Sub &#34;${PubSubnetAz1},  ${PubSubnetAz2}&#34; \]  
          \- \[ !Sub &#34;${PubSubnetAz1}&#34; \]  
        \- !Ref SubnetIds  
      LaunchConfigurationName: !Ref EcsInstanceLc  
      MinSize: &#39;0&#39;  
      MaxSize: !Ref AsgMaxSize  
      DesiredCapacity: !Ref AsgMaxSize  
      Tags:  
        \-  
          Key: Name  
          Value: !Sub &#34;ECS  Instance  \-  ${AWS::StackName}&#34;  
          PropagateAtLaunch: &#39;true&#39;  
        \-  
          Key: Description  
          Value: &#34;This  instance  is  the  part  of  the  Auto  Scaling  group  which  was  created  through  ECS  Console&#34;  
          PropagateAtLaunch: &#39;true&#39;  
  EcsSpotFleet:  
    Condition: CreateWithSpot  
    Type: AWS::EC2::SpotFleet  
    Properties:  
      SpotFleetRequestConfigData:  
        AllocationStrategy: !Ref SpotAllocationStrategy  
        IamFleetRole: !Ref IamSpotFleetRoleArn  
        TargetCapacity: !Ref AsgMaxSize  
        SpotPrice: !If \[ CreateWithSpotPrice, !Ref SpotPrice, !Ref &#39;AWS::NoValue&#39; \]  
        TerminateInstancesWithExpiration: true  
        LaunchSpecifications:   
            \-  
              IamInstanceProfile:  
                Arn: !Ref IamRoleInstanceProfile  
              ImageId: !Ref EcsAmiId  
              InstanceType: !Select \[ 0, !Ref EcsInstanceType \]  
              KeyName: !If \[ CreateEC2LCWithKeyPair, !Ref KeyName, !Ref &#34;AWS::NoValue&#34; \]  
              Monitoring:  
                Enabled: true  
              SecurityGroups:  
                \- GroupId: !If \[ CreateNewSecurityGroup, !Ref EcsSecurityGroup, !Ref SecurityGroupId \]  
              SubnetId: !If  
                      \- CreateSubnet1  
                      \- !If  
                        \- CreateSubnet2  
                        \- !If  
                          \- CreateSubnet3  
                          \- !Join \[ &#34;,&#34; , \[ !Ref PubSubnetAz1, !Ref PubSubnetAz2, !Ref PubSubnetAz3 \] \]  
                          \- !Join \[ &#34;,&#34; , \[ !Ref PubSubnetAz1, !Ref PubSubnetAz2 \] \]  
                        \- !Ref PubSubnetAz1  
                      \- !Join \[ &#34;,&#34; , !Ref SubnetIds \]  
              BlockDeviceMappings:  
                    \- DeviceName: !Ref DeviceName  
                      Ebs:  
                       VolumeSize: !Ref EbsVolumeSize  
                       VolumeType: !Ref EbsVolumeType  
              UserData:  
                    Fn::Base64: !Ref UserData  
Outputs:  
  EcsInstanceAsgName:  
    Condition: CreateWithASG  
    Description: Auto Scaling Group Name for ECS Instances  
    Value: !Ref EcsInstanceAsg  
  EcsSpotFleetRequestId:  
      Condition: CreateWithSpot  
      Description: Spot Fleet Request for ECS Instances  
      Value: !Ref EcsSpotFleet  
  UsedByECSCreateCluster:  
    Description: Flag used by ECS Create Cluster Wizard  
    Value: &#39;true&#39;  
  TemplateVersion:  
    Description: The version of the template used by Create Cluster Wizard  
    Value: &#39;2.0.0&#39;  
</code></pre><p>Here is the parameters for the template</p>
<p>AsgMaxSize: 1</p>
<p>DeviceName: /dev/xvdcz</p>
<p>EbsVolumeSize: 22</p>
<p>EbsVolumeType: gp2</p>
<p>EcsAmiId: ami-0a3f70f0111af1d29</p>
<p>EcsClusterName: testweb-clu</p>
<p>EcsEndpoint:</p>
<p>EcsInstanceType: t2.micro</p>
<p>IamRoleInstanceProfile: arn:aws:iam::123456789103:instance-profile/ecsInstanceRole</p>
<p>IamSpotFleetRoleArn:</p>
<p>IsWindows: false</p>
<p>KeyName: tom-testkey</p>
<p>SecurityGroupId: sg-014e7a96be118e111</p>
<p>SecurityIngressCidrIp: 0.0.0.0/0</p>
<p>SecurityIngressFromPort: 80</p>
<p>SecurityIngressToPort: 80</p>
<p>SpotAllocationStrategy: diversified</p>
<p>SpotPrice:</p>
<p>SubnetCidr1: 10.0.0.0/24</p>
<p>SubnetCidr2: 10.0.1.0/24</p>
<p>SubnetCidr3:</p>
<p>SubnetIds: subnet-057b8e2cd11183e28</p>
<p>UserData: #!/bin/bash echo ECS_CLUSTER=testweb-clu &raquo; /etc/ecs/ecs.config;echo ECS_BACKEND_HOST= &raquo; /etc/ecs/ecs.config;</p>
<p>UseSpot: false</p>
<p>VpcAvailabilityZones: ap-southeast-1a,ap-southeast-1b,ap-southeast-1c</p>
<p>VpcCidr: 10.0.0.0/16</p>
<p>VpcId: vpc-08d111d48c2f68111</p>
<p><strong>3. Create Service</strong></p>
<p>Back in ECS, click the Cluster name.</p>
<p>Under Services tab, click Create.</p>
<p>Choose EC2 as the Launch Type, select the Task Definition we created in step1, select the cluster we created in step 2. Set <strong>Number of tasks</strong> to 1.</p>
<p>Use <strong>AZ Balanced Spread</strong> for <strong>Task Placement</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-IS4BEcRirTo/W8a_XF8zl1I/AAAAAAAAKWk/Hp6ATq-rcoIW4pngMwrZIr4XacCdOqxawCHMYCw/s1600-h/image%255B20%255D"><img src="https://lh3.googleusercontent.com/-VqrZbvSBVpo/W8a_YhkXZ2I/AAAAAAAAKWo/aIITCq8C81A9wIX04xLJzVL7WMUO81ldQCHMYCw/image_thumb%255B6%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Leave everything unchanged in <strong>Configure network</strong>.</p>
<p>Leave AutoScaling as “Do not adjust…”.</p>
<p>Click <strong>Create Service</strong> to create the service.</p>
<p><strong>4. Test</strong></p>
<p>Once the service is successfully created, The cluster will then start to provision the Container Instance and deploy the container image onto it. This process does seem to take time. In my case, it took around 55 minutes for the container to be started! So you definitely want to warm up your cluster in production.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-hCZOFQmxD38/W8a_Zo1scJI/AAAAAAAAKWs/5FjRLEmT8T87T-9ElHHIzQLw8A7zQ1bQQCHMYCw/s1600-h/image%255B23%255D"><img src="https://lh3.googleusercontent.com/-hyMAslr4nqE/W8a_avbWPpI/AAAAAAAAKWw/ZLJ289pC2lksaQ_LJmJGpNWiI29dCftxQCHMYCw/image_thumb%255B7%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Under Tasks, you should see a running task there.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-wTWwbrHucLA/W8a_bZrGM9I/AAAAAAAAKW0/2kQUOsimCt0Gd47JmV8BVL7Rl_IA260LgCHMYCw/s1600-h/image%255B26%255D"><img src="https://lh3.googleusercontent.com/-zdvoDS4eG1M/W8a_cuux0iI/AAAAAAAAKW4/DeOPv5WeIewCn0guhW7Whxev19MXOtW9ACHMYCw/image_thumb%255B8%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Go to EC2 and find the public IP of the Container Instance.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-OH7PtlYaeoI/W8a_dgmJzCI/AAAAAAAAKW8/vwJ86ngNoY0eA8sgvGIjoYtEMpS1HT1QQCHMYCw/s1600-h/image%255B29%255D"><img src="https://lh3.googleusercontent.com/-M2MJ7-EfpKc/W8a_fDyzrBI/AAAAAAAAKXA/zZQ-V24F9dcGIjlKhtB7_rVLKdsILL5ZQCHMYCw/image_thumb%255B9%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>In browser, type <a class="gblog-markdown__link" href="http://publicIP">http://publicIP</a> and you should see the boring but exciting Hello World message!</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-HCtt7aR0V0o/W8a_gZTIUTI/AAAAAAAAKXE/PlfRNdumZk0iaV3EjvHZBTV8Ij6sqiqDACHMYCw/s1600-h/image%255B32%255D"><img src="https://lh3.googleusercontent.com/-7pF37V9EmUk/W8a_hY9w4zI/AAAAAAAAKXM/XOk9GOgnEGYWPrP53PkMEXgl3ecK3m29gCHMYCw/image_thumb%255B10%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>In case you run into any issues, like the task refuse to launch, one place to look is the ECS Agent log on the Container Instance. The log can be found at /var/logs.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Create a PowerShell Module]]></title>
            <link href="/2018/09/create-powershell-module.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/09/create-powershell-module.html</id>
            
            <published>2018-09-27T02:18:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I came across an issue on our Hyper-V Cluster. One of the VM was stuck in the “Stopping” state. I had to force the VM to shutdown by kill its process on the Hyper-V host. To do so, I first find out the VM’s GUID and then kill the process with the same GUID. Needless to say, the whole process can be achieved with the PowerShell commands below.</p>
<pre tabindex="0"><code>\# Get the VM GUID and find the process with the GUID  
$VM \= Get-VM \-Name $VMName \-ErrorAction Stop  
$VMGUID \= $VM.Id  
$VMWMProc \= (Get-WmiObject Win32\_Process | Where-Object {$\_.Name \-match &#39;VMWP&#39; \-and $\_.CommandLine \-match $VMGUID})  
      
\# Kill the VM process  
Stop-Process ($VMWMProc.ProcessId) –Force  
</code></pre><p>This is a pretty short and simple script, which is perfect for making a module. We have number of Hyper-V hosts across our environment. Instead of copying the script around, module will help us better organize the code and make it more re-usable. I also like to share this small function with our global team, which module will serve nicely for this purpose.</p>
<p>Alright, let’s convert the script into a module then. Here’s the script Kill-VM.ps1. Let’s save it as Kill-VM.psm1. And now we have a module! That’s it?! Noooo… of course not. There are a few things we need to change with the script, before we can call it a proper module.</p>
<pre tabindex="0"><code>#requires -module Hyper-V  
#requires -runasadministrator  
  
\[CmdletBinding(SupportsShouldProcess=$True)\]  
Param(  
    \[Parameter(Mandatory\=$true,  
        HelpMessage\=&#34;The VM Name&#34;  
        )\]  
    \[String\]$VMName  
)  
  
Try{  
    #Get a VM object from local HyperV host  
    $VM \= Get-VM \-Name $VMName \-ErrorAction Stop  
    $VMGUID \= $VM.Id  
    $VMWMProc \= (Get-WmiObject Win32\_Process | Where-Object {$\_.Name \-match &#39;VMWP&#39; \-and $\_.CommandLine \-match $VMGUID})  
      
    \# Kill the VM process  
    Stop-Process ($VMWMProc.ProcessId) –Force  
    Write-Output &#34;$VMName is stopped successfully&#34;  
}Catch{  
    $ErrorMsg \= $Error\[0\] #Get the latest Error  
    Write-Output $ErrorMsg  
}  
</code></pre><p>First, we need to give a name to our module. Let’s call it “VMKiller”! “VMTerminator” is cooler, but a bit too long for my like…</p>
<p>Create a folder named “VMKiller” under “C:\Windows\System32\WindowsPowerShell\v1.0\Modules”. There are a few other default folders for PowerShell modules. You can find them out by run the command below.</p>
<pre tabindex="0"><code>$env:PSModulePath  
</code></pre><p>Save our script as VMKiller.psm1 into the “VMKiller” folder.</p>
<p>In the psm1 file, wrap the parameter and try sections into a function call “Kill-VM”.</p>
<p>We will also add Begin, Process and End section to handle Pipeline input for the function. This is not essential in this particular case. But will be useful, if we have ForEach loop in the function. The Process section will help us properly handle multiple objects inputted from pipeline.</p>
<p>Below is the code in VMKiller.psm1 after above changes.</p>
<pre tabindex="0"><code>Function Kill-VM  
{  
    \[CmdletBinding(SupportsShouldProcess=$True)\]  
    Param(  
        \[Parameter(Mandatory\=$true,  
            ValueFromPipeline\=$True,  
            ValueFromPipelineByPropertyName\=$True,  
            HelpMessage\=&#34;The VM Name&#34;  
            )\]  
        \[String\]$VMName  
    )  
    Begin{}  
    Process{  
        Try{  
            #Get a VM object from local HyperV host  
            $VM \= Get-VM \-Name $VMName \-ErrorAction Stop  
            $VMGUID \= $VM.Id  
            $VMWMProc \= (Get-WmiObject Win32\_Process | Where-Object {$\_.Name \-match &#39;VMWP&#39; \-and $\_.CommandLine \-match $VMGUID})  
              
            \# Kill the VM process  
            Stop-Process ($VMWMProc.ProcessId) –Force  
            Write-Verbose &#34;$VMName is stopped successfully&#34;  
        }Catch{  
            $ErrorMsg \= $\_.Exception.Message #Get the latest Error  
            Write-Verbose $ErrorMsg  
        }  
    }  
    End{}  
}  
</code></pre><p>Next, to make our module more understandable to future readers, we will add some Comment based Help information into the code. Comment based Help will allow users to read about the module function by using Get-Help command. You can read more about Comment Based Help from <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_comment_based_help?view=powershell-6">here</a>.</p>
<pre tabindex="0"><code>&lt;#  
.SYNOPSIS  
Kill a VM Process.  
  
.DESCRIPTION  
Use this module when a VM is stuck at Stopping state. It will kill the VM Process on the HyperV host.  
  
.PARAMETER VMName  
Name of the virtual machine.  
  
.EXAMPLE  
C:\\PS&gt;Kill-VM -VMName &#34;contoso-av1&#34;  
  
#&gt;  
</code></pre><p>The VMKiller module so far only contains a single function, and a single psm1 file. With what we have done so far, it is ready to be used. However, there are some additional bits I would like to show you. </p>
<p>It is not necessary to create Module Manifest, while we have only one single psm1 file. Without the Manifest, PowerShell will try to load <em>ModuleName.DLL</em> first. If that is not successful, it will then try <em>ModuleName.psm1</em>. But in this case we will create one anyway. To create the manifest, use the command below.</p>
<pre tabindex="0"><code>New-ModuleManifest –path .\\VMKiller.psd1  –RootModule VMKiller.psm1  
</code></pre><p>With the module file, you only need to modify one line, on line 72, remove the wildcard ‘*’ and replace it with ‘Kill-VM’ function. This is to specifically tell PowerShell which function to be exposed from this module. When you have a lot of functions in a module, this will improve the module loading performance.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-iiiXyEYd0Do/W68ZnCFfrVI/AAAAAAAAKUI/xhO2CT1hSigSdltfiC8uIbOt6nFB5eXWQCHMYCw/s1600-h/image%255B2%255D"><img src="https://lh3.googleusercontent.com/-XAaCd9_M5xI/W68ZoDYoVYI/AAAAAAAAKUM/hltE6FXqc04VR9SJyh9euEJfSaJ8BS8xwCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a></p>
<p>Now save all the files, let’s try load the VMKiller module and put it to use.</p>
<p>First, let’s load the module with Import-Module. You will get a warning about unapproved verb. This is because “Kill” is not something Microsoft will use in their command… They need to make sure everything is 200% politically correct. You can just ignore that, as I do not care.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-pAjcLBNT_Dg/W68ZpO6D9UI/AAAAAAAAKUQ/1KOXv84aPU8wGNfBpdj-UNMFXHnRoxdOACHMYCw/s1600-h/image%255B8%255D"><img src="https://lh3.googleusercontent.com/-87bwxrNUBZ8/W68Zp2Zp3TI/AAAAAAAAKUU/gcpG7nY0UuYfCZnRj33q2_6qdrE_I4LawCHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Next, let’s see what users will see if they try to learn about the function with Get-Help.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-ChUA8LnIYxs/W68ZrW4M2fI/AAAAAAAAKUY/sSBC_mA7eGgZhZAvCalFbVPvc7cB5dBNQCHMYCw/s1600-h/image%255B5%255D"><img src="https://lh3.googleusercontent.com/-USLv9gbtAQI/W68ZsZSlb0I/AAAAAAAAKUc/dmZc8JWkLsY7bfT8zKpSs08i_qCnDBjIQCHMYCw/image_thumb%255B1%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Now they know how to use the cmdlet, let’s put it to real use. Let’s stop a running VM.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-WbVWPw30dMc/W68ZtHMbVUI/AAAAAAAAKUg/wX1egnCDmGYcg0SQ4bzPDHJmKxAM2ZWewCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-cFMtMnrHejc/W68ZuLk6pXI/AAAAAAAAKUk/CnhCfcBNZPwz1q01mtGJER1fx75KfIHhgCHMYCw/image_thumb%255B3%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>And that’s it! You now have a module can be deployed to any other Hyper-V hosts. You can even use DSC to make sure it is installed on all Hyper-V hosts!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Build a PDC in Azure with DSC]]></title>
            <link href="/2018/09/build-pdc-in-azure-with-dsc.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/09/build-pdc-in-azure-with-dsc.html</id>
            
            <published>2018-09-07T17:19:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>There are a lot ARM templates out there can do this. But in this post, we will go through the nitty gritty of using DSC to automate the PDC setup. Before we begin, I assume you already know what DSC is and does. Otherwise, check it out <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/powershell/dsc/overview">here</a>.</p>
<p>First, let’s build a new VM in Azure with these PowerShell commands. In this case, the VM will have direct Internet and can be accessed via Internet directly. Just to state here that this is definitely not the practice you want to adopt in production.</p>
<pre tabindex="0"><code>\# Create a new resource group  
New-AzureRmResourceGroup \-Name $rsgName \-Location $location  
  
\# Create a subnet configuration  
$subnetConfig \= New-AzureRmVirtualNetworkSubnetConfig \-Name &#34;adsubnet&#34; \-AddressPrefix 10.188.0.0/24  
  
\# Create a virtual network  
$vnet \= New-AzureRmVirtualNetwork \-ResourceGroupName $rsgName \-Location $location \-Name &#34;tomlabVNET1&#34; \-AddressPrefix 10.188.0.0/16 \-Subnet $subnetConfig  
  
\# Create a public IP address and specify a DNS name  
$pip \= New-AzureRmPublicIpAddress \-ResourceGroupName $rsgName \-Location $location \-AllocationMethod Static \-IdleTimeoutInMinutes 4 \-Name &#34;tomlabpubdns$(Get-Random)&#34;  
  
\# Create an inbound network security group rule for port 22  
$nsgRuleRDP \= New-AzureRmNetworkSecurityRuleConfig \-Name &#34;tomlabSGRuleRDP&#34;  \-Protocol &#34;Tcp&#34; \-Direction &#34;Inbound&#34; \-Priority 1000 \-SourceAddressPrefix \* \-SourcePortRange \* \-DestinationAddressPrefix \* \-DestinationPortRange 3389 \-Access &#34;Allow&#34;  
  
\# Create a network security group  
$nsg \= New-AzureRmNetworkSecurityGroup \-ResourceGroupName $rsgName \-Location $location \-Name &#34;tomlabWinSG&#34; \-SecurityRules $nsgRuleRDP  
  
\# Define a credential object  
$cred \= Get-Credential \-message &#34;login for the new vm&#34; \-UserName &#34;tom&#34;  
  
\# Create a virtual network card and associate with public IP address and NSG  
$nic \= New-AzureRmNetworkInterface \-Name &#34;dc1VMNic&#34; \-ResourceGroupName $rsgName \-Location $location \-SubnetId $vnet.Subnets\[0\].Id \-PublicIpAddressId $pip.Id \-NetworkSecurityGroupId $nsg.Id  
  
\# Create a virtual machine configuration  
$vmConfig \= New-AzureRmVMConfig \-VMName $vmName \-VMSize &#34;Standard\_DS1\_v2&#34; | \`  
Set-AzureRmVMOperatingSystem \-Windows \-ComputerName $vmName \-Credential $cred | \`  
Set-AzureRmVMSourceImage \-PublisherName &#34;MicrosoftWindowsServer&#34; \-Offer &#34;WindowsServer&#34; \-Skus &#34;2016-Datacenter&#34; \-Version &#34;latest&#34; | \`  
Add-AzureRmVMNetworkInterface \-Id $nic.Id  
  
\# Create the VM  
New-AzureRmVM \-ResourceGroupName $rsgName \-Location $location \-VM $vmConfig  
</code></pre><p>Next we need to create an Automation Account in Azure. I am sure you know how to get that done.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-AUA-dVgFsQQ/W54DqxDorNI/AAAAAAAAKSE/2PEoWFEBHNQHkD5JeAicAj5TbiadY0w3gCHMYCw/s1600-h/image%255B2%255D"><img src="https://lh3.googleusercontent.com/-9op6zUhDfGk/W54DsEiGXBI/AAAAAAAAKSI/iKrn1VmNIWsE352QTnvB3_oRPEAcmdGjgCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a></p>
<p>As you are already in the portal, let’s get all the prerequisites ready before upload the DSC configuration.</p>
<p>First, go to <strong>Modules</strong> in the Automation Account and install following modules, if not already installed. They can be imported directly from <strong>Gallery.</strong></p>
<ul>
<li>xActiveDirectory</li>
<li>xStorage</li>
<li>xPendingReboot</li>
</ul>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-bscT94aYlLo/W54FD9M7RDI/AAAAAAAAKSY/A23GE_O5qa0uqD5kEFKEgnK7i8vqtkB0gCHMYCw/s1600-h/image%255B8%255D"><img src="https://lh3.googleusercontent.com/-1J6BOKQA3Ms/W54FFKD-mII/AAAAAAAAKSc/Xf4rR0jYp7Y8eG-ZcVOHMOQxQipgi2XogCHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Next, go to <strong>Credentials</strong> in the Automation Account and create following two credentials. The reason behind this is well explained in this <a class="gblog-markdown__link" href="https://stackoverflow.com/questions/43508467/azure-automation-dsc-using-pscredential-in-dsc-configuration">Stackoverflow article</a>.</p>
<ul>
<li>DCcred – Domain admin account</li>
<li>DCRecoveryCred – AD Recovery Password, this one you can put anything in the username, as it will not be used</li>
</ul>
<p>Now we can start write DSC for the PDC. Below is the code. Save it as a ps1 file. You will notice it requires all those 3 modules we just installed in Azure Automation and the PSCredential it pulls from the Automation Account.</p>
<pre tabindex="0"><code>#Requires -module xActiveDirecotry  
#Requires -module xStorage  
#Requires -module xPendingReboot  
  
configuration DSCNewPDC               
{               
    \# Get Automation Credentials from Azure Automation Account  
    $safemodeAdministratorCred \= Get-AutomationPSCredential \-Name &#34;DCRecoveryCred&#34;  
    $domainCred \= Get-AutomationPSCredential \-Name &#34;DCcred&#34;  
              
    Import-DscResource \-ModuleName xActiveDirectory  
    Import-DscResource \-ModuleName xStorage  
    Import-DscResource \-ModuleName xPendingReboot  
              
    Node $AllNodes.Where{$\_.Role \-eq &#34;Primary DC&#34;}.Nodename               
    {                
        WindowsFeature ADDSInstall               
        {               
            Ensure \= &#34;Present&#34;               
            Name \= &#34;AD-Domain-Services&#34;               
        }              
              
        xWaitforDisk Disk2  
        {  
            DiskId \= 2  
            RetryIntervalSec \= 10  
            RetryCount \= 30  
        }  
  
        xDisk DiskF  
        {  
            DiskId \= 2  
            DriveLetter \= &#39;F&#39;  
            DependsOn \= &#39;\[xWaitforDisk\]Disk2&#39;  
        }  
  
        xPendingReboot BeforeDC  
        {  
            Name \= &#39;BeforeDC&#39;  
            SkipCcmClientSDK \= $true  
            DependsOn \= &#39;\[WindowsFeature\]ADDSInstall&#39;,&#39;\[xDisk\]DiskF&#39;  
        }  
  
        \# Optional GUI tools   
        WindowsFeature ADDSTools              
        {               
            Ensure \= &#34;Present&#34;               
            Name \= &#34;RSAT-ADDS&#34;               
        }              
              
        \# No slash at end of folder paths   
        xADDomain FirstDS               
        {               
            DomainName \= &#39;tomlab.briwave.com.au&#39;               
            DomainAdministratorCredential \= $domainCred               
            SafemodeAdministratorPassword \= $safemodeAdministratorCred              
            DatabasePath \= &#39;F:\\NTDS&#39;              
            LogPath \= &#39;F:\\NTDS&#39;              
            DependsOn \= &#34;\[WindowsFeature\]ADDSInstall&#34;,&#34;\[xDisk\]DiskF&#34;,&#34;\[xPendingReboot\]BeforeDC&#34;      
        }              
              
    }               
}              
</code></pre><p>Apart from install the necessary roles and features, another thing worth noting is the configuration to check and use a 2nd disk for the NTDS log files. This is due to the factor that Azure OS disk by default has writing cache feature enabled, which could cause data corruption on AD DS. Here is the <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/azure/architecture/reference-architectures/identity/adds-extend-domain">MS document</a> supporting this argument.</p>
<p>Now, our original VM created with PowerShell does not have a 2nd disk attached. Let’s use the commands below to achieve that.</p>
<pre tabindex="0"><code>\# Add a managed disk to VM  
$rsgName \= &#34;tomlab-au&#34;  
$location \= &#34;Australia Southeast&#34;  
$vmName \= &#34;testdc1&#34;  
$diskName \= $vmName + &#39;\_datadisk1&#39;  
$diskConfig \= New-AzureRmDiskConfig \-AccountType PremiumLRS \-Location $location  \-CreateOption Empty \-DiskSizeGB 128  
$dataDisk1 \= New-AzureRmDisk \-DiskName $diskName \-Disk $diskConfig  \-ResourceGroupName $rsgName  
$vm \= Get-AzureRmVM \-Name $vmName \-ResourceGroupName $rsgName  
$vm \= Add-AzureRmVMDataDisk \-VM $vm \-Name $diskName \-CreateOption Attach  \-ManagedDiskId $dataDisk1.Id \-Lun 1  
Update-AzureRmVM \-VM $vm \-ResourceGroupName $rsgName  
</code></pre><p>Next we upload our DSC script to Azure.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-DObUVrGLWE4/W54mqQBGA8I/AAAAAAAAKTE/EOfOuQsb-dAHUFttrOd3VPAuN8JlwYyVgCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-PmWLiylbavQ/W54mrmDv6VI/AAAAAAAAKTI/va4Mf211H9cVQ2-p7Re08NxqzIIntbJLACHMYCw/image_thumb%255B3%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>After upload the script, we need to compile it so it can be pushed to the Azure Windows VM. This is done through the PowerShell script below. As you can see, we need to specify the parameters and configData.</p>
<pre tabindex="0"><code>Param(  
\[Parameter(Mandatory=$True)\]  
\[String\]$rsgName,  
\[Parameter(Mandatory=$True)\]  
\[String\]$autoAccountName,  
\[Parameter(Mandatory=$True)\]  
\[String\]$configName  
)  
$ConfigData \= @{  
       AllNodes \= @(  
              @{  
                     NodeName \= &#34;\*&#34;  
                     PSDscAllowPlainTextPassword \= $True  
              },  
              @{  
                     NodeName \= &#34;testdc1&#34;  
            Role \= &#34;Primary DC&#34;  
              }  
       )  
}  
\# Script to complie DSC Configuration in Azure Automation Account  
Start-AzureRmAutomationDscCompilationJob \-ResourceGroupName $rsgName  \-AutomationAccountName $autoAccountName \-ConfigurationName $configName  \-ConfigurationData $ConfigData  
</code></pre><p>If all goes well, you should see something like below.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-Qzt6fP_2SNA/W54msVTaY_I/AAAAAAAAKTM/fhTv40ScJVUYmEZf2dA8fTffZxMyLUqsQCHMYCw/s1600-h/image%255B14%255D"><img src="https://lh3.googleusercontent.com/-Dsm52ZpZs24/W54mtK9zyII/AAAAAAAAKTQ/NXPyG2FN3B8frVBx_EstrNcy5PJwAd3rACHMYCw/image_thumb%255B4%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Next, we need to apply the complied configuration to the node (Windows VM). You will need to reboot the server after the initial DSC push. This is to apply those roles and features.</p>
<p>Once the configuration is applied, it will take a while for Azure Automation to get the final status.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-iWBT3FWKZhM/W54muZZBQEI/AAAAAAAAKTU/CfsX2SLF2c4-YGyDafMl7jl4e96fW532gCHMYCw/s1600-h/image%255B20%255D"><img src="https://lh3.googleusercontent.com/-dvHNKacGWHU/W54mvU351WI/AAAAAAAAKTY/rhejI5gntbcXeOJW9uG9XwWlNVgdkisxACHMYCw/image_thumb%255B6%255D?imgmax=800" alt="image"  title="image" /></a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Install AWS CLI on WSL Ubuntu]]></title>
            <link href="/2018/09/install-aws-cli-on-wsl-ubuntu.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/09/install-aws-cli-on-wsl-ubuntu.html</id>
            
            <published>2018-09-01T03:43:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Within Windows command prompt, AWS CLI does not provide autocomplete feature. According to AWS document, the feature is only available on “Unix-like systems”.</p>
<p>Luckily we have WSL (Windows Subsystem for Linux)on Windows 10. We can simply run AWS CLI from the WSL with autocomplete provided natively. Problem solved!</p>
<p>Here are the steps I took to get AWS CLI installed on my WSL Ubuntu.</p>
<p>Before we install AWS CLI package itself, we need to get Python package manager pip installed first.</p>
<p>Download pip install script. Notice I use –k here, this is because I am running this behind company proxy, the proxy changes HTTPS certificate to its own certificate. Without –k the command will fail. You can leave it out if you have direct Internet access</p>
<pre tabindex="0"><code>$ curl \-O \-k https://bootstrap.pypa.io/get-pip.py  
</code></pre><p>Next, the usual update apt command</p>
<pre tabindex="0"><code>$ sudo apt-gt update  
</code></pre><p>Then we download and install Python minimal</p>
<pre tabindex="0"><code>$ sudo apt install python-minimal  
</code></pre><p>Now we can install pip, the &ndash;trusted-host here is again due to the fact that I am downloading all these packages behind proxy.</p>
<pre tabindex="0"><code>$ python get-pip.py \--user \--trusted-host=files.pythonhosted.org  
</code></pre><p>Next, verify that pip is installed correctly.</p>
<pre tabindex="0"><code>$ pip \--version  
</code></pre><p>Finally, we can now use pip to install the AWS CLI.</p>
<pre tabindex="0"><code>$ pip install awscli \--upgrade \--user  
</code></pre><p>Verify the install by run aws &ndash;version.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-chodvDmzCKs/W4ptOAkOEDI/AAAAAAAAKQI/3ONu1xB5Dyc9V2W9ZO0ML4_ST7bJgtOfgCHMYCw/s1600-h/image%255B7%255D"><img src="https://lh3.googleusercontent.com/-3xgongnuYzw/W4ptPsakO2I/AAAAAAAAKQM/kl2cVBLQG4wDN5u81i71Ot6EB-2GSbv4QCHMYCw/image_thumb%255B5%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Now let’s do something fun. Rather than enable the native AWS Completer, there is a pretty cool autocomplete tool call <strong>aws-shell</strong>. Here is its <a class="gblog-markdown__link" href="https://github.com/awslabs/aws-shell">GitHub link</a>.</p>
<p>To install aws-shell, run the pip install command below</p>
<pre tabindex="0"><code>$ pip install aws-shell –-user  
</code></pre><p>Run aws-shell to get into the shell. The tool will not only prompt for parameters, but can also retrieve information from AWS and promote for completion, like existing stack name, Security Group Ids, etc.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-di6gYNL1iZw/W4ptQkmIN6I/AAAAAAAAKQQ/Hw9ztyqITi4lK03w4-c4jJnk2GndoIGcACHMYCw/s1600-h/image%255B13%255D"><img src="https://lh3.googleusercontent.com/-k9iwQjPluf4/W4ptR4tQImI/AAAAAAAAKQU/28exd4fNfacshoIazxd-8aAZaC9EGTlRgCHMYCw/image_thumb%255B9%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Have fun!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Deploy Docker Image with AWS ECS (Part 1)]]></title>
            <link href="/2018/08/deploy-docker-image-with-aws-ecs-part-1.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/08/deploy-docker-image-with-aws-ecs-part-1.html</id>
            
            <published>2018-08-24T13:00:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>One of the things I have been working on is to help our developers to containerize their applications and deploy them to AWS ECS. <br>
In this post, I will walk through the steps to upload a Docker image to AWS ECR (Elastic Container Repository).<br>
As the first step, we need to provision the ECR with CloudFormation template.<br>
Below is a simple CFN template written in YAML.</p>
<pre tabindex="0"><code>AWSTemplateFormatVersion: &#34;2010-09-09&#34;  
  
Description: \&gt;  
  Play stack  
  
Parameters:  
  RepoName:  
    Default: tomrepo  
    Description: ECR Repoistory Name       
    Type: String     \# required  
    ConstraintDescription: must be a name  
  
Resources:  
  myrepo:  
    Type: AWS::ECR::Repository  
    Properties:  
      RepositoryName: !Ref RepoName       
  mycluster:  
    Type: AWS::ECS::Cluster  
    Properties:  
      ClusterName: tomecscluster       
Outputs:  
  AWSTemplateFormatVersion:  
    Description: Tempalte version  
    Value: &#34;1.0&#34;  
</code></pre><p>Deploy the CFN template with AWS CLI command below.</p>
<pre tabindex="0"><code>aws cloudformation create-stack --stack-name createecs --template-body file://mytemplate.yaml --parameters ParameterKey=RepoName,ParameterValue=webfront  
</code></pre><p>After the template is successfully deployed, go to ECS in AWS Console and confirm the repository and cluster is created.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-COhxC5Zzdyc/W4Bu0A6hxZI/AAAAAAAAKOM/w3-Fz2iP6Z4WnlmI8mpP9o7jCH34-nu7gCHMYCw/s1600-h/image2"><img src="https://lh3.googleusercontent.com/-FHfR1n5QJZk/W4Bu12q0sjI/AAAAAAAAKOQ/55V03QsV8cYOJfNmT-lW7YRZ8yFf0KDXgCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a><br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-Ka6pWXgfFNA/W4Bu23wlw3I/AAAAAAAAKOU/a61y55PUPXg0mfCeRfGVhSNnikET1v2awCHMYCw/s1600-h/image8"><img src="https://lh3.googleusercontent.com/-Bn_yNuO_evo/W4Bu4LiU8vI/AAAAAAAAKOY/d655VtLTPK0_C7yPw72-5OIQwONfz2eYgCHMYCw/image_thumb2?imgmax=800" alt="image"  title="image" /></a><br>
Next we need to build the Docker image locally to be used for this deployment. In this example I use my local Docker engine installed on my Win 10 workstation. You can learn more about Docker for Windows <a class="gblog-markdown__link" href="https://docs.docker.com/docker-for-windows/">here</a>.<br>
Once Docker is installed and running, you should see a little docker icon in the task tray. Make sure it is switched to Linux Containers.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-EFUJu1rdWlM/W4Bu5AAtZzI/AAAAAAAAKOc/zSp9NIqfwUISXbExqIbj6SHEoAd87FCpwCHMYCw/s1600-h/image11"><img src="https://lh3.googleusercontent.com/-TtMI-MYdcKI/W4Bu6V2UEeI/AAAAAAAAKOg/XZhQZriiKKMXvMLvrfuxTPeEYUPJ8K9iACHMYCw/image_thumb3?imgmax=800" alt="image"  title="image" /></a><br>
Open CMD and create a folder to host the docker image.<br>
Create a file named as “Dockfile” in the folder. Yes, it does not have an extension in the file name.<br>
Copy the code below to the file. The Dockerfile basically uses a Ubuntu image and installed Apache web service on it along with a simple webpage named index.html.</p>
<pre tabindex="0"><code>FROM ubuntu:12.04  
  
\# Install dependencies  
RUN apt-get update -y  
RUN apt-get install -y apache2  
  
\# Install apache and write hello world message  
RUN echo &#34;Hello World!&#34; &gt; /var/www/index.html  
  
\# Configure apache  
RUN a2enmod rewrite  
RUN chown -R www-data:www-data /var/www  
ENV APACHE\_RUN\_USER www-data  
ENV APACHE\_RUN\_GROUP www-data  
ENV APACHE\_LOG\_DIR /var/log/apache2  
  
EXPOSE 80  
  
CMD \[&#34;/usr/sbin/apache2&#34;, &#34;-D&#34;,  &#34;FOREGROUND&#34;\]  
</code></pre><p>Save the file and we can now build the image.</p>
<pre tabindex="0"><code>docker build -t webserver .  
</code></pre><p>Before we upload the image to ECR, we need to test it out locally first.<br>
Before you run the image (create the container), make sure port 80 on your local machine is not used.</p>
<pre tabindex="0"><code>docker run -p 80:80 webserver  
</code></pre><p>Now, test access to <a class="gblog-markdown__link" href="http://localhost/">http://localhost</a>, you should see the “Hello World” page.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-Yng9YJFvA_M/W4Bu7KLRhyI/AAAAAAAAKOk/Wle0ovT7KfUZM8N3laPetigNEQb8FsXAQCHMYCw/s1600-h/Image%255B3%255D"><img src="https://lh3.googleusercontent.com/-i3nXESQAcss/W4Bu8D16b2I/AAAAAAAAKOo/4j4-TAtiP808GXvQp0InEI8zQTvXTkNYwCHMYCw/Image_thumb%255B1%255D?imgmax=800" alt="Image"  title="Image" /></a><br>
The image is now ready to be pushed to AWS ECR.<br>
First, we need to retrieve the login information to authenticate our local Docker client with ECR. Run the command below and <strong>COPY the output.</strong> Change the region parameter if your ECR is in a different region.</p>
<pre tabindex="0"><code>aws ecr get-login --no-include-email --region ap-southeast-1  
</code></pre><p>The output of the command should look like something below.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-4b9RT79-dDc/W4Bu8-oTKuI/AAAAAAAAKOs/2rAcor_I90wVrD6bFnw-5oOJAEfOTNX2ACHMYCw/s1600-h/image%255B6%255D"><img src="https://lh3.googleusercontent.com/-YRX2xPH3zTI/W4Bu9-IXmnI/AAAAAAAAKOw/dvVbGCibbgQPZh2UvlMpJNRu-86Ukr2qACHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a><br>
Copy the whole output and paste it into command line. Press enter to execute it.<br>
You should get output like below, indicating your Docker client is now authenticated with AWS ECR.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-GIP_Q6cKQoQ/W4Bu-sUZT1I/AAAAAAAAKO0/BTZr-y8mJukVijL9TGWeLjqaNXmPo3p6QCHMYCw/s1600-h/Image%255B9%255D"><img src="https://lh3.googleusercontent.com/-gKb1zqenPPs/W4BvAFtz6JI/AAAAAAAAKO4/2MTZnsYNZyEMEzpmK3Y-2aiCcs7vJ3JyACHMYCw/Image_thumb%255B3%255D?imgmax=800" alt="Image"  title="Image" /></a><br>
Tag the image you got and make it ready for the push. If you run docker images, you should see a new image with aws tag<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-54opLlWhbgU/W4BvBRTPv2I/AAAAAAAAKO8/L12b2JDn34s5ZJQeqSVPz8lM5ny0myAwwCHMYCw/s1600-h/Image%25288%2529%255B5%255D"><img src="https://lh3.googleusercontent.com/-Cd3DPbOb3Ns/W4BvCd9sfjI/AAAAAAAAKPA/tUnVjjymTP8ytNLMSdgaGZTrkFjDC1AwQCHMYCw/Image%25288%2529_thumb%255B2%255D?imgmax=800" alt="Image(8)"  title="Image(8)" /></a><br>
You can now push the image.</p>
<pre tabindex="0"><code>docker push 1234567891011.dkr.ecr.ap-southeast-1.amazonaws.com/webfront:latest  
</code></pre><p>It may take sometime for the image to be uploaded. But eventually you should see something like below.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-2nsci46X3tA/W4BvDXmDadI/AAAAAAAAKPE/FHwvDKiefrEqjqe4mMv16TCJCmUWgQUWACHMYCw/s1600-h/Image%255B13%255D"><img src="https://lh3.googleusercontent.com/-pGvW8cE4-4o/W4BvEn4QhgI/AAAAAAAAKPM/ZciAL3XjYxIAbiWedCnxPCgJHMc-w64SgCHMYCw/Image_thumb%255B5%255D?imgmax=800" alt="Image"  title="Image" /></a><br>
In AW Console, under the ECR “webfront” repository, you should now see the image tagged as latest.<br>
<a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-5ro2utj5Se4/W4BvFv9m-eI/AAAAAAAAKPQ/UwnXCfYrh7snOLXyVXU6OBtQnEcg_ECJgCHMYCw/s1600-h/image%255B16%255D"><img src="https://lh3.googleusercontent.com/-_SUf23sxBgo/W4BvGQkx67I/AAAAAAAAKPU/pmoWUFoNtYksSOToCR4yEB7K9TO4yrSRACHMYCw/image_thumb%255B6%255D?imgmax=800" alt="image"  title="image" /></a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Azure - Update Existing RSG with ARM Template]]></title>
            <link href="/2018/08/azure-update-existing-rsg-with-arm.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/08/azure-update-existing-rsg-with-arm.html</id>
            
            <published>2018-08-10T14:03:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Do you ever find yourself face this kind the situation: You are told to provision new resources with ARM templates to an existing resource group that already has VMs and vNets built and running. How can you add new subnets and VMs to the resource group without breaking those ones already there?</p>
<p>Unlike AWS Cloud Formation Templates, Microsoft ARM Templates do not provide “Update” option for past deployments. In order to modify the existing environment, the usual option is to make the change through CLI, PowerShell or Azure portal. But what if you somehow have to make the change through ARM template?</p>
<p>One way to do this is to create a template that enlists all existing resources and then add changes to it.</p>
<p>Below is a lab setup I created through portal. Assume there is a requirement to create another subnet in the same vNet.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-63puMomjfFE/W239dxOUQPI/AAAAAAAAKLM/wy8Xlu2W7nMa7IhORRljqQafijdYU2OBACHMYCw/s1600-h/Image14"><img src="https://lh3.googleusercontent.com/-zdY2MTJ220A/W239ez5FVKI/AAAAAAAAKLQ/yTVg_CazjHcnfOQA1ylzMe0PWan193rEgCHMYCw/Image_thumb4?imgmax=800" alt="Image"  title="Image" /></a></p>
<p>First, we use Azure Resource Group’s <strong>Automation script</strong> feature to generate a ARM template based on the existing resources.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-ZlAqXtDik30/W239f7bMuWI/AAAAAAAAKLU/vsPKFi8lkgEL8Wxmb9_o6mF_J5YxY0HnACHMYCw/s1600-h/image5"><img src="https://lh3.googleusercontent.com/-TIDpoUfy6OY/W239hV643pI/AAAAAAAAKLY/eP0zvNzZGpQfxiDSzNm2MsVNquhdkGMFgCHMYCw/image_thumb1?imgmax=800" alt="image"  title="image" /></a></p>
<p><strong>Reformat and Cleanup ARM Template</strong></p>
<p>Next, download the whole deployment package as a ZIP file and Extract to local folder.</p>
<p>Open the folder in <strong>Visual Studio Code</strong>. The only files we will be changing are “template.json” and “parameters.json”.</p>
<p>Enable GIT for source control.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-IqpXxAuatBM/W25jzbWJFVI/AAAAAAAAKMY/AYXp86Im0G8TB6udPbMXfaLxxMx-CHArgCHMYCw/s1600-h/image%255B5%255D"><img src="https://lh3.googleusercontent.com/-Q0WCl3WdZ3M/W25j0Qo0evI/AAAAAAAAKMc/w_y5_Fv8KUs-9zDcQFy7sWYviM3WLCxAACHMYCw/image_thumb%255B1%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Because all we want is to update existing resource, we will need to keep all the parameter values same as the resources already created. To do that, we convert all the parameters to variables.</p>
<p>In <strong>parameters.json</strong> file, cut all the parameters to a new file.</p>
<p>The parameters are list in the format below. Conveniently it has its current value appended in the parameter name. So first, we move this value into the “value” field.</p>
<pre tabindex="0"><code>&#34;virtualMachines\_proxyvm1\_name&#34;: {  
            &#34;value&#34;: null  
         },  
</code></pre><p>After the change, the parameter looks like this:</p>
<pre tabindex="0"><code>&#34;virtualMachines\_name&#34;: {  
             &#34;value&#34;: proxyvm1  
         },  
</code></pre><p>Next, remove “{}” and “value:” from the new file. (VS Tip: use Ctrl + F2 to edit multiple phrases)</p>
<pre tabindex="0"><code>&#34;virtualMachines\_name&#34;: &#34;proxyvm1&#34;,  
&#34;virtualNetworks\_name&#34;: &#34;tomlabVNET1&#34;,  
&#34;networkInterfaces\_name&#34;: &#34;proxyVMNic&#34;,  
&#34;networkSecurityGroups\_name&#34;: &#34;tomlabLinuxSG&#34;,  
&#34;publicIPAddresses\_name&#34;: &#34;tomlabpubdns1940326763&#34;,  
&#34;storageAccounts\_name&#34;: &#34;visuatomlabprox072316000&#34;,  
&#34;virtualMachines\_id&#34;: &#34;/subscriptions/911c1a11-0db1-11a1-b011-fa1a1e01ce11/resourceGroups/tomlab-kr/providers/Microsoft.Compute/disks/proxyvm1\_OsDisk\_1\_11a11f1c0c11111db1fcab111aa11cd1&#34;  
</code></pre><p>Paste the contents from the new file to Template.json Variables section.</p>
<p>We now need to remove the parameters from the “parameters{ }” section in <strong>template.json</strong>, as they are replaced with variables.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-P47t7-ksYhU/W25kr0maVfI/AAAAAAAAKNE/779qRbJrqf0HyknVxs0PqBaiLD-4hn5VgCHMYCw/s1600-h/Image%255B8%255D"><img src="https://lh3.googleusercontent.com/-IQyVaBXfywI/W25kszHkiZI/AAAAAAAAKNI/gbO1ElZZgPUty0Ga6J5tCD2cI3OYdwHyQCHMYCw/Image_thumb%255B2%255D?imgmax=800" alt="Image"  title="Image" /></a></p>
<p>We also need to replace the parameters in the “resources{ }” section with “variables” and their changed names as well. This is the most tedious part of the conversion. I found <strong>VS Code</strong>’s multi-selection feature is quite handy to do this job.</p>
<p>To further clean the template, remove “comments” and “provisioningState” lines.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-hsDBwuC07Z0/W25ktnEPmlI/AAAAAAAAKNM/_RkiZUCD0jwRHTXTNHFe_dRTKrH1CyrwQCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-FISZ9aVu0tI/W25kuRweBcI/AAAAAAAAKNQ/yYTbssZeFVMh9ITkPGSBXafw0oCDYBFrQCHMYCw/image_thumb%255B3%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-SHFZX6JYl7o/W25kvBMyabI/AAAAAAAAKNU/j3kVUfcbuOkuJJTiKA8DWvSrG0BDZfoZgCHMYCw/s1600-h/image%255B14%255D"><img src="https://lh3.googleusercontent.com/-xTPHL709uvY/W25kvyG--TI/AAAAAAAAKNY/DTbm7JrtPX45zQ7btda0dV7xrkjSPVhqQCHMYCw/image_thumb%255B4%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>After the modification your template should have all parameters replaced with variables.</p>
<p><strong>Validate the ARM Template</strong></p>
<p>Use Azure CLI to validate the template (you will need to sign in first with az login). Since there is no parameter used, we don’t need to specify the parameter file location.</p>
<pre tabindex="0"><code>az group deployment validate \--resource\-group tomlab\-kr \--template\-file .\\template.json   
</code></pre><p>Once confirm the template is all valid, we can now add the new resource, in this case a new subnet to this template.</p>
<p>The new subnet is added as a property of <strong>virtualNetwork.</strong></p>
<pre tabindex="0"><code>&#34;subnets&#34;: \[  
                    {  
                        &#34;name&#34;: &#34;proxysubnet&#34;,  
                        &#34;properties&#34;: {  
                            &#34;addressPrefix&#34;: &#34;192.168.188.0/24&#34;  
                        }  
                    },  
                    {  
                        &#34;name&#34;: &#34;secondSubnet&#34;,  
                        &#34;properties&#34;: {  
                            &#34;addressPrefix&#34;: &#34;192.168.180.0/24&#34;  
                        }  
                    }  
                \],  
</code></pre><p>Save the template and validate it again.</p>
<p><strong>Deploy the Template</strong></p>
<p>Now, let’s deploy the template with CLI.</p>
<pre tabindex="0"><code>az group deployment create \--resource\-group tomlab\-kr \--template\-file .\\template.json  
</code></pre><p>If all goes well, you should see the CLI returns the template in JSON.</p>
<p>Check in Azure portal, under the Resource Group –&gt; Deployments.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-RKnAZRXLQyc/W25j1IC_9qI/AAAAAAAAKMg/nT62XvIltZgYJONxmkp4RETkAonis0ijQCHMYCw/s1600-h/image%255B2%255D"><img src="https://lh3.googleusercontent.com/-_jKVwTdWNbk/W25j2IUIJZI/AAAAAAAAKMk/ZtmuISbs1WcSMW5Hh5D7p5AiqKo4CORFwCHMYCw/image_thumb?imgmax=800" alt="image"  title="image" /></a></p>
<p>Open Azure Portal and select <strong>Virtual networks</strong>, click <strong>Diagram.</strong> We can see the new subnet “secondSubnet” is now added. All existing resources are still intact.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-D_mBWuW-E-g/W25j240kNLI/AAAAAAAAKMo/nU5pFhTfVDc6ukEpVooCna1Ynk6nXuJugCHMYCw/s1600-h/Image14"><img src="https://lh3.googleusercontent.com/-o_eJbQCWydY/W25j4cy2zVI/AAAAAAAAKMs/2Ksmrd4j7u09CNvmnZczHyc9GPMhWwtqQCHMYCw/Image_thumb4%255B1%255D?imgmax=800" alt="Image"  title="Image" /></a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Monitor Office 365 Outages with Twitter]]></title>
            <link href="/2018/07/monitor-office-365-outages-with-twitter.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/07/monitor-office-365-outages-with-twitter.html</id>
            
            <published>2018-07-28T05:29:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Office 365 has high SLAs (<a class="gblog-markdown__link" href="http://www.microsoftvolumelicensing.com/Downloader.aspx?DocumentId=14033">the latest English version</a>) backed by Microsoft’s excellent Azure Cloud. However, like every other cloud services, there is always the chance for something unexpected to happen.</p>
<p>This year April Office 365 had a major hiccup to its service. Its Asia Pacific backend Azure AD authentication went haywire. As a result, users lost access all O365 services. To make it worse, the usual Office 365 monitoring channel: Office 365 dashboard was not accessible due to this fault.</p>
<p>To track the issue at the time, I ended up relying on the official MS Twitter account @Office365Status for updates. The account contains all Office 365 service outage notifications. This made it become the only source for people to track the issue at that time.</p>
<p>After the incident, it become obvious that the Twitter account is a pretty reliable source for monitoring Office 365 service status. Based on this, I developed a solution, it uses PowerShell script to check for new tweets from @Office365Status Twitter account. The scheduled script is executed from a AWS EC2 instance, this completely eliminates the dependence from Microsoft services.</p>
<p>Ok, let’s go through the script in details.</p>
<p>In order to allow our script to read tweets, we first need to create an app in <a class="gblog-markdown__link" href="https://apps.twitter.com">https://apps.twitter.com</a>. Please be aware the site will be consolidated into <a class="gblog-markdown__link" href="https://developer.twitter.com">https://developer.twitter.com</a> soon. And as of July 2018, you will need to apply for a Twitter developer account in order to create a new app.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-HMX0MrR4vyM/W1xhl1-KbaI/AAAAAAAAKIo/eYtJlzwhwRMcu855dDSodhxWdzq-4ytegCHMYCw/s1600-h/image%255B3%255D"><img src="https://lh3.googleusercontent.com/-uoIQj0dqUBQ/W1xhnQuSt9I/AAAAAAAAKIs/GdlEnFR79P4jYnGYlmyZ_mFeWNFSwsPAwCHMYCw/image_thumb%255B1%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>After created the app, under Permissions, set the permission as Read only, AS the monitoring script will have no need to write.</p>
<p><a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-7qbo8_iMsy0/W1xlp8_roII/AAAAAAAAKJI/0yxQF2JvjvYvF8z8yrzi_l3eu6LZK26_wCHMYCw/s1600-h/image%255B6%255D"><img src="https://lh3.googleusercontent.com/-D-DA5sebsmg/W1xlqxc3-7I/AAAAAAAAKJM/TQYiMVT2NvsXHY22zGPMNA2Bbs1H_qyywCHMYCw/image_thumb%255B2%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>Under Keys and Access Tokens, copy following key values to notepad. These values will be needed in the script to configure the access to Twitter.</p>
<ul>
<li>Consumer Key (API Key)</li>
<li>Consumer Secret (API Secret)</li>
<li>Access Token</li>
<li>Access Token Secret</li>
</ul>
<p>Now we just need a PowerShell module that uses Twitter API to pull out tweets. Among all the PowerShell modules I tested, MyTwitter, written by Adam Bertram is the only one works for me. You can find the module from <a class="gblog-markdown__link" href="https://gallery.technet.microsoft.com/scriptcenter/Tweet-and-send-Twitter-DMs-8c2d6f0a">here</a>.</p>
<p>First, we create a authenticated session with the generated app tokens.</p>
<pre tabindex="0"><code>New-MyTwitterConfiguration \-APIKey YourTwitterAppAPIKey \`  
\-APISecret YourTwitterAppAPISecret \`  
\-AccessToken YourTwitterAppAccessToken \`  
\-AccessTokenSecret YourTwitterAppAccessTokenSecret  
</code></pre><p>Next, we use Get-TweetTimeline to get recent tweets from @Office365Status account. Then we grab the first item from the array, which is the latest tweet from the account.</p>
<pre tabindex="0"><code>$TimeLine \= Get-TweetTimeline \-UserName &#34;office365status&#34; \-MaximumTweets 100   
$tweet \= $TimeLine\[0\]   
</code></pre><p>The script will then get the creation time of the latest tweet. Twitter uses a non standard time format. So first, we will need to convert the time to a conventional format. This will allow us to compare the creation time with the current time on the server.</p>
<p>Twitter original creation time format:</p>
<p> <a class="gblog-markdown__link--raw" href="https://lh3.googleusercontent.com/-d5C9qmNRzeY/W12X39ttQtI/AAAAAAAAKKM/66PB9LDr3A0ed0taIexstfyI0yewAhsjgCHMYCw/s1600-h/image%255B11%255D"><img src="https://lh3.googleusercontent.com/-ZDQBtBveKF0/W12X5ehaIuI/AAAAAAAAKKQ/OXF6CGtomH8e4gl3gVo4juRrbQ1TG9d2gCHMYCw/image_thumb%255B5%255D?imgmax=800" alt="image"  title="image" /></a></p>
<p>To convert the time we use a Convert-DateString function.</p>
<pre tabindex="0"><code>$createTime \= Convert-DateString \-Date $tweet.created\_at \`
</code></pre><p>-Format &lsquo;ddd MMM dd HH:mm:ss zzzz yyyy&rsquo;</p>
<pre tabindex="0"><code>
ddd = First 3 letter of weekday.

MMM = First 3 letter of month.

dd = day of the month

HH:mm:ss = hours:minutes:seconds

zzzz = time zone hours

yyyy = year

The function uses .Net method TryParseExact to convert the time format to something like below.

[![image](https://lh3.googleusercontent.com/-ijXZuR9Re_c/W12X7tgzLiI/AAAAAAAAKKY/EdOmprvEeQYekzQu7MI9aZTwT4pB5HhDACHMYCw/image_thumb%255B6%255D?imgmax=800 &#34;image&#34;)](https://lh3.googleusercontent.com/-12vIAsjFFQU/W12X6kad5eI/AAAAAAAAKKU/-GNbRmhKcTwrBma5Q4hpDLd3jBJpTfNkACHMYCw/s1600-h/image%255B14%255D)
</code></pre><p>function Convert-DateString ([String]$Date, [String[]]$Format)<br>
{<br>
$result = New-Object DateTime</p>
<p>$convertible = [DateTime]::TryParseExact(<br>
$Date,<br>
$Format,<br>
[System.Globalization.CultureInfo]::InvariantCulture,<br>
[System.Globalization.DateTimeStyles]::None,<br>
[ref]$result)</p>
<p>if ($convertible) { $result }<br>
}</p>
<pre tabindex="0"><code>
Once we have the correct creation time, my script will check if the tweet is a reply. This is to filter out those reply tweets. As we only care about the incident updates.

The script will then check if the tweet is created within last hour. Depends on the interval you schedule the script. You may set this to the frequent you desire. In my case, I schedule the script to check every hour, hence the 1 hour interval.

Upon confirm the tweet is posted within last hour and is not a reply tweet, the script will then send out an email to a monitoring mailbox (Not your Office 365 mailbox of course!).
</code></pre><p>if ($tweet.in_reply_to_user_id_str -like &ldquo;&rdquo;){<br>
# If the tweet is created within last hour, send out alert<br>
if ($createTime -gt (get-date).Addhours(-1)){</p>
<pre><code>    $body \= $tweet.text  
    $Emailsubject \= &quot;Microsoft Office365Status Twitter Account just post a new update&quot;  
    Send-MailMessage \-from &quot;tom.chen@contoso.com&quot; \-To &quot;it@contoso.com&quot; \-SmtpServer mailserver.contoso.com \-Subject $Emailsubject \-Body $body  
}else{  
    return  
}  
</code></pre>
<p>}</p>
<pre tabindex="0"><code>
The PowerShell monitoring script can be downloaded from my GitHub repo [here](https://github.com/tomkingchen/PowershellScripts/blob/master/MonitorO365Status_pub.ps1).

The last part of the solution is to upload the script to a server (EC2 in AWS in my case) and schedule it to run every hour!

Below is an Email I received for a recent incident. Hope you find this useful!

[![image](https://lh3.googleusercontent.com/-Qa6eKiG_uMw/W12X-Og0L2I/AAAAAAAAKKg/fGzwcS_QbEo4zhtzcKhKZteFUv3U-OYRgCHMYCw/image_thumb%255B7%255D?imgmax=800 &#34;image&#34;)](https://lh3.googleusercontent.com/-11H9StkvGaA/W12X84cg3ZI/AAAAAAAAKKc/GGfUZ6VM63kOJXtMy5jtMQO16Nzl4IIuwCHMYCw/s1600-h/image%255B17%255D)
</code></pre>]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="/tags/Office-365" term="Office-365" label="Office 365" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Over the Wall - with help from Azure]]></title>
            <link href="/2018/07/over-wall-with-help-from-azure.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/07/over-wall-with-help-from-azure.html</id>
            
            <published>2018-07-22T22:01:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>I had a long holiday in China recently. It had been a fun and eyes opening trip. Feels like the whole nation is in the middle of a “technology revolution”. Among all, Internet has become one of the core driving force. But while Wifi beomces a life essential here, the infamous Great Firewall (GFW) is also getting more effective. After tried numbers of paid and free VPN providers, I just could not find a reliable VPN service to simply let me post a picture on Facebook. I then decided it&rsquo;s time to bring the matter to my own hands. </p>
<p>My VPN solution is pretty simple. It consist of a Squid proxy server and Azure P2S VPN. Access the Squid proxy server through its public IP alone will not help. The GFW will still be able to detect traffic destinations and block access successfully. A VPN tunnel ensure the traffic is encapsulated and prevents the GFW to block any traffic.</p>
<p>It is worth mentioning that this solution does incur cost! Azure resources like VPN Gateway have ongoing cost. I, fortunately have access to a subscription with $200 credit. You can check the price details from <a class="gblog-markdown__link" href="https://azure.microsoft.com/en-gb/pricing/details/vpn-gateway/">here</a>.</p>
<p><strong>Setup Squid Proxy</strong></p>
<p>OK, lets get started! </p>
<p>So first, we need to build a Squid Proxy server in Azure. Below is the PowerShell script I use to provision the VM. It creates an Ubuntu Linux VM within a vNet. All resources are provisioned inside &ldquo;tomlab-kr&rdquo; resource group, which is located in Korea South region. The script also creates a Security Group which allows only SSH and TCP 7777 traffic. The custom TCP port will be used for Squid proxy service. </p>
<p># Create a resource group in KoreaSouth</p>
<p>New-AzureRmResourceGroup  -Name  &ldquo;tomlab-kr&rdquo;  -Location  &ldquo;Korea South&rdquo;</p>
<p># Create a subnet configuration</p>
<p>$subnetConfig  =  New-AzureRmVirtualNetworkSubnetConfig  -Name  &ldquo;proxysubnet1&rdquo; `<br>
-AddressPrefix  192.168.188.0/24</p>
<p># Create a virtual network</p>
<p>$vnet  =  New-AzureRmVirtualNetwork  -ResourceGroupName  &ldquo;tomlab-kr&rdquo; `<br>
-Location  &ldquo;Korea South&rdquo;-Name  &ldquo;p2sVNET1&rdquo;  -AddressPrefix  192.168.0.0/16 `<br>
-Subnet  $subnetConfig</p>
<p># Create a public IP address and specify a DNS name</p>
<p>$pip  =  New-AzureRmPublicIpAddress  -ResourceGroupName  &ldquo;tomlab-kr&rdquo; `<br>
-Location  &ldquo;Korea South&rdquo;-AllocationMethod  Static  -IdleTimeoutInMinutes  4 `</p>
<p>-Name  &ldquo;mypublicdns$(Get-Random)&rdquo;</p>
<p># Create an inbound network security group rule for port 22</p>
<p>$nsgRuleSSH  =  New-AzureRmNetworkSecurityRuleConfig  -Name  &ldquo;SGRuleSSH&rdquo; `<br>
-Protocol  &ldquo;Tcp&rdquo;-Direction  &ldquo;Inbound&rdquo;  -Priority  1000  -SourceAddressPrefix  * `</p>
<p>-SourcePortRange  *  -DestinationAddressPrefix  * -DestinationPortRange  22 `<br>
-Access  &ldquo;Allow&rdquo;</p>
<p># Create an inbound network security group rule for port 7777</p>
<p>$nsgRuleWeb  =  New-AzureRmNetworkSecurityRuleConfig  -Name  &ldquo;SGRuleWeb&rdquo; `<br>
-Protocol  &ldquo;Tcp&rdquo; -Direction  &ldquo;Inbound&rdquo;  -Priority  1001  -SourceAddressPrefix  * `</p>
<p>-SourcePortRange  *  -DestinationAddressPrefix  * -DestinationPortRange  7777 `<br>
-Access  &ldquo;Allow&rdquo;</p>
<p># Create a network security group</p>
<p>$nsg  =  New-AzureRmNetworkSecurityGroup  -ResourceGroupName  &ldquo;tomlab-kr&rdquo; `<br>
-Location  &ldquo;Korea South&rdquo;-Name  &ldquo;LinuxSG&rdquo;  -SecurityRules  $nsgRuleSSH,$nsgRuleWeb</p>
<p># Define the credential for the VM</p>
<p>$cred  =  Get-Credential  -message  &ldquo;login for the new vm&rdquo;  -UserName  &ldquo;tom&rdquo; </p>
<p># Create a virtual network card and associate with public IP address and NSG</p>
<p>$nic  =  New-AzureRmNetworkInterface  -Name  &ldquo;proxyVMNic&rdquo; `<br>
-ResourceGroupName  &ldquo;tomlab-kr&rdquo;-Location  &ldquo;Korea South&rdquo; `<br>
-SubnetId  $vnet.Subnets[0].Id -PublicIpAddressId  $pip.Id `<br>
-NetworkSecurityGroupId  $nsg.Id</p>
<p># Create a virtual machine configuration</p>
<p>$vmConfig  =  New-AzureRmVMConfig  -VMName  &ldquo;proxyvm1&rdquo;  -VMSize  &ldquo;Standard_DS1_v2&rdquo;  |  `</p>
<p>Set-AzureRmVMOperatingSystem  -Linux  -ComputerName  &ldquo;proxyvm1&rdquo;  -Credential  $cred  |  `</p>
<p>Set-AzureRmVMSourceImage  -PublisherName  &ldquo;Canonical&rdquo;  -Offer  &ldquo;UbuntuServer&rdquo; `<br>
-Skus  &ldquo;16.04-LTS&rdquo;-Version  &ldquo;latest&rdquo;  |Add-AzureRmVMNetworkInterface  -Id  $nic.Id</p>
<p># Create the VM</p>
<p>New-AzureRmVM  -ResourceGroupName  &ldquo;tomlab-kr&rdquo;  -Location  &ldquo;Korea South&rdquo;  -VM  $vmConfig</p>
<p>Once the VM is provisioned, the next step is to install and configure Squid on the Linux VM. </p>
<p>The installation of Squid proxy server is pretty straight forward on a Ubuntu server. We simply run the command the below.</p>
<p># sudo apt-get install squid</p>
<p>To configure Squid proxy, we need to modify the squid configuration file. </p>
<p>First, we make a backup copy of the configuration file, so in case we mess up, there is a clean copy to roll back to.</p>
<p># cp /etc/squid/squid.conf /etc/squid/squid.conf.bkcopy</p>
<p>Next, use your favorite text editor to modify the configuration file</p>
<p># sudo nano /etc/squid/squid.conf</p>
<p>Based on my tests, the default squid port 3128 is blocked by GFW. So the first thing to change is the default Squid port. In my case I changed it to 7777. </p>
<p>http_port 7777</p>
<p>Enable HTTP proxy access to all IPs.</p>
<p>http_access allow all</p>
<p>Add ACL to allow VPN and local VM subnet IPs. Those lines should be added in two different sections in the configuration file. Please refer to the squid configuration guidance for details line locations.</p>
<p><strong>vpnsubnet</strong> refers to the VPN Client Subnet, proxysubnet refers to the subnet where the Squid proxy server lives.</p>
<p>acl vpnsubnet 172.180.10.0/24</p>
<p>acl proxysubnet 192.168.188.0/24</p>
<p>http_access allow proxysubnet</p>
<p>http_access allow vpnsubnet</p>
<p>Now save the configuration file and restart squid service </p>
<p># sudo systemctl restart squid</p>
<p>You can test the proxy by creating another VM in the same proxyvmsubnet. </p>
<p><strong>Setup P2SVPN</strong></p>
<p>The steps to setup Azure Point-to-Site VPN is quite straight forward through Azure portal. Again, I will show you how this can be achieved through PowerShell script.</p>
<p>First, we need to create a Gateway Subnet within the vNet we created for the Squid proxy. The PowerShell script below simply created a subnet named &ldquo;GatewaySubnet&rdquo;. This is a reserved name for the &ldquo;Gateway Subnet&rdquo;.</p>
<p>$vNet  =  Get-AzureRmVirtualNetwork  -ResourceGroupName  &ldquo;tomlab-kr&rdquo;  -Name  &ldquo;p2sVNET1&rdquo;</p>
<p>Add-AzureRmVirtualNetworkSubnetConfig  -VirtualNetwork  $vNet  -Name  &ldquo;GatewaySubnet&rdquo; `<br>
-AddressPrefix  &ldquo;192.168.200.0/24&rdquo;</p>
<p>Set-AzureRmVirtualNetwork  -VirtualNetwork  $vNet</p>
<p>Then we just need to provision a VPN Gateway within the subnet. </p>
<p>Note: The last command &ldquo;New-AzureRmVirtualNetworkGateway&rdquo; may fail with unexpected errors. This is likely due to the Gateway Subnet actually takes a few hours to be properly configured in Azure. The solution is simply to try the command again after a few hours&hellip; </p>
<p># Get gatewaysubnet</p>
<p>$subnet  =  Get-AzureRmVirtualNetworkSubnetConfig  -Name  &ldquo;GatewaySubnet&rdquo;  -VirtualNetwork  $vNet</p>
<p>$pip  =  New-AzureRmPublicIpAddress  -Name  &ldquo;gwpip1&rdquo;  -ResourceGroupName  &ldquo;tomlab-kr&rdquo; `<br>
-Location  &ldquo;korea south&rdquo;  -AllocationMethod  Dynamic</p>
<p>$ipconf  =  New-AzureRmVirtualNetworkGatewayIpConfig  -Name  &ldquo;p2sgwIPconf&rdquo;  -Subnet  $subnet `<br>
-PublicIpAddress  $pip</p>
<p># Create VPN Gateway, remeber to wait for sunbnet ID to be generated<br>
# (may take a few hours, use $subnet.id to check)</p>
<p>New-AzureRmVirtualNetworkGateway  -Name  &ldquo;p2sgw&rdquo;  -ResourceGroupName  &ldquo;tomlab-kr&rdquo; `<br>
-Location  &ldquo;korea south&rdquo;  -IpConfigurations  $ipconf  -GatewayType  VPN `<br>
-VpnType  RouteBased  -GatewaySku  VpnGw1</p>
<p>After provision the VPN Gateway, the next step is to configure the <strong>Client Address Space.</strong> This is the IP range reserved for VPN clients. You may notice this is the IP range we defined in the Squid proxy configuration file.</p>
<p># configure the client address space with the VPN gateway</p>
<p>$Gateway  =  Get-AzureRmVirtualNetworkGateway  -ResourceGroupName  $RG  -Name  $GWName</p>
<p>Set-AzureRmVirtualNetworkGateway  -VirtualNetworkGateway  $Gateway `<br>
-VpnClientAddressPool  &ldquo;172.180.10.0/24&rdquo;</p>
<p>Next, we need to generate a pair of Root and Client certificates for the VPN Gateway authentication. </p>
<p># Generate New Self Singed Certificate</p>
<p>$cert  =  New-SelfSignedCertificate  -Type  Custom  -KeySpec  Signature  `</p>
<p>-Subject  &ldquo;CN=P2SRootCert&rdquo;  -KeyExportPolicy  Exportable  `</p>
<p>-HashAlgorithm  sha256  -KeyLength  2048  `</p>
<p>-CertStoreLocation  &ldquo;Cert:CurrentUserMy&rdquo;  -KeyUsageProperty  Sign  -KeyUsage  CertSign</p>
<p># Generate Client CCertificate</p>
<p>New-SelfSignedCertificate  -Type  Custom  -DnsName  P2SChildCert  -KeySpec  Signature  `</p>
<p>-Subject  &ldquo;CN=P2SChildCert&rdquo;  -KeyExportPolicy  Exportable  `</p>
<p>-HashAlgorithm  sha256  -KeyLength  2048  `</p>
<p>-CertStoreLocation  &ldquo;Cert:CurrentUserMy&rdquo;  `</p>
<p>-Signer  $cert  -TextExtension  @(&ldquo;2.5.29.37={text}1.3.6.1.5.5.7.3.2&rdquo;)</p>
<p>The script generates both the Root and Client certificates under CurrentUserMy folder inside your certificate repo. </p>
<p>Export the Root and Client certificates. The Root certificate needs to be exported without password as a <strong>Base-64 encoded X.509 cer</strong> file. The Client certificate needs to be exported with password in <strong>PKCS #12 (.pfx)</strong> format.</p>
<p>Run the PowerShell commands below to configure the VPN gateway with the Root certificate </p>
<p># configure VPN Gateway with the root certificate</p>
<p>$P2SRootCertName  =  &ldquo;P2SRootCert.cer&rdquo;</p>
<p>$filePathForCert  =  &ldquo;C:Downloads ootcert2.cer&rdquo;</p>
<p>$cert  =  new-object  System.Security.Cryptography.X509Certificates.X509Certificate2($filePathForCert)</p>
<p>$CertBase64_3  =  [system.convert]::ToBase64String($cert.RawData)</p>
<p>$p2srootcert  =  New-AzureRmVpnClientRootCertificate  -Name  $P2SRootCertName  -PublicCertData  $CertBase64_3</p>
<p># upload the public key information to Azure</p>
<p>Add-AzureRmVpnClientRootCertificate  -VpnClientRootCertificateName  $P2SRootCertName  `</p>
<p>-VirtualNetworkGatewayName  &ldquo;p2sgw&rdquo;  -ResourceGroupName  &ldquo;tomlab-kr&rdquo;  `</p>
<p>-PublicCertData  $CertBase64_3</p>
<p>The solution is now ready for testing.</p>
<p><strong>Connect from PC</strong></p>
<p>Setup P2S VPN client on PC is pretty easy. After download the VPN Client from Azure Portal, simply follow the prompts to install the client.</p>
<p>You may get a security warning from Windows 10. This can be simply ignored as you know this is a trusted app from Microsoft. Though it may look like the only option here is &ldquo;<strong>Don&rsquo;t run</strong>&rdquo;<strong>,</strong> as soon as you click <strong>More Info,</strong> you will be given the option to bypass the prompt.</p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-JrIGsAVrrJY/W1Veno2vCAI/AAAAAAAAKHI/0a1WiXKLGGsSKjkKVmkjJB0Ox7lMOPdWQCLcBGAs/s1600/protectWarn1.png"><img src="https://4.bp.blogspot.com/-JrIGsAVrrJY/W1Veno2vCAI/AAAAAAAAKHI/0a1WiXKLGGsSKjkKVmkjJB0Ox7lMOPdWQCLcBGAs/s320/protectWarn1.png" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-JCCZjkaJ0ps/W1Ve3HRoZ5I/AAAAAAAAKHM/VxJ6BgxW8hc86hPWHG7rs2sOf_-3x1nVACLcBGAs/s1600/protectwarn2.png"><img src="https://1.bp.blogspot.com/-JCCZjkaJ0ps/W1Ve3HRoZ5I/AAAAAAAAKHM/VxJ6BgxW8hc86hPWHG7rs2sOf_-3x1nVACLcBGAs/s320/protectwarn2.png" alt=""  /></a></p>
<p>Once the client is installed, don&rsquo;t forget to change PC&rsquo;s proxy to the private IP of the Squid proxy server.</p>
<p><strong>Connect from iPhone</strong></p>
<p>By now you probably already jumped on Facebook posted 10 pictures and read 20 Emails from your Gmail inbox. But transferring photos from your phone to PC is still bit painful. You still miss the joy of sending those nice posts with a few clicks from your phone. OK, let&rsquo;s make that happen.</p>
<p>Unlike PC, there is no app to be installed on iOS. Instead, we need to install the client certificate we created for the VPN connection onto our iPhone. </p>
<p>First, upload the .pfx file to OneDrive (get one, it&rsquo;s free and can be accessed in China). </p>
<p>Next, install and open OneDrive app from your phone. Select the pfx file and choose Share. You should now see the &ldquo;Copy Link&rdquo; option. Click it to copy the link.</p>
<p>Open the URL with Safari (Not Chrome) from your phone. Safari will recognize it is a certificate and ask you if allow it to be installed. Click Allow to proceed. </p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-EcQJLjgAtls/W1Vfum4X2ZI/AAAAAAAAKHY/S0M6GEurlfgFu5kvvKkZjCipltbUwl_rACLcBGAs/s1600/allowopen.png"><img src="https://3.bp.blogspot.com/-EcQJLjgAtls/W1Vfum4X2ZI/AAAAAAAAKHY/S0M6GEurlfgFu5kvvKkZjCipltbUwl_rACLcBGAs/s320/allowopen.png" alt=""  /></a></p>
<p>Follow the prompts to install.</p>
<p><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-bLSHMr58DFA/W1Vf6meBBEI/AAAAAAAAKHc/Nf10nNDnJYA28iQofaI_BC78oFr5hXp4ACLcBGAs/s1600/installprofile.png"><img src="https://4.bp.blogspot.com/-bLSHMr58DFA/W1Vf6meBBEI/AAAAAAAAKHc/Nf10nNDnJYA28iQofaI_BC78oFr5hXp4ACLcBGAs/s320/installprofile.png" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-roDmZm1ICUo/W1VgaQ6dPLI/AAAAAAAAKHw/nG_IhXa2dEE2Y_2sZD0YfMaJOxTawM35QCEwYBhgL/s1600/pass.png"><img src="https://4.bp.blogspot.com/-roDmZm1ICUo/W1VgaQ6dPLI/AAAAAAAAKHw/nG_IhXa2dEE2Y_2sZD0YfMaJOxTawM35QCEwYBhgL/s320/pass.png" alt=""  /></a></p>
<p>After install the certificate, next step is to add a VPN configuration. </p>
<p>The Server and Remote ID can be obtained from the XML file <strong>vpnSettings.xml</strong>downloaded alone with the VPN Client.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-QIVWMkxonSQ/W1Vg7YzxONI/AAAAAAAAKH8/NOXE9z8eV3wgx_D0rhppLqE_vcOoOjYXwCLcBGAs/s1600/vpnconfig.png"><img src="https://3.bp.blogspot.com/-QIVWMkxonSQ/W1Vg7YzxONI/AAAAAAAAKH8/NOXE9z8eV3wgx_D0rhppLqE_vcOoOjYXwCLcBGAs/s320/vpnconfig.png" alt=""  /></a></p>
<p>The Proxy settings within the VPN configuration somehow does not work. The only way is to add Proxy setting to local WiFi connection. </p>
<p><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link" href="https://www.blogger.com/blogger.g?blogID=3427010633633716171"></a><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-cFOie6qWtJs/W1VhF9p7sHI/AAAAAAAAKIA/1DdLANwEnPQC53NhiQq3KkJRopAAGoY0wCLcBGAs/s1600/wifiproxy.png"><img src="https://4.bp.blogspot.com/-cFOie6qWtJs/W1VhF9p7sHI/AAAAAAAAKIA/1DdLANwEnPQC53NhiQq3KkJRopAAGoY0wCLcBGAs/s320/wifiproxy.png" alt=""  /></a></p>
<p>After completed all these settings, fire up the VPN connection from your phone and you should now be able to access Facebook, Google, Instagram!</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="/tags/PowerShell" term="PowerShell" label="PowerShell" />
                             
                                <category scheme="/tags/VPN" term="VPN" label="VPN" />
                             
                                <category scheme="/tags/Azure" term="Azure" label="Azure" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup AWS SSO CLI & API Access]]></title>
            <link href="/2018/06/setup-aws-sso-cli-api-access.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/06/setup-aws-sso-cli-api-access.html</id>
            
            <published>2018-06-25T17:24:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>In my last article, I discussed the steps to setup AWS SSO  through Azure AD. By using Azure AD app roles, we are able to use our Azure AD accounts to access AWS Console. But with this measure, you will find there is no option in AWS IAM to generate Access Key and Secrete for CLI and API access.</p>
<p>Fortunately, we are not the only ones out there have this problem. David T Johnson faced the same issue and he is kind and smart enough (unlike me) to create a tool to address this issue. The tool source code can be found on Github <a class="gblog-markdown__link" href="https://github.com/dtjohnson/aws-azure-login">https://github.com/dtjohnson/aws-azure-login</a></p>
<p>The tool is written in Node.Js. So if you don&rsquo;t have Node, the first thing will be to install Node from <a class="gblog-markdown__link" href="https://nodejs.org/en/">https://nodejs.org/en/</a>. After that simply follow the installation instructions to get the tool going. </p>
<p>The example show here is tested from Windows 10. </p>
<p>Before start, log into Azure portal to get Azure Tenant ID and the AWS SSO App ID.</p>
<p>The tenant ID can be found in Properties section of Azure AD. The Directory ID is basically your Tenant ID.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-W_iJiXwELG0/WzGFgUnr_RI/AAAAAAAAKFU/0fpRAs8TCY4BaN19Fr7jnOzASs1HAfNDwCLcBGAs/s1600/directoryid.png"><img src="https://2.bp.blogspot.com/-W_iJiXwELG0/WzGFgUnr_RI/AAAAAAAAKFU/0fpRAs8TCY4BaN19Fr7jnOzASs1HAfNDwCLcBGAs/s400/directoryid.png" alt=""  /></a></p>
<p>In Azure AD, go to App Registrations and select the AWS SSO app, if you don&rsquo;t see the AWS SSO app, change the scope to &ldquo;All Apps&rdquo;.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-JbWxtW2J7PE/WzGFyk0jauI/AAAAAAAAKFc/JqLnsXqi308eDmJpyEfVSqBlTRk-bXEUACLcBGAs/s1600/awsapp.png"><img src="https://3.bp.blogspot.com/-JbWxtW2J7PE/WzGFyk0jauI/AAAAAAAAKFc/JqLnsXqi308eDmJpyEfVSqBlTRk-bXEUACLcBGAs/s400/awsapp.png" alt=""  /></a></p>
<p>Click Settings -&gt; Properties, copy the App ID URI.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-dVFKPkQ9OpU/WzGGAnH35qI/AAAAAAAAKFg/bDp4PazR-W8Bk0EUQ9V8JLe3iIC8NgFowCLcBGAs/s1600/appid.png"><img src="https://2.bp.blogspot.com/-dVFKPkQ9OpU/WzGGAnH35qI/AAAAAAAAKFg/bDp4PazR-W8Bk0EUQ9V8JLe3iIC8NgFowCLcBGAs/s320/appid.png" alt=""  /></a></p>
<p>In the Properties section, copy the App ID URI.</p>
<p>Open Node.js command prompt</p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/--44256V9T_s/WzGGQdv9UdI/AAAAAAAAKFo/zWtlSf-eDbwt1oKZSB7gZ1TeCqfoCTGIACLcBGAs/s1600/nodecmd.png"><img src="https://4.bp.blogspot.com/--44256V9T_s/WzGGQdv9UdI/AAAAAAAAKFo/zWtlSf-eDbwt1oKZSB7gZ1TeCqfoCTGIACLcBGAs/s320/nodecmd.png" alt=""  /></a></p>
<p>Setup the authentication profile</p>
<p> </p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-9YQ4rtDXorE/WzGGeD8cBmI/AAAAAAAAKFw/eHsrFV7g6Rc-7PyDb2Ga8VjJi_S2xhg-wCLcBGAs/s1600/authprofile.png"><img src="https://3.bp.blogspot.com/-9YQ4rtDXorE/WzGGeD8cBmI/AAAAAAAAKFw/eHsrFV7g6Rc-7PyDb2Ga8VjJi_S2xhg-wCLcBGAs/s400/authprofile.png" alt=""  /></a></p>
<p> Sign in with the profile, select the role in each accounts. It will be a lot easier to tell which account the roles belong to, if you named your roles with the AWS Account name. E.g. ContosoDev-Admins-SAML -AzureAD.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-eldJHICpoWw/WzGGpj3l5PI/AAAAAAAAKF4/tpDVEWvo9VMeNpqT6_bIcOgik50wOd0RwCLcBGAs/s1600/samlsignin.png"><img src="https://2.bp.blogspot.com/-eldJHICpoWw/WzGGpj3l5PI/AAAAAAAAKF4/tpDVEWvo9VMeNpqT6_bIcOgik50wOd0RwCLcBGAs/s400/samlsignin.png" alt=""  /></a></p>
<p>The session duration needs to be changed in your AWS role definition first. By default it is set to 1 hour only. So if you keep getting errors when setting duration longer than 1 hour, please follow this article to change the API duration. <a class="gblog-markdown__link" href="https://aws.amazon.com/blogs/security/enable-federated-api-access-to-your-aws-resources-for-up-to-12-hours-using-iam-roles/">https://aws.amazon.com/blogs/security/enable-federated-api-access-to-your-aws-resources-for-up-to-12-hours-using-iam-roles/</a></p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/--V5L0ImASlM/WzGG22yIPRI/AAAAAAAAKGA/eo_XHYW-MF03vetk5F9lqjMnSY8Dnbr5gCLcBGAs/s1600/iamroles.png"><img src="https://4.bp.blogspot.com/--V5L0ImASlM/WzGG22yIPRI/AAAAAAAAKGA/eo_XHYW-MF03vetk5F9lqjMnSY8Dnbr5gCLcBGAs/s400/iamroles.png" alt=""  /></a></p>
<p>After finish the sign-in process, go to .aws subfolder and you should see a credentials file there. The file contains all the existing profiles and the STS keys and tokens. </p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-YeOtTolJGgU/WzGHCU4oXvI/AAAAAAAAKGI/w9-6GcJGVn4UUtc-MyRPxx3g7QxaRqP6ACLcBGAs/s1600/keys.png"><img src="https://3.bp.blogspot.com/-YeOtTolJGgU/WzGHCU4oXvI/AAAAAAAAKGI/w9-6GcJGVn4UUtc-MyRPxx3g7QxaRqP6ACLcBGAs/s400/keys.png" alt=""  /></a></p>
<p>You can then use the keys to access AWS resources via CLI and API calls. Below is an example from AWS Toolkit for Visual Studio.</p>
<p>AWS Toolkit for Visual Studio can be downloaded from <a class="gblog-markdown__link" href="https://aws.amazon.com/visualstudio/">https://aws.amazon.com/visualstudio/</a></p>
<p>Create a new AWS template project in Visual Studio, and open AWS Explorer. Choose &ldquo;contosoaws&rdquo; as the profile.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-3owxaDalaOs/WzGHWDCZLdI/AAAAAAAAKGQ/ALZ3aK49s1YELN5s4gTO6yrl9yrA4-w-wCLcBGAs/s1600/awsexpl.png"><img src="https://3.bp.blogspot.com/-3owxaDalaOs/WzGHWDCZLdI/AAAAAAAAKGQ/ALZ3aK49s1YELN5s4gTO6yrl9yrA4-w-wCLcBGAs/s400/awsexpl.png" alt=""  /></a></p>
<p>With the STS credential, you should see all the instances in the region.</p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-qVmikO_JliY/WzGHtC2p6qI/AAAAAAAAKGc/PXbLtOMZnvUFhpAjkNqy5O-X_fgzB8jxgCLcBGAs/s1600/instances.png"><img src="https://1.bp.blogspot.com/-qVmikO_JliY/WzGHtC2p6qI/AAAAAAAAKGc/PXbLtOMZnvUFhpAjkNqy5O-X_fgzB8jxgCLcBGAs/s400/instances.png" alt=""  /></a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Setup SSO Access to AWS Console with Azure AD]]></title>
            <link href="/2018/06/setup-sso-access-to-aws-console-with.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/06/setup-sso-access-to-aws-console-with.html</id>
            
            <published>2018-06-21T23:05:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>As organization acquires more AWS accounts, it becomes quite a challenge for IT to manage the access to all those accounts. Instead of dealing with individual IAM accounts across multiple accounts. We need an identity solution to simplify the user access provision and removal process.</p>
<p>AWS itself offers a service called AWS SSO, which allows integrate AWS access with on premise AD through SAML. However, the service does incur charges and will require provision of an AD Connect appliance in AWS, if you don&rsquo;t already have ADFS in place(Yes, it has the same name as Azure AD Connect).</p>
<p>Instead of AWS SSO, in this article I will talk about how you can setup Azure AD as the sole IDP for all your AWS accounts through SAML. Azure AD as the core of Office 365 services is widely used across businesses these days. In a lot cases, organization has their on premise AD forests synced with Azure AD. This allows you to manage AWS access from on premise tools like ADUC. It also means you can take advantages like MFA from Office 365.</p>
<p>The solution requires NO additional infrastructure from either AWS or Azure, and comes with zero cost.</p>
<p>Microsoft has an <a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/azure/active-directory/active-directory-saas-amazon-web-service-tutorial">official guide</a> to show how to setup SSO between Azure AD and AWS. However, the configuration does require store the AWS root credential in Azure. The solution described below utilizes &ldquo;App Roles&rdquo; available in Azure App to define AWS access roles, therefore save the need of using the root account. </p>
<p>To start, we setup Enterprise App in Azure AD.</p>
<ul>
<li>
<p>Sign in to your Azure AD tenant as Global Administrator</p>
</li>
<li>
<p>Click on Azure AD → Enterprise applications → All applications → New Application → All</p>
</li>
<li>
<p>In the text box enter “AWS” and you should see 2 applications</p>
</li>
<li>
<p>Select the one with the black icon “Amazon Web Services (AWS) - Developer services”</p>
</li>
<li>
<p>Change the name as &ldquo;AWS SSO&rdquo;, as it will display to end users, then click Add</p>
</li>
<li>
<p>Click on Single sign-on and select SAML-based Sign-on</p>
</li>
<li>
<p>Check the checkbox View and edit all other user attributes</p>
</li>
<li>
<p>Click on Add attribute and add the following</p>
</li>
</ul>
<p>Name</p>
<p>Value</p>
<p><a class="gblog-markdown__link" href="https://aws.amazon.com/SAML/Attributes/RoleSessionName">https://aws.amazon.com/SAML/Attributes/RoleSessionName</a></p>
<p>user.userprincipalname</p>
<p><a class="gblog-markdown__link" href="https://aws.amazon.com/SAML/Attributes/Role">https://aws.amazon.com/SAML/Attributes/Role</a></p>
<p>user.assignedroles</p>
<ul>
<li>
<p>Download the metadata XML file </p>
</li>
<li>
<p>Click on Save</p>
</li>
</ul>
<p>Next we need to setup Identity Provider in AWS.</p>
<ul>
<li>
<p>Login to the AWS Console and click on IAM → Identity Providers → Create Providers</p>
</li>
<li>
<p>Select SAML as Provider Type</p>
</li>
<li>
<p>Enter &ldquo;Contoso AzureAD&rdquo; as Provider Name</p>
</li>
<li>
<p>Upload the Metadata XML file downloaded previously</p>
</li>
<li>
<p>Click on Next → Create</p>
</li>
<li>
<p>Copy the ARN of Identity Provider to Notepad for later use</p>
</li>
</ul>
<p>Now we need to setup IAM Roles to define the access.</p>
<ul>
<li>Click on Roles → Create new role → Select SAML 2.0 federation as the trusted entity type</li>
</ul>
<p>         </p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-Kgotfam37oE/WyyRT4hR_TI/AAAAAAAAKEw/0YEtTrb4WosbX_OblT3nAtGYzBYkMeEcQCLcBGAs/s1600/saml.png"><img src="https://4.bp.blogspot.com/-Kgotfam37oE/WyyRT4hR_TI/AAAAAAAAKEw/0YEtTrb4WosbX_OblT3nAtGYzBYkMeEcQCLcBGAs/s320/saml.png" alt=""  /></a></p>
<ul>
<li>
<p>Select &ldquo;Contoso AzureAD&rdquo; as SAML Provider</p>
</li>
<li>
<p>Click on Next Step</p>
</li>
<li>
<p>Select AdministratorAccess as policy</p>
</li>
<li>
<p>Enter &ldquo;ContosoDev-Admins-SAML-AzureAD&rdquo; as Role name. </p>
</li>
<li>
<p>Copy the Role ARN to Notepad</p>
</li>
</ul>
<p>Back in Azure AD, we now need to configure App Registrations.</p>
<ul>
<li>
<p>Click on Azure AD → App Registrations</p>
</li>
<li>
<p>Select the &ldquo;AWS SSO&rdquo; application created. If you cannot find the app, make sure the search scope is set to &ldquo;All Apps&rdquo; instead of &ldquo;My Apps&rdquo;.</p>
</li>
<li>
<p>Click the edit Manifest icon</p>
</li>
<li>
<p>Make sure you download a copy of the original Manifest file before carry out any changes</p>
</li>
<li>
<p>Edit the manifest by adding the following code to the appRoles array. The id has to be a unique number, which you can copy from an existing AppRole and modify it. Make sure &ldquo;isEnabled&rdquo; is &ldquo;true&rdquo;, otherwise you won&rsquo;t be able to assign the role. For &ldquo;value&rdquo;, simply copy the Role ARN and Identity Provider ARN from your Notepad.</p>
</li>
</ul>
<p>            {&ldquo;allowedMemberTypes&rdquo;: [</p>
<p>                &ldquo;User&rdquo;</p>
<p>                ],</p>
<p>            &ldquo;displayName&rdquo;: &ldquo;Contoso Dev AWS Admins&rdquo;,</p>
<p>            &ldquo;id&rdquo;: &ldquo;7abcd756e-8c27-4472-b2b7-38c17ab51234&rdquo;,</p>
<p>            &ldquo;isEnabled&rdquo;: true,</p>
<p>            &ldquo;description&rdquo;: &ldquo;ContosoDev-Admins-SAML-AzureAD&rdquo;,</p>
<p>            &ldquo;value&rdquo;: &ldquo;arn:aws:iam::591818111111:role/ContosoDev-Admins-SAML-AzureAD,arn:aws:iam::591818111111:saml-provider/AzureAD&rdquo;</p>
<p>            },</p>
<ul>
<li>Click Save to validate and save the file. If the validation failed, review your JSON format. You can use the downloaded copy as your recover point.</li>
</ul>
<p>Now you are ready to assign AD groups to the AWS App.</p>
<ul>
<li>
<p>Click on Azure AD → Enterprise applications → All applications</p>
</li>
<li>
<p>Select your application</p>
</li>
<li>
<p>Click on Users and Groups → Add user</p>
</li>
<li>
<p>Click on Users and groups</p>
</li>
<li>
<p>Select the AD group you created for AWS access, click Select</p>
</li>
<li>
<p>Click on Select Role</p>
</li>
<li>
<p>Select &ldquo;Contoso Dev AWS Admin&rdquo;, click Select, click Assign</p>
</li>
<li>
<p>If you don&rsquo;t see the role here, go back to App Registration manifest file for AWS SSO and make sure &ldquo;isEnabled&rdquo; is set to true.</p>
</li>
</ul>
<p>You can repeat above steps to setup SSO for additional roles and AWS accounts. </p>
<p>Now if you go to <a class="gblog-markdown__link" href="https://myapps.microsoft.com/">https://myapps.microsoft.com</a> and login as the group member of the AWS AD group, you should be able to see the AWS SSO app in the portal. Upon clicking the app, you will be redirected to the AWS Account if you only have one role and one account setup. Otherwise, you will see the options to choose between different roles under all your AWS accounts.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-s-A0FKsnhhc/WyyRjPjZZ2I/AAAAAAAAKE0/kn2ygY1OKpwJptvqMCVMNxqnpny02qBCgCLcBGAs/s1600/SSORoles.png"><img src="https://3.bp.blogspot.com/-s-A0FKsnhhc/WyyRjPjZZ2I/AAAAAAAAKE0/kn2ygY1OKpwJptvqMCVMNxqnpny02qBCgCLcBGAs/s320/SSORoles.png" alt=""  /></a></p>
<p>That&rsquo;s it! You have now setup AWS SSO across multiple accounts with your Azure AD identities.</p>
<p>But, hang on, there is one more thing&hellip; What about CLI and API access. With the default SSO setup, there is no option in AWS Console to generate Access Keys and Secrets. In the next blog, I will explain how we can get the necessary tokens through AWS SSO.</p>
<p>References:</p>
<p><a class="gblog-markdown__link" href="https://docs.microsoft.com/en-us/azure/active-directory/active-directory-saas-amazon-web-service-tutorial">https://docs.microsoft.com/en-us/azure/active-directory/active-directory-saas-amazon-web-service-tutorial</a></p>
<p><a class="gblog-markdown__link" href="http://blog.flux7.com/aws-best-practice-azure-ad-saml-authentication-configuration-for-aws-console">http://blog.flux7.com/aws-best-practice-azure-ad-saml-authentication-configuration-for-aws-console</a></p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Migrate Azure AD Connect Between AD Forests]]></title>
            <link href="/2018/05/migrate-azure-ad-connect-between-ad.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2018/05/migrate-azure-ad-connect-between-ad.html</id>
            
            <published>2018-05-10T04:22:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Migrate Azure AD Connect Between AD Forests</p>
<p>I was recently involved in an AD forest migration project for one of our customers. As part of the requirements, we need to move the existing AD Connect server to a newly created AD forest. While the process itself is pretty straight forward, I do notice there aren&rsquo;t many online resource out there detail the whole process. So to make things even easier for the folks out there, I will share the steps I took to complete this AD Connect migration.</p>
<p>The details shown here are from my sandpit environment. </p>
<p>The existing/source AD forest: <a class="gblog-markdown__link" href="http://old.contoso.com/">old.contoso.com</a></p>
<p>The existing AD DC/AD Connect server: old-dc1.old.contoso.com</p>
<p>The new/target AD forest: <a class="gblog-markdown__link" href="http://new.contoso.com/">new.contoso.com</a></p>
<p>The new AD DC/AD Connect server: <a class="gblog-markdown__link" href="http://new-dc1.new.contoso.com/">new-dc1.new.contoso.com</a></p>
<p>UPN Suffix: <a class="gblog-markdown__link" href="http://briwave.com/">briwave.com</a></p>
<p>Here&rsquo;s a simple diagram to help you visualize the whole setup.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-MBe5BCiGRP4/WvQqOqpv_LI/AAAAAAAAKB0/i2IyGMQf6N8wEN7USlo3Vz4OAM4MEuE3QCLcBGAs/s1600/adconnectmigdiagrm.png"><img src="https://2.bp.blogspot.com/-MBe5BCiGRP4/WvQqOqpv_LI/AAAAAAAAKB0/i2IyGMQf6N8wEN7USlo3Vz4OAM4MEuE3QCLcBGAs/s400/adconnectmigdiagrm.png" alt=""  /></a></p>
<p><strong>Add New AD Forest to Existing AD Connect</strong></p>
<p>First, we need to add the new forest <a class="gblog-markdown__link" href="http://new.contoso.com/">new.contoso.com</a>into the existing AD Connect sync scope.</p>
<p>Below are the steps to do so:</p>
<p>1. On old-dc1.old.contoso.com, open AD Connect. Under the Task, choose Customize synchronization options.</p>
<p>2. Enter the Azure AD Global Admin credential to connect.</p>
<p>3. Add the new AD forests into AD Connect.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-9Ovfmq16xeo/WvQq0jrRY3I/AAAAAAAAKB8/MOtA2jXY0bkB59KNZtEZUw7_qUrYBnqwgCLcBGAs/s1600/confirmnewad.png"><img src="https://2.bp.blogspot.com/-9Ovfmq16xeo/WvQq0jrRY3I/AAAAAAAAKB8/MOtA2jXY0bkB59KNZtEZUw7_qUrYBnqwgCLcBGAs/s400/confirmnewad.png" alt=""  /></a></p>
<p>4. Confirm the UPN Suffix is verified.</p>
<p>5. Record the OU Filtering settings for the forest. These settings will need to be applied to the new AD Connect later.</p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-et7Ze8mlOcA/WvQrI_fj-lI/AAAAAAAAKCE/MwzjfYOf17gDDvyiVOhi8MAjjTfiQsu0ACLcBGAs/s1600/OUfiltering_old.png"><img src="https://4.bp.blogspot.com/-et7Ze8mlOcA/WvQrI_fj-lI/AAAAAAAAKCE/MwzjfYOf17gDDvyiVOhi8MAjjTfiQsu0ACLcBGAs/s400/OUfiltering_old.png" alt=""  /></a></p>
<p>6. Record the settings here as well for later configuration on new-dc1.new.contoso.com.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-IGhcRbpOvzs/WvQrJAl32bI/AAAAAAAAKCI/SlBdFUVaFKYPTrrT0ATQtY9CeshJ4PwfQCEwYBhgL/s1600/option_old.png"><img src="https://2.bp.blogspot.com/-IGhcRbpOvzs/WvQrJAl32bI/AAAAAAAAKCI/SlBdFUVaFKYPTrrT0ATQtY9CeshJ4PwfQCEwYBhgL/s400/option_old.png" alt=""  /></a></p>
<p>7. Click Configure to apply the changes.<br>
8. Click Exist to close.</p>
<p>To verify the configuration, open Synchronization Service on OLD-DC1 after run start-ADSyncSyncCycle and you should be able to see bunch of actions for <a class="gblog-markdown__link" href="http://new.contoso.com/">new.contoso.com</a>forest. If you check Azure Portal, you should also be able to see the newly provisioned users and groups from <a class="gblog-markdown__link" href="http://new.contoso.com/">new.contoso.com</a>AD.</p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-MAtvQdL0nq4/WvQrg_kAtrI/AAAAAAAAKCU/K4LWNXgtJYwMIZibaIaQBoXjZSUnhwRqgCLcBGAs/s1600/syncresult_old.png"><img src="https://4.bp.blogspot.com/-MAtvQdL0nq4/WvQrg_kAtrI/AAAAAAAAKCU/K4LWNXgtJYwMIZibaIaQBoXjZSUnhwRqgCLcBGAs/s400/syncresult_old.png" alt=""  /></a></p>
<hr>
<p><strong>Install AD Connect in Target AD Forest</strong></p>
<p>The next step is to install and configure AD Connect in the new forest.</p>
<p>1. After download the AD Connect onto new-dc1.new.contoso.com, choose Custom install. </p>
<p>2. Click <strong>Install</strong> to proceed.</p>
<p>3. Depend on the sign on method your organization use, you will need to choose the one accordingly. For my lab, I simply use the default password hash sync.</p>
<p>4. Put in Azure AD Global admin account to connect.</p>
<p>5. Add both forests to the scope.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-L76X_H6cKnY/Wvq_hiXc7OI/AAAAAAAAKCw/p2uTlDEXIX4KzEt9eN_OmGZAHmGdZpsCQCLcBGAs/s1600/syncbothad.png"><img src="https://2.bp.blogspot.com/-L76X_H6cKnY/Wvq_hiXc7OI/AAAAAAAAKCw/p2uTlDEXIX4KzEt9eN_OmGZAHmGdZpsCQCLcBGAs/s400/syncbothad.png" alt=""  /></a></p>
<p>6. Confirm UPN suffix and username option.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-E_6MR7i2s8g/Wvq_tnsjujI/AAAAAAAAKC0/0_mNpsCs3PYlB6OFQuQxTLvLDxGJOoT7gCLcBGAs/s1600/confirmupn.png"><img src="https://2.bp.blogspot.com/-E_6MR7i2s8g/Wvq_tnsjujI/AAAAAAAAKC0/0_mNpsCs3PYlB6OFQuQxTLvLDxGJOoT7gCLcBGAs/s400/confirmupn.png" alt=""  /></a></p>
<p>7. Select OU Filter for both forests, make sure it mirrors the current settings on Old-dc1</p>
<p>8. Choose Users are represented only once across all directories, as we will migrate the users across to the new forest later. With Source anchor, unless you use non default AD attribute for GUID, leave it unchanged.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-L-QJlB44SME/Wvq_-cgeEOI/AAAAAAAAKDA/3vm3Fynoqkg9dSj7xLLvx1MnrEIYUPoxgCLcBGAs/s1600/uniqueiduser.png"><img src="https://3.bp.blogspot.com/-L-QJlB44SME/Wvq_-cgeEOI/AAAAAAAAKDA/3vm3Fynoqkg9dSj7xLLvx1MnrEIYUPoxgCLcBGAs/s400/uniqueiduser.png" alt=""  /></a></p>
<p>9. Leave this unchanged, unless you use group based filtering.</p>
<p>10. With the optional features, you will want to keep the same settings as old-dc1 to avoid any unexpected issues.</p>
<p>11. Choose Enable staging mode&hellip; this will ensure that the AD Connect will not sync after complete the setup.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-Et1c-QXfgrw/WvrAM0PFtSI/AAAAAAAAKDE/6D6jxjHQTLUdu2nTTg2_eWilF816k2FnwCLcBGAs/s1600/enablestaging.png"><img src="https://3.bp.blogspot.com/-Et1c-QXfgrw/WvrAM0PFtSI/AAAAAAAAKDE/6D6jxjHQTLUdu2nTTg2_eWilF816k2FnwCLcBGAs/s400/enablestaging.png" alt=""  /></a></p>
<p>That&rsquo;s it. Click <strong>Install</strong> to finish the setup.</p>
<p>To confirm the staging mode setup, you can run start-ADSyncSyncCycle and check the log in Synchronization Service. As you can see below, there is no Export action to Azure.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-qEidzOAp2cI/WvrAfqPW1FI/AAAAAAAAKDQ/be-0b0t0FDErRb88BMg-Kya51er9dd13wCLcBGAs/s1600/noexport.png"><img src="https://2.bp.blogspot.com/-qEidzOAp2cI/WvrAfqPW1FI/AAAAAAAAKDQ/be-0b0t0FDErRb88BMg-Kya51er9dd13wCLcBGAs/s400/noexport.png" alt=""  /></a></p>
<hr>
<p><strong>Switch Sync Source From Old to New</strong></p>
<p>The next step is to switch the synchronization from the old AD Connect to the new one.</p>
<p>To do this, we first turn AD Connect on OLD-DC1 to Staging mode.</p>
<p>Open AD Connect on OLD-DC1 and choose <strong>Configure staging mode (current state: disabled)</strong></p>
<p><a class="gblog-markdown__link--raw" href="https://1.bp.blogspot.com/-AJXc1ExQ3qw/WvrAz358GZI/AAAAAAAAKDY/QNv15k2dgD0tSQWjjkb5ThsH41M7eS91wCEwYBhgL/s1600/enablestagingonold.png"><img src="https://1.bp.blogspot.com/-AJXc1ExQ3qw/WvrAz358GZI/AAAAAAAAKDY/QNv15k2dgD0tSQWjjkb5ThsH41M7eS91wCEwYBhgL/s400/enablestagingonold.png" alt=""  /></a></p>
<p>Tick the checkbox to enable <strong>Staging</strong> mode.</p>
<p>Uncheck <strong>Start the synchronization process when configuration completes</strong>. If you leave this checked, AD objects from OLD forest will still be synced to AD Connect metaverse(database), but will not be exported to Azure AD.</p>
<p><a class="gblog-markdown__link--raw" href="https://3.bp.blogspot.com/-5YEPYd9peUI/WvrBP_HuZRI/AAAAAAAAKDg/hbBA4314cjwd4JFQJ64S-WLxgZShr_RfACLcBGAs/s1600/unchecksync.png"><img src="https://3.bp.blogspot.com/-5YEPYd9peUI/WvrBP_HuZRI/AAAAAAAAKDg/hbBA4314cjwd4JFQJ64S-WLxgZShr_RfACLcBGAs/s400/unchecksync.png" alt=""  /></a></p>
<p>Once click <strong>Configure</strong>, the AD Connect will proceed to enable staging mode and stop sync to Azure AD.</p>
<p>Next, we disable staging mode on NEW-DC1. </p>
<p>On NEW-DC1, launch AD Connect and select <strong>Configure staging mode (current state: enabled)</strong></p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-vZK2Akb0T7o/WvrBc1OIOvI/AAAAAAAAKDk/bLFeqYR5VbQU1yy_iBVBwr7NkIzICq6XACLcBGAs/s1600/disablestaging.png"><img src="https://4.bp.blogspot.com/-vZK2Akb0T7o/WvrBc1OIOvI/AAAAAAAAKDk/bLFeqYR5VbQU1yy_iBVBwr7NkIzICq6XACLcBGAs/s400/disablestaging.png" alt=""  /></a></p>
<p>Put in Azure AD credential and uncheck <strong>Enable staging mode.</strong></p>
<p><a class="gblog-markdown__link--raw" href="https://4.bp.blogspot.com/-BPdmo4bmZFQ/WvrBxNTJBmI/AAAAAAAAKDw/yOHF5vsQUc8Vfm5FFExB6eThH2X8kIz5gCLcBGAs/s1600/uncheckstaging.png"><img src="https://4.bp.blogspot.com/-BPdmo4bmZFQ/WvrBxNTJBmI/AAAAAAAAKDw/yOHF5vsQUc8Vfm5FFExB6eThH2X8kIz5gCLcBGAs/s400/uncheckstaging.png" alt=""  /></a></p>
<p>Check <strong>Start the synchronization process when configuration completes.</strong></p>
<p>Click <strong>Configure</strong> to apply the changes.</p>
<p><strong>Test AD Sync from NEW-DC1</strong></p>
<p>Run <strong>Start-ADSyncSyncCyle Initial</strong> to kick off a AD Sync from NEW-DC1. From the result below you can see it picks up an update.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-c5EQoA1eCa4/WvrB7BQFrWI/AAAAAAAAKD0/ZoF4rUz-GgA5FqzLrEED9a81OaTyAV48gCLcBGAs/s1600/synctestresult.png"><img src="https://2.bp.blogspot.com/-c5EQoA1eCa4/WvrB7BQFrWI/AAAAAAAAKD0/ZoF4rUz-GgA5FqzLrEED9a81OaTyAV48gCLcBGAs/s400/synctestresult.png" alt=""  /></a></p>
<p>In the properties view of the update item, you can see <strong>ms-DS-ConsistencyGUID</strong> of <strong>Tony Zhang</strong> in <strong>NEW.contoso.com</strong> has now been added to Azure AD**.** It wasn&rsquo;t before because the user was just created when AD Connect in <strong>OLD.contoso.com</strong> did the sync.</p>
<p><a class="gblog-markdown__link--raw" href="https://2.bp.blogspot.com/-XK4fEcPEUGo/WvrCEzN_83I/AAAAAAAAKD8/f-k8ZjfyoekrFr04_J9Da5xl9xoQLVrhQCLcBGAs/s1600/syncoutput.png"><img src="https://2.bp.blogspot.com/-XK4fEcPEUGo/WvrCEzN_83I/AAAAAAAAKD8/f-k8ZjfyoekrFr04_J9Da5xl9xoQLVrhQCLcBGAs/s400/syncoutput.png" alt=""  /></a></p>
<p>So that&rsquo;s it. As simple as that, we have completed the migration of AD Connect from one AD forest to another!</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[On Premise Mailbox user missing in Exchange Online GAL]]></title>
            <link href="/2015/11/on-premise-mailbox-user-does-not-show.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2015/11/on-premise-mailbox-user-does-not-show.html</id>
            
            <published>2015-11-01T20:12:00-08:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Ran into an interesting issue with one of our Exchange Online customer. Thought it probably worth sharing with the solution I found.</p>
<p>The customer has an Exchange Hybrid setup. Recently some of  Office 365 Exchange Online users complain they cannot email to a particular on premise mailbox: <a class="gblog-markdown__link" href="mailto:Paul.Smith@contoso.com">Paul.Smith@contoso.com</a>. The user bascialy does not show up in Exchange Online GAL. The on premise mailbox is working fine and other on premise staff can send emails to it without issue. When I checked Exchange Online, I cannot find the contact object for this account.</p>
<p>Initially I thought it could be for some reason the user&rsquo;s on premise AD object has not been synced properly to Office 365. So I tried with recreating the object in Office 365 by moving the on premise AD account to a non-synced OU. This delete the user&rsquo;s Office 365 account. Then I move the on premise account back into its original OU. After re-sync the account back into Office 365, it still refuse to show up in Exchange Online contact list.</p>
<p>As always, I contacted Microsoft support to try get an answer. But that didn&rsquo;t prove to be too useful either. The suggestion MS provided is to &ldquo;Change our Email domain <strong>contoso.com</strong> from Authority to Internal Relay&rdquo;. This issue only affects a single user out of 500 and MS is suggesting to change a Global setting. I give them up quickly and focus on research the issue myself.</p>
<p>I then took a close look at the user&rsquo;s Office 365 account, while compare it with other accounts I notice a difference in the field of <strong>CloudExchangeRecipientDisplayType</strong>. </p>
<p>Get-MSOLUser –UserPrincipalName <em>paul.smith@contoso.com</em> | fl &gt; c:\paul.txt</p>
<p>Get-MSOLUser –UserPrincipalName <em>tom.leigh@contoso.com</em> | fl &gt; c:\tom.txt</p>
<p><strong>Paul&rsquo;s user details</strong></p>
<p><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/Image%2821%29.png" alt=""  /></p>
<p><a class="gblog-markdown__link--raw" href="http://3.bp.blogspot.com/-N2nK4e3yCJY/VjLAxmgWt7I/AAAAAAAAJKg/8HdHpZ62UUo/s1600/pauldetail.png"><img src="http://3.bp.blogspot.com/-N2nK4e3yCJY/VjLAxmgWt7I/AAAAAAAAJKg/8HdHpZ62UUo/s320/pauldetail.png" alt=""  /></a></p>
<p><strong>Another user details</strong></p>
<p><a class="gblog-markdown__link--raw" href="http://1.bp.blogspot.com/-YeM7H3u7D3I/VjLBDL5g_HI/AAAAAAAAJKo/V_sT-LIQQW0/s1600/userdetail.png"><img src="http://1.bp.blogspot.com/-YeM7H3u7D3I/VjLBDL5g_HI/AAAAAAAAJKo/V_sT-LIQQW0/s320/userdetail.png" alt=""  /></a></p>
<p><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/Image%2822%29.png" alt=""  /></p>
<p>According to this <a class="gblog-markdown__link" href="http://blogs.technet.com/b/johnbai/archive/2013/09/11/o365-msexchangerecipienttypedetails.aspx">article</a>, -2147483642 means &ldquo;SynedMailboxUser&rdquo;. This is the recipient type of Paul&rsquo;s Office 365 account. So now we know what&rsquo;s wrong with Paul&rsquo;s account.</p>
<p>To fix the issue, we just need to set <strong>CloudExchangeRecipientDisplayType</strong> with the correct value. To do so we need to make change to Paul&rsquo;s on premise AD account.</p>
<p>In on premise ADUC, enable <strong>Advanced Features</strong> in view. Open the user properties and select <strong>Attribute Editor</strong> tab.</p>
<p>Change following fields</p>
<p><strong>msExchRecipietnDisplayType -&gt; 6</strong></p>
<p><strong>msExchRecipientTypeDetails -&gt; 128</strong></p>
<p><strong>targetAddress -&gt; <a class="gblog-markdown__link" href="mailto:paul.smith@contoso.com">paul.smith@contoso.com</a></strong></p>
<p>Make sure you take a screenshot of the existing value</p>
<p><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/Image%2823%29.png" alt=""  /></p>
<p><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/Image%2824%29.png" alt=""  /></p>
<p>After change the value, perform a AD SYNC from DirSync or run <strong>Start-OnlineCoexistenceSync</strong>. </p>
<p>In the SYNC log, confirm the user object has been updated. If you go into the Exchange Online contact list, Paul&rsquo;s email address should be available now.</p>
<p>Once confirmed user email address available in Office 365 again, we need to reverse the above change to the on premise user account. Otherwise, Paul will not be able to access his on premise mailbox.</p>
<p>To do so, simply apply the following changes</p>
<p><strong>msExchRecipietnDisplayType -&gt; 1073741824</strong></p>
<p><strong>msExchRecipientTypeDetails -&gt; 1</strong></p>
<p><strong>targetAddress -&gt; <not set></strong></p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="/tags/GAL" term="GAL" label="GAL" />
                             
                                <category scheme="/tags/Exchange-Hybrid" term="Exchange-Hybrid" label="Exchange Hybrid" />
                             
                                <category scheme="/tags/Office-365" term="Office-365" label="Office 365" />
                             
                                <category scheme="/tags/Cloud" term="Cloud" label="Cloud" />
                            
                        
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Enable local timestamp on Office 365 SharePoint site]]></title>
            <link href="/2015/09/enable-local-timestamp-on-office-365.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2015/09/enable-local-timestamp-on-office-365.html</id>
            
            <published>2015-09-14T21:01:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I received some complains from our US users about the timestamp of our SharePoint site hosted in Office 365. The timestamp of the documents and folders on the site are all showing Australia Eastern time for them. It takes me sometime to find out how to allow each individual user to see their local timestamp. This requires some setting changes from the user end. Here is what needs to be done.</p>
<p>1. Log into Office 365 SharePoint site and click your name in the top right corner, then the drop down list will appear with <strong>About me</strong> option.</p>
<p><a class="gblog-markdown__link--raw" href="http://3.bp.blogspot.com/-qnUfajGKXQY/VfeTu5wv1lI/AAAAAAAAJIs/KNQVW8YpO7I/s1600/Image1.jpg"><img src="https://3.bp.blogspot.com/-qnUfajGKXQY/VfeTu5wv1lI/AAAAAAAAJIs/KNQVW8YpO7I/s320/Image1.jpg" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image002.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c6ce.png" alt=""  /></a></p>
<p>2. Click <strong>About me</strong>, then click <strong>edit</strong> in the next page.</p>
<p><a class="gblog-markdown__link--raw" href="http://1.bp.blogspot.com/-xEcDAfa1764/VfeUPzNHe_I/AAAAAAAAJI8/7YlFDJ_mfUQ/s1600/Image1.jpg"><img src="https://1.bp.blogspot.com/-xEcDAfa1764/VfeUPzNHe_I/AAAAAAAAJI8/7YlFDJ_mfUQ/s1600/Image1.jpg" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image003.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c6dd.png" alt=""  /></a></p>
<p>3. In the next page, click <strong>…</strong> icon next to <strong>Details.</strong> Then select <strong>Language and Region</strong></p>
<p><a class="gblog-markdown__link--raw" href="http://1.bp.blogspot.com/-qIEjJYr514Y/VfeUf5KDK0I/AAAAAAAAJJE/-lnPpTxE6gw/s1600/Image1.jpg"><img src="https://1.bp.blogspot.com/-qIEjJYr514Y/VfeUf5KDK0I/AAAAAAAAJJE/-lnPpTxE6gw/s1600/Image1.jpg" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image004.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c6ed.png" alt=""  /></a></p>
<p>4. In the next page, under <strong>Choose your settings</strong>, select <strong>Always use my personal settings</strong>. This will override the SharePoint site settings. Make sure <strong>Time Zone</strong> is set to correct local time zone.</p>
<p><a class="gblog-markdown__link--raw" href="http://1.bp.blogspot.com/-hrIDrbIeRf0/VfeUumYylRI/AAAAAAAAJJM/YHlDyKP4JeE/s1600/Image1.jpg"><img src="https://1.bp.blogspot.com/-hrIDrbIeRf0/VfeUumYylRI/AAAAAAAAJJM/YHlDyKP4JeE/s320/Image1.jpg" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image005.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c6fc.png" alt=""  /></a></p>
<p>5. <strong>VERY IMPORTANT</strong>, after make all these changes, please scroll to the bottom of the page and click <strong>Save all and close</strong>.</p>
<p><a class="gblog-markdown__link--raw" href="http://2.bp.blogspot.com/-F2P0KTz4r10/VfeU8E_yHNI/AAAAAAAAJJU/EPE3PSW_4cs/s1600/Image1.jpg"><img src="https://2.bp.blogspot.com/-F2P0KTz4r10/VfeU8E_yHNI/AAAAAAAAJJU/EPE3PSW_4cs/s320/Image1.jpg" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image006.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c70c.png" alt=""  /></a></p>
<p>6. You will get a prompt after save the change. As it indicates the change will take some time. So you won’t be able to see US timestamp in the SharePoint site straight away. Please wait for a few hours and try log into Office 365 again. You should then be able to view the SharePoint Site with US timestamp.</p>
<p><a class="gblog-markdown__link--raw" href="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/image019.jpg@01D00971.4C38F520"><img src="file:///C:/Users/chent/AppData/Local/Temp/enhtmlclip/EN2ef2c71c.png" alt=""  /></a></p>
<p><a class="gblog-markdown__link--raw" href="http://4.bp.blogspot.com/-V--cSJKQbEo/VfeVE-k4suI/AAAAAAAAJJc/S7KSscGeXtU/s1600/Image1.jpg"><img src="https://4.bp.blogspot.com/-V--cSJKQbEo/VfeVE-k4suI/AAAAAAAAJJc/S7KSscGeXtU/s320/Image1.jpg" alt=""  /></a></p>
<blockquote>
</blockquote>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Annoying Office 365 issues: Username won't sync through DirSync]]></title>
            <link href="/2014/06/annoying-office-365-issues-username.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2014/06/annoying-office-365-issues-username.html</id>
            
            <published>2014-06-18T18:03:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>It happens to me quite a few times. After synced On-Premise AD user objects into Office365 through DirSync, users&rsquo; specified UPN will not be synced correctly into Office 365.</p>
<p>Tried all the suggested tricks from Office 365 forum. None of them worked. I even tried to re-configure our DirSync settings. But the username in O365 just refuse to change. After banging my head on this issue for a few days. Eventually I figured it out, the solution is simply to revoke any O365 licenses assigned to the user. Then conduct a full sync with &ldquo;Start-OnlineCoexistenceSync&rdquo;. With licenses allocated to the user, Office 365 cannot change the account&rsquo;s UPN.</p>
<p><a class="gblog-markdown__link--raw" href="http://3.bp.blogspot.com/-69Bjg7NwmVs/U6I1MnKGLMI/AAAAAAAAJF0/QZSAeomoFME/s1600/nolicenses.png"><img src="http://3.bp.blogspot.com/-69Bjg7NwmVs/U6I1MnKGLMI/AAAAAAAAJF0/QZSAeomoFME/s1600/nolicenses.png" alt=""  /></a></p>
<p>It may be necessary to change the username AFTER revoke all the O365 licenses and resync. DirSync doesn&rsquo;t seem to pickup the difference between the On-Premise AD object and the O365 Azure AD object. It only upload whatever attribute is changed.</p>
]]></content>
            
                 
                    
                 
                    
                
            
        </entry>
    
        
        <entry>
            <title type="html"><![CDATA[Receive error: Target mailbox doesn't have an SMTP proxy matching '.mail.onmicrosoft.com' when move mailbox to Office 365]]></title>
            <link href="/2014/04/office-365-exchange-hybrid-deployment.html?utm_source=atom_feed" rel="alternate" type="text/html"  hreflang="en" />
            <id>/2014/04/office-365-exchange-hybrid-deployment.html</id>
            
            <published>2014-04-06T23:51:00-07:00</published>
            <updated>2021-09-19T15:05:42+10:00</updated>
            
            
            <content type="html"><![CDATA[<p>Recently I was working on Exchange Hybrid Deployment for one of our customer. The Hybrid Configuration process itself went smoothly. No errors for HCW. </p>
<p>The problems came when I tried to move mailbox to Exchange Online. The migration fails with error:<br>
<strong>Target mailbox doesn&rsquo;t have an SMTP proxy matching &lsquo;<domain>.mail.onmicrosoft.com&rsquo;&quot;</strong></p>
<p>To check that I ran the following command against the On-premise Exchange.</p>
<p>Get-Mailbox &ldquo;O365 Test5&rdquo; | fl and check the EmailAddress filed</p>
<p><a class="gblog-markdown__link--raw" href="http://3.bp.blogspot.com/-xfnTdyJyULk/U0JKuLIyF0I/AAAAAAAAJCI/N_bl6fxO_0o/s1600/getmailbox.png"><img src="http://3.bp.blogspot.com/-xfnTdyJyULk/U0JKuLIyF0I/AAAAAAAAJCI/N_bl6fxO_0o/s1600/getmailbox.png" alt=""  /></a></p>
<p>Then I ran get-mailuser in Exchange Online. Because the mailbox hasn&rsquo;t been migrated, the user still show up as Mailuser(Contact) in Office 365.</p>
<p>Get-MailUser &ldquo;o365 test6&rdquo; |fl</p>
<p><a class="gblog-markdown__link--raw" href="http://4.bp.blogspot.com/-uIrsas0ytfw/U0JINlFHZDI/AAAAAAAAJB0/mxOOdYZj5S4/s1600/getmailuser.png"><img src="http://4.bp.blogspot.com/-uIrsas0ytfw/U0JINlFHZDI/AAAAAAAAJB0/mxOOdYZj5S4/s1600/getmailuser.png" alt=""  /></a></p>
<p>Noticed the proxy address &ldquo;contoso.mail.onmicrosoft.com&rdquo; does not show up into the Azure AD object. The user object in Office 365  only has our primary SMTP address &ldquo;<a class="gblog-markdown__link" href="mailto:username@contoso.com">username@contoso.com</a>&rdquo; and &ldquo;<a class="gblog-markdown__link" href="mailto:username@contoso.onmicrosoft.com">username@contoso.onmicrosoft.com</a>&rdquo; stamped. &ldquo;CONTOSO.MAIL.onmicrosoft.com&rdquo; is missing from the address list.</p>
<p>After review the DirSync rules, I notice for user object, the proxyAddress attribute does not have an &ldquo;Import&rdquo; rule associated with it. As a test I manually added a Import rule for &ldquo;proxyAddresses&rdquo;.<br>
The rule details are:</p>
<p>cd.all:proxyAddresses-&gt;mv.all:proxyAddresses</p>
<p><a class="gblog-markdown__link--raw" href="http://1.bp.blogspot.com/-sV_2gnEWJ54/UzpSrP0C_LI/AAAAAAAAJBU/JshPC8DHPjU/s1600/dirsync&#43;rule.png"><img src="http://1.bp.blogspot.com/-sV_2gnEWJ54/UzpSrP0C_LI/AAAAAAAAJBU/JshPC8DHPjU/s1600/dirsync&#43;rule.png" alt=""  /></a></p>
<p>After kicked off a full sync by run &ldquo;Start-OnlineCoexistenceSync&rdquo;. </p>
<p>&ldquo;get-mailuser&rdquo; was able to return with the SMTP addresses &ldquo;contoso.mail.onmicrosoft.com&rdquo;. Mailbox migration to Office 365 went successfully after that.</p>
]]></content>
            
                 
                    
                 
                    
                         
                        
                            
                             
                                <category scheme="/tags/Exchange-Hybrid" term="Exchange-Hybrid" label="Exchange Hybrid" />
                             
                                <category scheme="/tags/mail.onmicrosoft.com" term="mail.onmicrosoft.com" label="mail.onmicrosoft.com" />
                             
                                <category scheme="/tags/Mailbox-Migration" term="Mailbox-Migration" label="Mailbox Migration" />
                             
                                <category scheme="/tags/Office-365" term="Office-365" label="Office 365" />
                            
                        
                    
                
            
        </entry>
    
</feed>
